{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXjVxSG4yt-o"
      },
      "source": [
        "#Import libraies and packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVA_ZA7c5BE0",
        "outputId": "8241eb14-fba0-4c05-816c-4f9e44415497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ray in /usr/local/lib/python3.7/dist-packages (1.12.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.7/dist-packages (from ray) (20.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.4.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray) (1.3.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.3)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray) (2.23.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray) (1.2.0)\n",
            "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.43.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.7.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.11.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: platformdirs<3,>=2 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray) (2.5.2)\n",
            "Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray) (0.3.4)\n"
          ]
        }
      ],
      "source": [
        "pip install ray torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PlaA55u5HNx",
        "outputId": "877fa9df-c286-4f41-dc6d-e7efd6159cc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lU9kk9xU5K4-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
        "from ray.tune.suggest import ConcurrencyLimiter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yvgorDkMN429"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtyQ06Iu5MP2",
        "outputId": "5f347465-866d-4984-c479-630610d335cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoUYbBj2yxpO"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HmIVYXYN5Nv9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def ConvNet(config, len_classes=2):\n",
        "    input = tf.keras.layers.Input(shape=(400, 1))\n",
        "    x = input\n",
        "    # for i in range(config['layers']):\n",
        "    x = tf.keras.layers.Conv1D(filters=config['conv_block1_filters'], kernel_size=3, strides=1)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(filters=config['conv_block2_filters'], kernel_size=3, strides=1)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(filters=config['conv_block3_filters'], kernel_size=3, strides=1)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(filters=config['conv_block4_filters'], kernel_size=2, strides=1)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(filters=config['conv_block5_filters'], kernel_size=1, strides=1)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    if config['fc_layer_type'] == 'dense':\n",
        "        if config['pool_type'] == 'max':\n",
        "            x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "        else:\n",
        "            x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Fully connected layer 1\n",
        "        x = tf.keras.layers.Dense(units=config['fc1_units'])(x)\n",
        "        x = tf.keras.layers.Dropout(config['dropout_rate'])(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "        # Fully connected layer 2\n",
        "        x = tf.keras.layers.Dense(units=len_classes)(x)\n",
        "        x = tf.keras.layers.Dropout(config['dropout_rate'])(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        predictions = tf.keras.layers.Dense(1,tf.keras.layers.Activation('sigmoid'))(x)\n",
        "\n",
        "    else:\n",
        "        # Fully connected layer 1\n",
        "        x = tf.keras.layers.Conv1D(filters=config['fc1_units'], kernel_size=1, strides=1)(x)\n",
        "        x = tf.keras.layers.Dropout(config['dropout_rate'])(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "\n",
        "        # Fully connected layer 2\n",
        "        x = tf.keras.layers.Conv1D(filters=len_classes, kernel_size=1, strides=1)(x)\n",
        "        x = tf.keras.layers.Dropout(config['dropout_rate'])(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        \n",
        "        if config['pool_type'] == 'max':\n",
        "            x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "        else:\n",
        "            x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "        \n",
        "        predictions = tf.keras.layers.Dense(1,tf.keras.layers.Activation('sigmoid'))(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input, outputs=predictions)\n",
        "    \n",
        "    print(model.summary())\n",
        "    print(f'Total number of layers: {len(model.layers)}')\n",
        "\n",
        "    return model\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7vViFfkzJTH"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3bJZCOYSB1qA"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oss9TBkZzMYA"
      },
      "source": [
        "## Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BSIMfshH5Qzx"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "def train_mnist( config):\n",
        "  path ='/content/drive/MyDrive/UPM/Clinical_data_and_RNA_total_Features_PFS.csv'\n",
        "  data_frame = pd.read_csv(path)\n",
        "  \n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X = data_frame[['5S_rRNA', 'AADACL2-AS1', 'AB015752.3', 'AB019438.66', 'ABHD17AP7', 'AC000077.2', 'AC000111.3', 'AC000111.5', 'AC000124.1', 'AC002368.4', 'AC002472.11', 'AC002486.3', 'AC002542.2', 'AC003084.2', 'AC003682.16', 'AC003682.17', 'AC003986.5', 'AC004006.2', 'AC004012.1', 'AC004014.3', 'AC004066.3', 'AC004112.4', 'AC004158.3', 'AC004221.2', 'AC004449.6', 'AC004510.3', 'AC004549.6', 'AC004862.6', 'AC004870.3', 'AC004941.3', 'AC004947.2', 'AC005008.2', 'AC005094.2', 'AC005178.1', 'AC005197.2', 'AC005235.1', 'AC005307.1', 'AC005355.1', 'AC005387.3', 'AC005392.13', 'AC005498.4', 'AC005518.2', 'AC005550.5', 'AC005597.1', 'AC005624.2', 'AC005722.4', 'AC005740.5', 'AC006003.3', 'AC006007.1', 'AC006041.1', 'AC006116.14', 'AC006130.4', 'AC006153.3', 'AC006296.2', 'AC006372.4', 'AC006372.5', 'AC006372.6', 'AC006373.1', 'AC006427.2', 'AC006499.1', 'AC006499.3', 'AC006499.5', 'AC006499.6', 'AC006509.4', 'AC006960.7', 'AC006987.5', 'AC007128.1', 'AC007131.2', 'AC007163.6', 'AC007193.9', 'AC007249.3', 'AC007253.1', 'AC007292.7', 'AC007322.1', 'AC007349.7', 'AC007391.2', 'AC007403.3', 'AC007463.2', 'AC007557.4', 'AC007563.3', 'AC007563.5', 'AC007568.1', 'AC007679.3', 'AC007682.1', 'AC007740.1', 'AC007742.3', 'AC007796.1', 'AC007879.1', 'AC007879.3', 'AC008067.2', 'AC008074.4', 'AC008154.4', 'AC008278.2', 'AC008281.1', 'AC008746.10', 'AC008746.9', 'AC008834.2', 'AC008940.1', 'AC009161.1', 'AC009237.16', 'AC009237.17', 'AC009238.7', 'AC009238.8', 'AC009310.1', 'AC009312.1', 'AC009313.1', 'AC009410.1', 'AC009477.7', 'AC009487.4', 'AC009487.5', 'AC009541.1', 'AC009970.1', 'AC010096.2', 'AC010141.1', 'AC010518.3', 'AC010729.1', 'AC010729.2', 'AC010731.2', 'AC010731.6', 'AC010884.1', 'AC010907.5', 'AC010967.1', 'AC010967.2', 'AC010974.3', 'AC010983.1', 'AC010987.6', 'AC011196.3', 'AC011243.1', 'AC011286.1', 'AC011343.1', 'AC011516.2', 'AC011524.3', 'AC011648.1', 'AC011742.5', 'AC012074.2', 'AC012075.2', 'AC012322.1', 'AC012354.6', 'AC012363.13', 'AC012456.4', 'AC012506.1', 'AC012506.2', 'AC012506.3', 'AC012506.4', 'AC012507.3', 'AC012507.4', 'AC012513.6', 'AC012668.1', 'AC013402.5', 'AC013429.4', 'AC013448.2', 'AC013480.2', 'AC013733.3', 'AC015936.3', 'AC016694.2', 'AC016735.1', 'AC016745.3', 'AC016831.3', 'AC016831.5', 'AC016912.3', 'AC017019.1', 'AC017048.2', 'AC018359.1', 'AC018643.4', 'AC018685.1', 'AC018717.1', 'AC018799.1', 'AC018804.3', 'AC018880.1', 'AC018880.2', 'AC018892.3', 'AC019064.1', 'AC019118.4', 'AC020595.1', 'AC020743.2', 'AC020915.2', 'AC022201.4', 'AC023115.4', 'AC023128.1', 'AC024082.4', 'AC024704.2', 'AC026202.5', 'AC026954.6', 'AC027124.2', 'AC034228.2', 'AC060834.3', 'AC062016.1', 'AC062022.1', 'AC063976.1', 'AC063976.3', 'AC064834.1', 'AC064834.2', 'AC067945.2', 'AC067956.1', 'AC068057.1', 'AC068490.1', 'AC068492.1', 'AC068535.3', 'AC068831.3', 'AC069155.1', 'AC073218.1', 'AC073464.6', 'AC073641.2', 'AC074338.4', 'AC078842.3', 'AC078882.1', 'AC079112.1', 'AC079117.1', 'AC079145.4', 'AC079154.1', 'AC079325.6', 'AC079586.1', 'AC079610.1', 'AC079776.1', 'AC079987.2', 'AC083864.3', 'AC083939.1', 'AC090018.1', 'AC090952.5', 'AC090957.2', 'AC091133.1', 'AC091729.8', 'AC091814.2', 'AC092155.4', 'AC092159.3', 'AC092168.3', 'AC092566.1', 'AC092570.1', 'AC092570.2', 'AC092578.1', 'AC092601.1', 'AC092635.1', 'AC092657.2', 'AC092933.4', 'AC093063.2', 'AC093084.1', 'AC093106.4', 'AC093326.1', 'AC093381.2', 'AC093590.1', 'AC093642.6', 'AC096582.9', 'AC096669.2', 'AC096732.2', 'AC097468.7', 'AC097713.3', 'AC097713.4', 'AC098592.6', 'AC098828.3', 'AC098872.3', 'AC098973.1', 'AC099344.2', 'AC099344.3', 'AC099552.3', 'AC102948.2', 'AC103563.5', 'AC103563.7', 'AC103563.8', 'AC104076.3', 'AC104306.4', 'AC104777.2', 'AC104777.4', 'AC107083.1', 'AC108032.1', 'AC108066.1', 'AC109589.1', 'AC110620.1', 'AC110754.3', 'AC112198.1', 'AC112220.2', 'AC112518.3', 'AC113167.1', 'AC113167.2', 'AC113607.2', 'AC114752.3', 'AC114814.2', 'AC114877.3', 'AC118653.2', 'AC123900.2', 'AC124057.5', 'AC125421.1', 'AC128709.2', 'AC128709.3', 'AC128709.4', 'AC133633.1', 'AC133680.1', 'AC133785.1', 'AC140481.8', 'AC142119.1', 'AC142293.3', 'AC144450.1', 'AC144833.1', 'ADI1P2', 'AE000661.36', 'AF186192.6', 'AHCYP3', 'AL133249.1', 'AP000345.4', 'AP000356.2', 'AP000619.5', 'AP000648.6', 'AP000997.2', 'AP006285.7', 'ATP5F1P4', 'ATP5G1P8', 'BAATP1', 'BOK-AS1', 'BRI3BPP1', 'BSN-AS1', 'BSN-AS2', 'CASC16', 'CDCA4P3', 'CDKN2AIPNLP2', 'CICP1', 'CLUHP4', 'CNN2P2', 'CTA-392E5.1', 'CTB-138E5.1', 'CTB-164N12.1', 'CTB-49A3.1', 'CTC-304I17.5', 'CTC-339O9.2', 'CTC-501O10.1', 'CTD-2130F23.2', 'CTD-2269E23.3', 'CTD-2308B18.1', 'CTD-2319I12.5', 'CTD-2555O16.1', 'CTD-3187F8.11', 'CYCSP26', 'CYP2T3P', 'DDX43P2', 'DLG3-AS1', 'DPRXP6', 'GLUD1P4', 'GLULP6', 'GRAMD4P7', 'GS1-114I9.1', 'GS1-251I9.2', 'GTSCR1', 'HNRNPH3P1', 'IFT74-AS1', 'IGKV7-3', 'KRT8P6', 'KRTAP20-3', 'LINC00309', 'LINC00364', 'LINC00391', 'LINC00466', 'LINC00838', 'LINC00856', 'LINC00864', 'LINC00879', 'LINC01326', 'LINC01524', 'LRRC34P1', 'MARCKSL1P2', 'MIR1273F', 'MIR4290HG', 'MRLN', 'MTATP6P23', 'MTCO2P16', 'NHEG1', 'NOS2P4', 'OR10B1P', 'OR13Z2P', 'OR2AD1P', 'OR2B8P', 'OR5BN2P', 'OSBPL9P1', 'PDX1-AS1', 'PGBD4P5', 'RN7SKP238', 'RNA5SP205', 'RNF212', 'RNU6-1090P', 'RP1-151B14.9', 'RP11-100K18.1', 'RP11-107N7.1', 'RP11-123J14.1', 'RP11-203B7.2', 'RP11-2H8.3', 'RP11-307O10.1', 'RP11-359G22.2', 'RP11-361A23.3', 'RP11-378I6.1', 'RP11-430C1.1', 'RP11-434D11.4', 'RP11-438B23.2', 'RP11-520D19.2', 'RP11-661A12.12', 'RP11-846C15.2', 'RP11-97F8.1', 'RP13-631K18.2', 'RP3-406P24.4', 'RP4-581O6.1', 'RP5-998G20.1', 'RPL35AP', 'XXyac-YR12DB5.1', 'YBX1P9']].astype(float)\n",
        "  Y=[]\n",
        "  for i in range (len(data_frame)):\n",
        "      if data_frame.Target[i]=='NR': # If PFS is lower than 3 months, I will consider it as NonResponder (NR)\n",
        "          Y.append(0)\n",
        "      else:\n",
        "          Y.append(1)# If PFS is over 3 months, I will consider it as Responder (R)\n",
        "  scaler = preprocessing.MinMaxScaler()\n",
        "  names = X.columns\n",
        "  d = scaler.fit_transform(X)\n",
        "  X = pd.DataFrame(d, columns=names)\n",
        "  XTrain, XTest, yTrain, yTest = train_test_split(X, Y, test_size=0.20, random_state=125)\n",
        "  # Convert sets to arrays\n",
        "  XTrain = XTrain.values\n",
        "  XTest = XTest.values\n",
        "  # It is mandatory to transform Y list into array for trainning the model\n",
        "  yTrain=np.array(yTrain)\n",
        "  yTest=np.array(yTest)\n",
        "\n",
        "  # building the input vector from the 20x20 pixels\n",
        "  X_train = XTrain.reshape(XTrain.shape[0], 400 , 1)\n",
        "  X_test = XTest.reshape(XTest.shape[0], 400, 1)\n",
        "  X_train = X_train.astype('float32')\n",
        "  X_test = X_test.astype('float32')\n",
        "  # Create model\n",
        "  model = ConvNet(config)\n",
        "\n",
        "  # Compile model with losses and metrics\n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate =config['lr']),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', f1_m,precision_m, recall_m\n",
        "                           ])\n",
        "\n",
        "  # Start model training\n",
        "  history_m = model.fit(X_train, yTrain,\n",
        "                      epochs=1,\n",
        "                      validation_data=(X_test, yTest),\n",
        "                      )\n",
        "\n",
        "  history_m = {\n",
        "  \"loss\": history_m.history[\"loss\"][0],\n",
        "  \"val_loss\": history_m.history[\"val_loss\"][0],\n",
        "  \"accuracy\": history_m.history[\"accuracy\"][0],\n",
        "  \"val_accuracy\": history_m.history[\"val_accuracy\"][0]\n",
        "  }\n",
        "  return history_m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QP5Zl8izRcd"
      },
      "source": [
        "# Bayesian Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvwt9luxAGLl",
        "outputId": "2f8e2758-9e0a-4ed6-ddad-772ca5ab098c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hpbandster\n",
            "  Downloading hpbandster-0.7.4.tar.gz (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 84 kB/s \n",
            "\u001b[?25hRequirement already satisfied: ConfigSpace in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Collecting Pyro4\n",
            "  Downloading Pyro4-4.82-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting serpent\n",
            "  Downloading serpent-1.40-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hpbandster) (1.21.6)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from hpbandster) (0.10.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hpbandster) (1.4.1)\n",
            "Collecting netifaces\n",
            "  Downloading netifaces-0.11.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (32 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace) (3.0.9)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace) (0.29.30)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from statsmodels->hpbandster) (1.3.5)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->hpbandster) (0.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->statsmodels->hpbandster) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->statsmodels->hpbandster) (2022.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.0->statsmodels->hpbandster) (1.15.0)\n",
            "Building wheels for collected packages: hpbandster\n",
            "  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hpbandster: filename=hpbandster-0.7.4-py3-none-any.whl size=80006 sha256=470ddce526a7f1d4ff7552f9d8abcd5e0c3c1ea609d2a5f4e228fe983ede8828\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/88/fc/61ab6b9f386a386839668631c39a6dc3c2fb0ec7000d552faa\n",
            "Successfully built hpbandster\n",
            "Installing collected packages: serpent, Pyro4, netifaces, hpbandster\n",
            "Successfully installed Pyro4-4.82 hpbandster-0.7.4 netifaces-0.11.0 serpent-1.40\n"
          ]
        }
      ],
      "source": [
        "pip install hpbandster ConfigSpace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCRd1vVu-GX5",
        "outputId": "1b2f322f-ea92-4db2-b549-750c64de7914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ConfigSpace in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace) (3.0.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ConfigSpace) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ConfigSpace) (1.21.6)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace) (0.29.30)\n"
          ]
        }
      ],
      "source": [
        "pip install ConfigSpace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yoSc_LZj-PrU"
      },
      "outputs": [],
      "source": [
        "import ConfigSpace as CS\n",
        "config_space = CS.ConfigurationSpace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aagkiF3-Syc",
        "outputId": "b8ab7a43-0d73-4425-f431-cfcf7a8fecca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "fc1_units, Type: Categorical, Choices: {32, 64, 128, 256, 512}, Default: 32"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config_space.add_hyperparameter(\n",
        "    CS.CategoricalHyperparameter(\n",
        "        name=\"lr\", choices=[0.0001, 0.001, 0.01, 0.1]))\n",
        "config_space.add_hyperparameter(\n",
        "    CS.CategoricalHyperparameter(\n",
        "        name=\"dropout_rate\", choices=[0.1, 0.2, 0.3, 0.4, 0.5]))\n",
        "config_space.add_hyperparameter(\n",
        "    CS.CategoricalHyperparameter(\n",
        "        name=\"conv_block1_filters\", choices=[32, 64, 128, 256, 512]))\n",
        "config_space.add_hyperparameter(\n",
        "    CS.CategoricalHyperparameter(\n",
        "        name=\"conv_block2_filters\", choices=[32, 64, 128, 256, 512]))\n",
        "config_space.add_hyperparameter(\n",
        "    CS.CategoricalHyperparameter(\n",
        "        name=\"conv_block3_filters\", choices=[32, 64, 128, 256, 512]))\n",
        "config_space.add_hyperparameter(\n",
        "    CS.CategoricalHyperparameter(\n",
        "        name=\"conv_block4_filters\", choices=[32, 64, 128, 256, 512]))\n",
        "config_space.add_hyperparameter(\n",
        "    CS.CategoricalHyperparameter(\n",
        "        name=\"conv_block5_filters\", choices=[32, 64, 128, 256]))\n",
        "config_space.add_hyperparameter(\n",
        "    CS.CategoricalHyperparameter(\n",
        "        name=\"fc_layer_type\", choices= ['dense', 'convolution']))\n",
        "config_space.add_hyperparameter(\n",
        "    CS.CategoricalHyperparameter(\n",
        "        name=\"pool_type\", choices= ['max', 'average']))\n",
        "config_space.add_hyperparameter(\n",
        "    CS.CategoricalHyperparameter(\n",
        "        name=\"fc1_units\", choices=[32, 64, 128, 256, 512]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b5NabPB3_mlz",
        "outputId": "f417d209-04ad-4822-ef81-f74074156892"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-24 17:01:15,747\tWARNING callback.py:126 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
            "2022-05-24 17:01:15,755\tWARNING tune.py:637 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n",
            "2022-05-24 17:01:15,871\tINFO trial_runner.py:803 -- starting train_mnist_1f8470da\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "== Status ==<br>Current time: 2022-05-24 17:01:19 (running for 00:00:04.12)<br>Memory usage on this node: 2.1/12.7 GiB<br>Using HyperBand: num_stopped=0 total_brackets=1\n",
              "Round #0:\n",
              "  Bracket(Max Size (n)=81, Milestone (r)=1, completed=0.0%): {RUNNING: 1} <br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.32 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/train_mnist_2022-05-24_17-01-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name          </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  conv_block1_filters</th><th style=\"text-align: right;\">  conv_block2_filters</th><th style=\"text-align: right;\">  conv_block3_filters</th><th style=\"text-align: right;\">  conv_block4_filters</th><th style=\"text-align: right;\">  conv_block5_filters</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  fc1_units</th><th>fc_layer_type  </th><th style=\"text-align: right;\">    lr</th><th>pool_type  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_1f8470da</td><td>RUNNING </td><td>172.28.0.2:9286</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                   64</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">           0.2</td><td style=\"text-align: right;\">        128</td><td>dense          </td><td style=\"text-align: right;\">0.0001</td><td>average    </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "== Status ==<br>Current time: 2022-05-24 17:01:24 (running for 00:00:09.12)<br>Memory usage on this node: 2.3/12.7 GiB<br>Using HyperBand: num_stopped=0 total_brackets=1\n",
              "Round #0:\n",
              "  Bracket(Max Size (n)=81, Milestone (r)=1, completed=0.0%): {RUNNING: 1} <br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.32 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/train_mnist_2022-05-24_17-01-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name          </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  conv_block1_filters</th><th style=\"text-align: right;\">  conv_block2_filters</th><th style=\"text-align: right;\">  conv_block3_filters</th><th style=\"text-align: right;\">  conv_block4_filters</th><th style=\"text-align: right;\">  conv_block5_filters</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  fc1_units</th><th>fc_layer_type  </th><th style=\"text-align: right;\">    lr</th><th>pool_type  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_1f8470da</td><td>RUNNING </td><td>172.28.0.2:9286</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                   64</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">           0.2</td><td style=\"text-align: right;\">        128</td><td>dense          </td><td style=\"text-align: right;\">0.0001</td><td>average    </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m 2022-05-24 17:01:27.325050: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m Model: \"model\"\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m _________________________________________________________________\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  Layer (type)                Output Shape              Param #   \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m =================================================================\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  input_1 (InputLayer)        [(None, 400, 1)]          0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  conv1d (Conv1D)             (None, 398, 128)          512       \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  batch_normalization (BatchN  (None, 398, 128)         512       \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  ormalization)                                                   \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  activation (Activation)     (None, 398, 128)          0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  conv1d_1 (Conv1D)           (None, 396, 256)          98560     \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  batch_normalization_1 (Batc  (None, 396, 256)         1024      \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  hNormalization)                                                 \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  activation_1 (Activation)   (None, 396, 256)          0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  conv1d_2 (Conv1D)           (None, 394, 128)          98432     \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  batch_normalization_2 (Batc  (None, 394, 128)         512       \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  hNormalization)                                                 \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  activation_2 (Activation)   (None, 394, 128)          0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  conv1d_3 (Conv1D)           (None, 393, 64)           16448     \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  batch_normalization_3 (Batc  (None, 393, 64)          256       \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  hNormalization)                                                 \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  activation_3 (Activation)   (None, 393, 64)           0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  conv1d_4 (Conv1D)           (None, 393, 256)          16640     \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  batch_normalization_4 (Batc  (None, 393, 256)         1024      \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  hNormalization)                                                 \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  activation_4 (Activation)   (None, 393, 256)          0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  global_average_pooling1d (G  (None, 256)              0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  lobalAveragePooling1D)                                          \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  dense (Dense)               (None, 128)               32896     \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  dropout (Dropout)           (None, 128)               0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  batch_normalization_5 (Batc  (None, 128)              512       \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  hNormalization)                                                 \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  activation_5 (Activation)   (None, 128)               0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  dense_1 (Dense)             (None, 2)                 258       \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  dropout_1 (Dropout)         (None, 2)                 0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  batch_normalization_6 (Batc  (None, 2)                8         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  hNormalization)                                                 \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m  dense_2 (Dense)             (None, 1)                 3         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m =================================================================\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m Total params: 267,597\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m Trainable params: 265,673\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m Non-trainable params: 1,924\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m _________________________________________________________________\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m None\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m Total number of layers: 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function f1_m at 0x7f763d2b0050> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m Cause: Unable to locate the source code of <function f1_m at 0x7f763d2b0050>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function precision_m at 0x7f763d2b8830> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m Cause: Unable to locate the source code of <function precision_m at 0x7f763d2b8830>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function recall_m at 0x7f75d61e8f80> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m Cause: Unable to locate the source code of <function recall_m at 0x7f75d61e8f80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9286)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "== Status ==<br>Current time: 2022-05-24 17:01:29 (running for 00:00:14.14)<br>Memory usage on this node: 2.2/12.7 GiB<br>Using HyperBand: num_stopped=0 total_brackets=1\n",
              "Round #0:\n",
              "  Bracket(Max Size (n)=81, Milestone (r)=1, completed=0.0%): {RUNNING: 1} <br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.32 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/train_mnist_2022-05-24_17-01-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name          </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  conv_block1_filters</th><th style=\"text-align: right;\">  conv_block2_filters</th><th style=\"text-align: right;\">  conv_block3_filters</th><th style=\"text-align: right;\">  conv_block4_filters</th><th style=\"text-align: right;\">  conv_block5_filters</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  fc1_units</th><th>fc_layer_type  </th><th style=\"text-align: right;\">    lr</th><th>pool_type  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_1f8470da</td><td>RUNNING </td><td>172.28.0.2:9286</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                   64</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">           0.2</td><td style=\"text-align: right;\">        128</td><td>dense          </td><td style=\"text-align: right;\">0.0001</td><td>average    </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/5 [=====>........................] - ETA: 13s - loss: 0.7632 - accuracy: 0.3750 - f1_m: 0.3333 - precision_m: 0.2778 - recall_m: 0.4167\n",
            "2/5 [===========>..................] - ETA: 1s - loss: 0.7411 - accuracy: 0.4375 - f1_m: 0.4444 - precision_m: 0.4722 - recall_m: 0.4464 \n",
            "3/5 [=================>............] - ETA: 0s - loss: 0.7211 - accuracy: 0.4688 - f1_m: 0.4781 - precision_m: 0.5148 - recall_m: 0.4643\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7192 - accuracy: 0.4688 - f1_m: 0.4872 - precision_m: 0.5267 - recall_m: 0.4666\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.7071 - accuracy: 0.4931 - f1_m: 0.5309 - precision_m: 0.5714 - recall_m: 0.5066\n",
            "Result for train_mnist_1f8470da:\n",
            "  accuracy: 0.4930555522441864\n",
            "  date: 2022-05-24_17-01-32\n",
            "  done: false\n",
            "  experiment_id: d6e86483659d4663bcfa83e15a865a64\n",
            "  hostname: bec2c43fd2a7\n",
            "  iterations_since_restore: 1\n",
            "  loss: 0.7070592045783997\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9286\n",
            "  time_since_restore: 12.901546955108643\n",
            "  time_this_iter_s: 12.901546955108643\n",
            "  time_total_s: 12.901546955108643\n",
            "  timestamp: 1653411692\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 1f8470da\n",
            "  val_accuracy: 0.45945945382118225\n",
            "  val_loss: 0.6933807730674744\n",
            "  warmup_time: 0.0038542747497558594\n",
            "  \n",
            "5/5 [==============================] - 5s 491ms/step - loss: 0.7071 - accuracy: 0.4931 - f1_m: 0.5309 - precision_m: 0.5714 - recall_m: 0.5066 - val_loss: 0.6934 - val_accuracy: 0.4595 - val_f1_m: 0.6793 - val_precision_m: 0.5188 - val_recall_m: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-24 17:01:32,994\tINFO trial_runner.py:803 -- starting train_mnist_1f8470da\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m 2022-05-24 17:01:36,499\tINFO trainable.py:535 -- Restored on 172.28.0.2 from checkpoint: /root/ray_results/train_mnist_2022-05-24_17-01-15/train_mnist_1f8470da_1_conv_block1_filters=128,conv_block2_filters=256,conv_block3_filters=128,conv_block4_filters=64,conv_block5__2022-05-24_17-01-15/checkpoint_tmpbe77d7/./\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m 2022-05-24 17:01:36,499\tINFO trainable.py:543 -- Current state after restoring: {'_iteration': 0, '_timesteps_total': 0, '_time_total': 12.901546955108643, '_episodes_total': 0}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "== Status ==<br>Current time: 2022-05-24 17:01:38 (running for 00:00:22.25)<br>Memory usage on this node: 2.2/12.7 GiB<br>Using HyperBand: num_stopped=0 total_brackets=1\n",
              "Round #0:\n",
              "  Bracket(Max Size (n)=27, Milestone (r)=3, completed=0.2%): {RUNNING: 1} <br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.32 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/train_mnist_2022-05-24_17-01-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name          </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  conv_block1_filters</th><th style=\"text-align: right;\">  conv_block2_filters</th><th style=\"text-align: right;\">  conv_block3_filters</th><th style=\"text-align: right;\">  conv_block4_filters</th><th style=\"text-align: right;\">  conv_block5_filters</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  fc1_units</th><th>fc_layer_type  </th><th style=\"text-align: right;\">    lr</th><th>pool_type  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  val_loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_1f8470da</td><td>RUNNING </td><td>172.28.0.2:9286</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                   64</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">           0.2</td><td style=\"text-align: right;\">        128</td><td>dense          </td><td style=\"text-align: right;\">0.0001</td><td>average    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.9015</td><td style=\"text-align: right;\">0.707059</td><td style=\"text-align: right;\">  0.693381</td><td style=\"text-align: right;\">  0.493056</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "== Status ==<br>Current time: 2022-05-24 17:01:43 (running for 00:00:27.26)<br>Memory usage on this node: 2.2/12.7 GiB<br>Using HyperBand: num_stopped=0 total_brackets=1\n",
              "Round #0:\n",
              "  Bracket(Max Size (n)=27, Milestone (r)=3, completed=0.2%): {RUNNING: 1} <br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.32 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/train_mnist_2022-05-24_17-01-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name          </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  conv_block1_filters</th><th style=\"text-align: right;\">  conv_block2_filters</th><th style=\"text-align: right;\">  conv_block3_filters</th><th style=\"text-align: right;\">  conv_block4_filters</th><th style=\"text-align: right;\">  conv_block5_filters</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  fc1_units</th><th>fc_layer_type  </th><th style=\"text-align: right;\">    lr</th><th>pool_type  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  val_loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_1f8470da</td><td>RUNNING </td><td>172.28.0.2:9286</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                   64</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">           0.2</td><td style=\"text-align: right;\">        128</td><td>dense          </td><td style=\"text-align: right;\">0.0001</td><td>average    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.9015</td><td style=\"text-align: right;\">0.707059</td><td style=\"text-align: right;\">  0.693381</td><td style=\"text-align: right;\">  0.493056</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m 2022-05-24 17:01:43.834783: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m Model: \"model\"\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m _________________________________________________________________\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  Layer (type)                Output Shape              Param #   \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m =================================================================\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  input_1 (InputLayer)        [(None, 400, 1)]          0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  conv1d (Conv1D)             (None, 398, 128)          512       \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  batch_normalization (BatchN  (None, 398, 128)         512       \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  ormalization)                                                   \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  activation (Activation)     (None, 398, 128)          0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  conv1d_1 (Conv1D)           (None, 396, 256)          98560     \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  batch_normalization_1 (Batc  (None, 396, 256)         1024      \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  hNormalization)                                                 \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  activation_1 (Activation)   (None, 396, 256)          0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  conv1d_2 (Conv1D)           (None, 394, 128)          98432     \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  batch_normalization_2 (Batc  (None, 394, 128)         512       \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  hNormalization)                                                 \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  activation_2 (Activation)   (None, 394, 128)          0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  conv1d_3 (Conv1D)           (None, 393, 64)           16448     \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  batch_normalization_3 (Batc  (None, 393, 64)          256       \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  hNormalization)                                                 \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  activation_3 (Activation)   (None, 393, 64)           0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  conv1d_4 (Conv1D)           (None, 393, 256)          16640     \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  batch_normalization_4 (Batc  (None, 393, 256)         1024      \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  hNormalization)                                                 \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  activation_4 (Activation)   (None, 393, 256)          0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  global_average_pooling1d (G  (None, 256)              0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  lobalAveragePooling1D)                                          \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  dense (Dense)               (None, 128)               32896     \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  dropout (Dropout)           (None, 128)               0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  batch_normalization_5 (Batc  (None, 128)              512       \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  hNormalization)                                                 \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  activation_5 (Activation)   (None, 128)               0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  dense_1 (Dense)             (None, 2)                 258       \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  dropout_1 (Dropout)         (None, 2)                 0         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  batch_normalization_6 (Batc  (None, 2)                8         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  hNormalization)                                                 \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m  dense_2 (Dense)             (None, 1)                 3         \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m                                                                  \n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m =================================================================\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m Total params: 267,597\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m Trainable params: 265,673\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m Non-trainable params: 1,924\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m _________________________________________________________________\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m None\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m Total number of layers: 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function f1_m at 0x7f51e51a5050> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m Cause: Unable to locate the source code of <function f1_m at 0x7f51e51a5050>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function precision_m at 0x7f51e51ad830> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m Cause: Unable to locate the source code of <function precision_m at 0x7f51e51ad830>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function recall_m at 0x7f517e0e2f80> and will run it as-is.\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m Cause: Unable to locate the source code of <function recall_m at 0x7f517e0e2f80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/5 [=====>........................] - ETA: 12s - loss: 0.7299 - accuracy: 0.4688 - f1_m: 0.5405 - precision_m: 0.6250 - recall_m: 0.4762\n",
            "2/5 [===========>..................] - ETA: 1s - loss: 0.7022 - accuracy: 0.5312 - f1_m: 0.5846 - precision_m: 0.6181 - recall_m: 0.5616 \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "== Status ==<br>Current time: 2022-05-24 17:01:48 (running for 00:00:32.28)<br>Memory usage on this node: 2.2/12.7 GiB<br>Using HyperBand: num_stopped=0 total_brackets=1\n",
              "Round #0:\n",
              "  Bracket(Max Size (n)=27, Milestone (r)=3, completed=0.2%): {RUNNING: 1} <br>Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.32 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/train_mnist_2022-05-24_17-01-15<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name          </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  conv_block1_filters</th><th style=\"text-align: right;\">  conv_block2_filters</th><th style=\"text-align: right;\">  conv_block3_filters</th><th style=\"text-align: right;\">  conv_block4_filters</th><th style=\"text-align: right;\">  conv_block5_filters</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  fc1_units</th><th>fc_layer_type  </th><th style=\"text-align: right;\">    lr</th><th>pool_type  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  val_loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_1f8470da</td><td>RUNNING </td><td>172.28.0.2:9286</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                   64</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">           0.2</td><td style=\"text-align: right;\">        128</td><td>dense          </td><td style=\"text-align: right;\">0.0001</td><td>average    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.9015</td><td style=\"text-align: right;\">0.707059</td><td style=\"text-align: right;\">  0.693381</td><td style=\"text-align: right;\">  0.493056</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/5 [=================>............] - ETA: 0s - loss: 0.7131 - accuracy: 0.5417 - f1_m: 0.5772 - precision_m: 0.6263 - recall_m: 0.5411\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7063 - accuracy: 0.5781 - f1_m: 0.5936 - precision_m: 0.6197 - recall_m: 0.5789\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.7330 - accuracy: 0.5556 - f1_m: 0.5499 - precision_m: 0.5958 - recall_m: 0.5231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-24 17:01:49,267\tINFO hyperband.py:467 -- Restoring from a previous point in time. Previous=1; Now=1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for train_mnist_1f8470da:\n",
            "  accuracy: 0.5555555820465088\n",
            "  date: 2022-05-24_17-01-49\n",
            "  done: false\n",
            "  episodes_total: 0\n",
            "  experiment_id: d6e86483659d4663bcfa83e15a865a64\n",
            "  hostname: bec2c43fd2a7\n",
            "  iterations_since_restore: 1\n",
            "  loss: 0.73299241065979\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9354\n",
            "  time_since_restore: 12.765836954116821\n",
            "  time_this_iter_s: 12.765836954116821\n",
            "  time_total_s: 25.667383909225464\n",
            "  timestamp: 1653411709\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 1f8470da\n",
            "  val_accuracy: 0.6756756901741028\n",
            "  val_loss: 0.6930209398269653\n",
            "  warmup_time: 0.004525899887084961\n",
            "  \n",
            "Result for train_mnist_1f8470da:\n",
            "  accuracy: 0.5555555820465088\n",
            "  date: 2022-05-24_17-01-49\n",
            "  done: true\n",
            "  episodes_total: 0\n",
            "  experiment_id: d6e86483659d4663bcfa83e15a865a64\n",
            "  experiment_tag: 1_conv_block1_filters=128,conv_block2_filters=256,conv_block3_filters=128,conv_block4_filters=64,conv_block5_filters=256,dropout_rate=0.2,fc1_units=128,fc_layer_type=dense,lr=0.0001,pool_type=average\n",
            "  hostname: bec2c43fd2a7\n",
            "  iterations_since_restore: 1\n",
            "  loss: 0.73299241065979\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9354\n",
            "  time_since_restore: 12.765836954116821\n",
            "  time_this_iter_s: 12.765836954116821\n",
            "  time_total_s: 25.667383909225464\n",
            "  timestamp: 1653411709\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 1f8470da\n",
            "  val_accuracy: 0.6756756901741028\n",
            "  val_loss: 0.6930209398269653\n",
            "  warmup_time: 0.004525899887084961\n",
            "  \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "== Status ==<br>Current time: 2022-05-24 17:01:49 (running for 00:00:33.53)<br>Memory usage on this node: 2.1/12.7 GiB<br>Using HyperBand: num_stopped=0 total_brackets=1\n",
              "Round #0:\n",
              "  Bracket(Max Size (n)=9, Milestone (r)=9, completed=0.2%): {TERMINATED: 1} <br>Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.32 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/train_mnist_2022-05-24_17-01-15<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name          </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  conv_block1_filters</th><th style=\"text-align: right;\">  conv_block2_filters</th><th style=\"text-align: right;\">  conv_block3_filters</th><th style=\"text-align: right;\">  conv_block4_filters</th><th style=\"text-align: right;\">  conv_block5_filters</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  fc1_units</th><th>fc_layer_type  </th><th style=\"text-align: right;\">    lr</th><th>pool_type  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  val_loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_1f8470da</td><td>TERMINATED</td><td>172.28.0.2:9354</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">                  128</td><td style=\"text-align: right;\">                   64</td><td style=\"text-align: right;\">                  256</td><td style=\"text-align: right;\">           0.2</td><td style=\"text-align: right;\">        128</td><td>dense          </td><td style=\"text-align: right;\">0.0001</td><td>average    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         25.6674</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0.732992</td><td style=\"text-align: right;\">  0.693021</td><td style=\"text-align: right;\">  0.555556</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-24 17:01:49,404\tINFO tune.py:702 -- Total run time: 33.69 seconds (33.52 seconds for the tuning loop).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(train_mnist pid=9354)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5/5 [==============================] - 5s 493ms/step - loss: 0.7330 - accuracy: 0.5556 - f1_m: 0.5499 - precision_m: 0.5958 - recall_m: 0.5231 - val_loss: 0.6930 - val_accuracy: 0.6757 - val_f1_m: 0.5448 - val_precision_m: 0.5833 - val_recall_m: 0.5238\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f0f16bc3ad0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
        "from ray.tune.suggest.bohb import TuneBOHB\n",
        "\n",
        "algo = TuneBOHB(\n",
        "    config_space, metric=\"val_accuracy\", mode=\"max\")\n",
        "bohb = HyperBandForBOHB(\n",
        "    time_attr=\"training_iteration\",\n",
        "    metric=\"val_accuracy\",\n",
        "    mode=\"max\",\n",
        "    max_t=100)\n",
        "tune.run(train_mnist, scheduler=bohb, search_alg=algo)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RayTune_400x1_acc=0_675_1D_Bayesian_Opt.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
