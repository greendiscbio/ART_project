{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcxzx8CA6ZBk",
        "outputId": "249d03ca-36f3-4c83-c3d9-da600d558add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKbzHHTTN6hf"
      },
      "source": [
        "# 1. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "igiBGczI6OQB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "np.random.seed(5)\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7rURrT964u6",
        "outputId": "efcdf100-5d00-47a3-bd28-a9adb6fb20ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HahZouMFOD5L"
      },
      "source": [
        "# 2. Read and store data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "avdjw-cJ6REy"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/UPM/Internship/Clinical_data_and_RNA_total_Features_PFS.csv\")\n",
        "# data=data.reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "Ybaw-Ethi1mc",
        "outputId": "1d539d10-674a-4ef1-d354-30de6af34de7"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0  Cohort                                RNA_ID  Sex  Age  \\\n",
              "0             0       1              P66432-07E-Run1_S20_L002    1 -1.0   \n",
              "1             1       1              P66451-08H-Run1_S26_L002    1 -1.0   \n",
              "2             2       1              P66291-06C-Run1_S26_L002    1 -1.0   \n",
              "3             3       1                              EA595529    0 -1.0   \n",
              "4             4       1              P66425-10B-Run1_S14_L001    1 -1.0   \n",
              "..          ...     ...                                   ...  ...  ...   \n",
              "176         176       1                              EA595597    0 -1.0   \n",
              "177         177       1               P66282-01G-Run1_S3_L001    1 -1.0   \n",
              "178         178       1               P66287-07B-Run1_S2_L001    0 -1.0   \n",
              "179         179       1               P66291-01B-Run1_S2_L001    0 -1.0   \n",
              "180         180       0  G138701_RCCBMS-00141-T_v1_RNA_OnPrem    0 -1.0   \n",
              "\n",
              "     MSKCC  IMDC  Sarc  Rhab  Sarc_or_Rhab  ...    ZWILCH     ZWINT      ZXDA  \\\n",
              "0        0     1   0.0   1.0           1.0  ...  31.34679  30.75900  31.42843   \n",
              "1        1     1   0.0   0.0           0.0  ...  30.87430  31.05250  31.80081   \n",
              "2        0     0   0.0   0.0           0.0  ...  31.25167  31.44371  31.51154   \n",
              "3        0     0  -1.0  -1.0          -1.0  ...  33.89329  31.52823  31.37139   \n",
              "4        1     1   0.0   0.0           0.0  ...  31.34078  31.20506  30.44983   \n",
              "..     ...   ...   ...   ...           ...  ...       ...       ...       ...   \n",
              "176      1     1   0.0   0.0           0.0  ...  31.79512  29.54665  30.23505   \n",
              "177      1     0   0.0   1.0           1.0  ...  31.35633  30.63953  30.91397   \n",
              "178      1     1   0.0   0.0           0.0  ...  31.42083  30.26672  31.70032   \n",
              "179      1     1   0.0   0.0           0.0  ...  31.12877  31.52142  30.97558   \n",
              "180      0    -1   0.0   0.0           0.0  ...  30.74474  31.95812  30.73898   \n",
              "\n",
              "         ZXDB      ZXDC    ZYG11A    ZYG11B       ZYX     ZZEF1      ZZZ3  \n",
              "0    31.08705  32.71107  25.90385  31.67474  34.12401  34.32899  32.14828  \n",
              "1    30.73640  32.28674  21.07951  32.48797  33.79261  34.00829  32.13593  \n",
              "2    30.21717  32.46077  25.01312  32.26571  34.29307  34.64321  32.03359  \n",
              "3    31.67742  32.38818  27.13897  32.69528  33.70234  32.16150  33.38502  \n",
              "4    30.39079  31.93868  21.07951  31.34827  34.48915  33.57104  30.52599  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "176  30.31045  31.37708  26.69069  32.42070  34.35072  34.08129  32.67870  \n",
              "177  30.80688  31.80574  21.07951  31.98301  34.71904  33.83702  32.75734  \n",
              "178  30.50743  31.69798  30.53831  32.88405  33.83220  35.05889  32.80163  \n",
              "179  30.63280  31.89907  27.25043  32.08933  34.30456  33.96936  32.18444  \n",
              "180  30.15898  31.69473  26.33098  32.68330  33.96047  32.97135  32.20607  \n",
              "\n",
              "[181 rows x 43910 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6f4f1bd0-aa4f-41fd-af68-39ba7ef4d175\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Cohort</th>\n",
              "      <th>RNA_ID</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>MSKCC</th>\n",
              "      <th>IMDC</th>\n",
              "      <th>Sarc</th>\n",
              "      <th>Rhab</th>\n",
              "      <th>Sarc_or_Rhab</th>\n",
              "      <th>...</th>\n",
              "      <th>ZWILCH</th>\n",
              "      <th>ZWINT</th>\n",
              "      <th>ZXDA</th>\n",
              "      <th>ZXDB</th>\n",
              "      <th>ZXDC</th>\n",
              "      <th>ZYG11A</th>\n",
              "      <th>ZYG11B</th>\n",
              "      <th>ZYX</th>\n",
              "      <th>ZZEF1</th>\n",
              "      <th>ZZZ3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>P66432-07E-Run1_S20_L002</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>31.34679</td>\n",
              "      <td>30.75900</td>\n",
              "      <td>31.42843</td>\n",
              "      <td>31.08705</td>\n",
              "      <td>32.71107</td>\n",
              "      <td>25.90385</td>\n",
              "      <td>31.67474</td>\n",
              "      <td>34.12401</td>\n",
              "      <td>34.32899</td>\n",
              "      <td>32.14828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>P66451-08H-Run1_S26_L002</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>30.87430</td>\n",
              "      <td>31.05250</td>\n",
              "      <td>31.80081</td>\n",
              "      <td>30.73640</td>\n",
              "      <td>32.28674</td>\n",
              "      <td>21.07951</td>\n",
              "      <td>32.48797</td>\n",
              "      <td>33.79261</td>\n",
              "      <td>34.00829</td>\n",
              "      <td>32.13593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>P66291-06C-Run1_S26_L002</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>31.25167</td>\n",
              "      <td>31.44371</td>\n",
              "      <td>31.51154</td>\n",
              "      <td>30.21717</td>\n",
              "      <td>32.46077</td>\n",
              "      <td>25.01312</td>\n",
              "      <td>32.26571</td>\n",
              "      <td>34.29307</td>\n",
              "      <td>34.64321</td>\n",
              "      <td>32.03359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>EA595529</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>33.89329</td>\n",
              "      <td>31.52823</td>\n",
              "      <td>31.37139</td>\n",
              "      <td>31.67742</td>\n",
              "      <td>32.38818</td>\n",
              "      <td>27.13897</td>\n",
              "      <td>32.69528</td>\n",
              "      <td>33.70234</td>\n",
              "      <td>32.16150</td>\n",
              "      <td>33.38502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>P66425-10B-Run1_S14_L001</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>31.34078</td>\n",
              "      <td>31.20506</td>\n",
              "      <td>30.44983</td>\n",
              "      <td>30.39079</td>\n",
              "      <td>31.93868</td>\n",
              "      <td>21.07951</td>\n",
              "      <td>31.34827</td>\n",
              "      <td>34.48915</td>\n",
              "      <td>33.57104</td>\n",
              "      <td>30.52599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>176</td>\n",
              "      <td>1</td>\n",
              "      <td>EA595597</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>31.79512</td>\n",
              "      <td>29.54665</td>\n",
              "      <td>30.23505</td>\n",
              "      <td>30.31045</td>\n",
              "      <td>31.37708</td>\n",
              "      <td>26.69069</td>\n",
              "      <td>32.42070</td>\n",
              "      <td>34.35072</td>\n",
              "      <td>34.08129</td>\n",
              "      <td>32.67870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>177</td>\n",
              "      <td>1</td>\n",
              "      <td>P66282-01G-Run1_S3_L001</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>31.35633</td>\n",
              "      <td>30.63953</td>\n",
              "      <td>30.91397</td>\n",
              "      <td>30.80688</td>\n",
              "      <td>31.80574</td>\n",
              "      <td>21.07951</td>\n",
              "      <td>31.98301</td>\n",
              "      <td>34.71904</td>\n",
              "      <td>33.83702</td>\n",
              "      <td>32.75734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>178</td>\n",
              "      <td>1</td>\n",
              "      <td>P66287-07B-Run1_S2_L001</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>31.42083</td>\n",
              "      <td>30.26672</td>\n",
              "      <td>31.70032</td>\n",
              "      <td>30.50743</td>\n",
              "      <td>31.69798</td>\n",
              "      <td>30.53831</td>\n",
              "      <td>32.88405</td>\n",
              "      <td>33.83220</td>\n",
              "      <td>35.05889</td>\n",
              "      <td>32.80163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>179</td>\n",
              "      <td>1</td>\n",
              "      <td>P66291-01B-Run1_S2_L001</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>31.12877</td>\n",
              "      <td>31.52142</td>\n",
              "      <td>30.97558</td>\n",
              "      <td>30.63280</td>\n",
              "      <td>31.89907</td>\n",
              "      <td>27.25043</td>\n",
              "      <td>32.08933</td>\n",
              "      <td>34.30456</td>\n",
              "      <td>33.96936</td>\n",
              "      <td>32.18444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>180</td>\n",
              "      <td>0</td>\n",
              "      <td>G138701_RCCBMS-00141-T_v1_RNA_OnPrem</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>30.74474</td>\n",
              "      <td>31.95812</td>\n",
              "      <td>30.73898</td>\n",
              "      <td>30.15898</td>\n",
              "      <td>31.69473</td>\n",
              "      <td>26.33098</td>\n",
              "      <td>32.68330</td>\n",
              "      <td>33.96047</td>\n",
              "      <td>32.97135</td>\n",
              "      <td>32.20607</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>181 rows × 43910 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6f4f1bd0-aa4f-41fd-af68-39ba7ef4d175')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6f4f1bd0-aa4f-41fd-af68-39ba7ef4d175 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6f4f1bd0-aa4f-41fd-af68-39ba7ef4d175');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "b6quQhpas_7k"
      },
      "outputs": [],
      "source": [
        "data = data.iloc[:,16:43911] "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "G9JElOQhi7g8",
        "outputId": "fab0080f-ecb6-432f-e014-fd0c90163691"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Target    5S_rRNA        7SK       A1BG   A1BG-AS1       A1CF        A2M  \\\n",
              "0       NR  20.980824  37.282634  32.394764  28.870768  31.258914  35.951604   \n",
              "1       NR  20.980824  38.258157  30.965497  23.859635  32.949781  37.521337   \n",
              "2       NR  20.980824  38.188964  31.930133  28.485719  26.168791  36.015727   \n",
              "3       NR  21.026286  36.605262  32.546553  26.604504  31.235049  36.850599   \n",
              "4       NR  20.980824  35.832174  33.406547  24.918545  32.777268  38.545393   \n",
              "..     ...        ...        ...        ...        ...        ...        ...   \n",
              "176      R  21.026286  37.564794  31.436146  28.223177  29.279089  37.581987   \n",
              "177      R  20.980824  36.103756  31.742840  28.947436  27.082321  38.180753   \n",
              "178      R  20.980824  36.899816  24.438962  21.286965  34.362365  36.996222   \n",
              "179      R  20.980824  36.500470  30.264581  25.252020  33.043464  36.728673   \n",
              "180      R  21.038652  37.141292  30.092851  26.933385  26.684228  36.851946   \n",
              "\n",
              "       A2M-AS1      A2ML1  A2ML1-AS1  ...    ZWILCH     ZWINT      ZXDA  \\\n",
              "0    26.832775  26.323583  20.980667  ...  31.34679  30.75900  31.42843   \n",
              "1    27.417967  24.845262  20.980667  ...  30.87430  31.05250  31.80081   \n",
              "2    25.608517  28.564779  20.980667  ...  31.25167  31.44371  31.51154   \n",
              "3    24.398145  27.577558  20.976688  ...  33.89329  31.52823  31.37139   \n",
              "4    21.357557  24.892587  20.980667  ...  31.34078  31.20506  30.44983   \n",
              "..         ...        ...        ...  ...       ...       ...       ...   \n",
              "176  26.919700  26.091240  20.976688  ...  31.79512  29.54665  30.23505   \n",
              "177  24.669716  20.860170  20.980667  ...  31.35633  30.63953  30.91397   \n",
              "178  21.357557  20.860170  20.980667  ...  31.42083  30.26672  31.70032   \n",
              "179  27.583360  26.983274  20.980667  ...  31.12877  31.52142  30.97558   \n",
              "180  22.705126  30.406352  21.076515  ...  30.74474  31.95812  30.73898   \n",
              "\n",
              "         ZXDB      ZXDC    ZYG11A    ZYG11B       ZYX     ZZEF1      ZZZ3  \n",
              "0    31.08705  32.71107  25.90385  31.67474  34.12401  34.32899  32.14828  \n",
              "1    30.73640  32.28674  21.07951  32.48797  33.79261  34.00829  32.13593  \n",
              "2    30.21717  32.46077  25.01312  32.26571  34.29307  34.64321  32.03359  \n",
              "3    31.67742  32.38818  27.13897  32.69528  33.70234  32.16150  33.38502  \n",
              "4    30.39079  31.93868  21.07951  31.34827  34.48915  33.57104  30.52599  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "176  30.31045  31.37708  26.69069  32.42070  34.35072  34.08129  32.67870  \n",
              "177  30.80688  31.80574  21.07951  31.98301  34.71904  33.83702  32.75734  \n",
              "178  30.50743  31.69798  30.53831  32.88405  33.83220  35.05889  32.80163  \n",
              "179  30.63280  31.89907  27.25043  32.08933  34.30456  33.96936  32.18444  \n",
              "180  30.15898  31.69473  26.33098  32.68330  33.96047  32.97135  32.20607  \n",
              "\n",
              "[181 rows x 43894 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e79eb54-4559-4fee-83b2-3a90d3afe515\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>5S_rRNA</th>\n",
              "      <th>7SK</th>\n",
              "      <th>A1BG</th>\n",
              "      <th>A1BG-AS1</th>\n",
              "      <th>A1CF</th>\n",
              "      <th>A2M</th>\n",
              "      <th>A2M-AS1</th>\n",
              "      <th>A2ML1</th>\n",
              "      <th>A2ML1-AS1</th>\n",
              "      <th>...</th>\n",
              "      <th>ZWILCH</th>\n",
              "      <th>ZWINT</th>\n",
              "      <th>ZXDA</th>\n",
              "      <th>ZXDB</th>\n",
              "      <th>ZXDC</th>\n",
              "      <th>ZYG11A</th>\n",
              "      <th>ZYG11B</th>\n",
              "      <th>ZYX</th>\n",
              "      <th>ZZEF1</th>\n",
              "      <th>ZZZ3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NR</td>\n",
              "      <td>20.980824</td>\n",
              "      <td>37.282634</td>\n",
              "      <td>32.394764</td>\n",
              "      <td>28.870768</td>\n",
              "      <td>31.258914</td>\n",
              "      <td>35.951604</td>\n",
              "      <td>26.832775</td>\n",
              "      <td>26.323583</td>\n",
              "      <td>20.980667</td>\n",
              "      <td>...</td>\n",
              "      <td>31.34679</td>\n",
              "      <td>30.75900</td>\n",
              "      <td>31.42843</td>\n",
              "      <td>31.08705</td>\n",
              "      <td>32.71107</td>\n",
              "      <td>25.90385</td>\n",
              "      <td>31.67474</td>\n",
              "      <td>34.12401</td>\n",
              "      <td>34.32899</td>\n",
              "      <td>32.14828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NR</td>\n",
              "      <td>20.980824</td>\n",
              "      <td>38.258157</td>\n",
              "      <td>30.965497</td>\n",
              "      <td>23.859635</td>\n",
              "      <td>32.949781</td>\n",
              "      <td>37.521337</td>\n",
              "      <td>27.417967</td>\n",
              "      <td>24.845262</td>\n",
              "      <td>20.980667</td>\n",
              "      <td>...</td>\n",
              "      <td>30.87430</td>\n",
              "      <td>31.05250</td>\n",
              "      <td>31.80081</td>\n",
              "      <td>30.73640</td>\n",
              "      <td>32.28674</td>\n",
              "      <td>21.07951</td>\n",
              "      <td>32.48797</td>\n",
              "      <td>33.79261</td>\n",
              "      <td>34.00829</td>\n",
              "      <td>32.13593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NR</td>\n",
              "      <td>20.980824</td>\n",
              "      <td>38.188964</td>\n",
              "      <td>31.930133</td>\n",
              "      <td>28.485719</td>\n",
              "      <td>26.168791</td>\n",
              "      <td>36.015727</td>\n",
              "      <td>25.608517</td>\n",
              "      <td>28.564779</td>\n",
              "      <td>20.980667</td>\n",
              "      <td>...</td>\n",
              "      <td>31.25167</td>\n",
              "      <td>31.44371</td>\n",
              "      <td>31.51154</td>\n",
              "      <td>30.21717</td>\n",
              "      <td>32.46077</td>\n",
              "      <td>25.01312</td>\n",
              "      <td>32.26571</td>\n",
              "      <td>34.29307</td>\n",
              "      <td>34.64321</td>\n",
              "      <td>32.03359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NR</td>\n",
              "      <td>21.026286</td>\n",
              "      <td>36.605262</td>\n",
              "      <td>32.546553</td>\n",
              "      <td>26.604504</td>\n",
              "      <td>31.235049</td>\n",
              "      <td>36.850599</td>\n",
              "      <td>24.398145</td>\n",
              "      <td>27.577558</td>\n",
              "      <td>20.976688</td>\n",
              "      <td>...</td>\n",
              "      <td>33.89329</td>\n",
              "      <td>31.52823</td>\n",
              "      <td>31.37139</td>\n",
              "      <td>31.67742</td>\n",
              "      <td>32.38818</td>\n",
              "      <td>27.13897</td>\n",
              "      <td>32.69528</td>\n",
              "      <td>33.70234</td>\n",
              "      <td>32.16150</td>\n",
              "      <td>33.38502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NR</td>\n",
              "      <td>20.980824</td>\n",
              "      <td>35.832174</td>\n",
              "      <td>33.406547</td>\n",
              "      <td>24.918545</td>\n",
              "      <td>32.777268</td>\n",
              "      <td>38.545393</td>\n",
              "      <td>21.357557</td>\n",
              "      <td>24.892587</td>\n",
              "      <td>20.980667</td>\n",
              "      <td>...</td>\n",
              "      <td>31.34078</td>\n",
              "      <td>31.20506</td>\n",
              "      <td>30.44983</td>\n",
              "      <td>30.39079</td>\n",
              "      <td>31.93868</td>\n",
              "      <td>21.07951</td>\n",
              "      <td>31.34827</td>\n",
              "      <td>34.48915</td>\n",
              "      <td>33.57104</td>\n",
              "      <td>30.52599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>R</td>\n",
              "      <td>21.026286</td>\n",
              "      <td>37.564794</td>\n",
              "      <td>31.436146</td>\n",
              "      <td>28.223177</td>\n",
              "      <td>29.279089</td>\n",
              "      <td>37.581987</td>\n",
              "      <td>26.919700</td>\n",
              "      <td>26.091240</td>\n",
              "      <td>20.976688</td>\n",
              "      <td>...</td>\n",
              "      <td>31.79512</td>\n",
              "      <td>29.54665</td>\n",
              "      <td>30.23505</td>\n",
              "      <td>30.31045</td>\n",
              "      <td>31.37708</td>\n",
              "      <td>26.69069</td>\n",
              "      <td>32.42070</td>\n",
              "      <td>34.35072</td>\n",
              "      <td>34.08129</td>\n",
              "      <td>32.67870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>R</td>\n",
              "      <td>20.980824</td>\n",
              "      <td>36.103756</td>\n",
              "      <td>31.742840</td>\n",
              "      <td>28.947436</td>\n",
              "      <td>27.082321</td>\n",
              "      <td>38.180753</td>\n",
              "      <td>24.669716</td>\n",
              "      <td>20.860170</td>\n",
              "      <td>20.980667</td>\n",
              "      <td>...</td>\n",
              "      <td>31.35633</td>\n",
              "      <td>30.63953</td>\n",
              "      <td>30.91397</td>\n",
              "      <td>30.80688</td>\n",
              "      <td>31.80574</td>\n",
              "      <td>21.07951</td>\n",
              "      <td>31.98301</td>\n",
              "      <td>34.71904</td>\n",
              "      <td>33.83702</td>\n",
              "      <td>32.75734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>R</td>\n",
              "      <td>20.980824</td>\n",
              "      <td>36.899816</td>\n",
              "      <td>24.438962</td>\n",
              "      <td>21.286965</td>\n",
              "      <td>34.362365</td>\n",
              "      <td>36.996222</td>\n",
              "      <td>21.357557</td>\n",
              "      <td>20.860170</td>\n",
              "      <td>20.980667</td>\n",
              "      <td>...</td>\n",
              "      <td>31.42083</td>\n",
              "      <td>30.26672</td>\n",
              "      <td>31.70032</td>\n",
              "      <td>30.50743</td>\n",
              "      <td>31.69798</td>\n",
              "      <td>30.53831</td>\n",
              "      <td>32.88405</td>\n",
              "      <td>33.83220</td>\n",
              "      <td>35.05889</td>\n",
              "      <td>32.80163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>R</td>\n",
              "      <td>20.980824</td>\n",
              "      <td>36.500470</td>\n",
              "      <td>30.264581</td>\n",
              "      <td>25.252020</td>\n",
              "      <td>33.043464</td>\n",
              "      <td>36.728673</td>\n",
              "      <td>27.583360</td>\n",
              "      <td>26.983274</td>\n",
              "      <td>20.980667</td>\n",
              "      <td>...</td>\n",
              "      <td>31.12877</td>\n",
              "      <td>31.52142</td>\n",
              "      <td>30.97558</td>\n",
              "      <td>30.63280</td>\n",
              "      <td>31.89907</td>\n",
              "      <td>27.25043</td>\n",
              "      <td>32.08933</td>\n",
              "      <td>34.30456</td>\n",
              "      <td>33.96936</td>\n",
              "      <td>32.18444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>R</td>\n",
              "      <td>21.038652</td>\n",
              "      <td>37.141292</td>\n",
              "      <td>30.092851</td>\n",
              "      <td>26.933385</td>\n",
              "      <td>26.684228</td>\n",
              "      <td>36.851946</td>\n",
              "      <td>22.705126</td>\n",
              "      <td>30.406352</td>\n",
              "      <td>21.076515</td>\n",
              "      <td>...</td>\n",
              "      <td>30.74474</td>\n",
              "      <td>31.95812</td>\n",
              "      <td>30.73898</td>\n",
              "      <td>30.15898</td>\n",
              "      <td>31.69473</td>\n",
              "      <td>26.33098</td>\n",
              "      <td>32.68330</td>\n",
              "      <td>33.96047</td>\n",
              "      <td>32.97135</td>\n",
              "      <td>32.20607</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>181 rows × 43894 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e79eb54-4559-4fee-83b2-3a90d3afe515')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e79eb54-4559-4fee-83b2-3a90d3afe515 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e79eb54-4559-4fee-83b2-3a90d3afe515');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRXkCY3uOPEI"
      },
      "source": [
        "# 3. Data pre-processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBc4JlRpOhvn"
      },
      "source": [
        "### 3.3 Normalize data (each column to the max value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "wXuour7n75KX"
      },
      "outputs": [],
      "source": [
        "for i in data[:0].drop(['Target'], axis=1):\n",
        "    # print(i)\n",
        "    data[i] = data[i] /data[i].abs().max() \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "FoqZ_bL1jTZZ",
        "outputId": "f73717b3-b900-40bd-88e0-1eee868b433e"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Target   5S_rRNA       7SK      A1BG  A1BG-AS1      A1CF       A2M  \\\n",
              "0       NR  0.714991  0.932338  0.921685  0.918377  0.868816  0.920867   \n",
              "1       NR  0.714991  0.956733  0.881020  0.758973  0.915812  0.961075   \n",
              "2       NR  0.714991  0.955003  0.908466  0.906128  0.727340  0.922510   \n",
              "3       NR  0.716540  0.915399  0.926004  0.846287  0.868152  0.943894   \n",
              "4       NR  0.714991  0.896066  0.950472  0.792657  0.911017  0.987305   \n",
              "..     ...       ...       ...       ...       ...       ...       ...   \n",
              "176      R  0.716540  0.939394  0.894411  0.897777  0.813788  0.962628   \n",
              "177      R  0.714991  0.902858  0.903137  0.920815  0.752731  0.977965   \n",
              "178      R  0.714991  0.922765  0.695329  0.677136  0.955074  0.947624   \n",
              "179      R  0.714991  0.912778  0.861078  0.803264  0.918416  0.940771   \n",
              "180      R  0.716961  0.928804  0.856192  0.856749  0.741666  0.943929   \n",
              "\n",
              "      A2M-AS1     A2ML1  A2ML1-AS1  ...    ZWILCH     ZWINT      ZXDA  \\\n",
              "0    0.900190  0.777727   0.995452  ...  0.923409  0.924559  0.959142   \n",
              "1    0.919822  0.734050   0.995452  ...  0.909491  0.933381  0.970506   \n",
              "2    0.859118  0.843943   0.995452  ...  0.920607  0.945140  0.961678   \n",
              "3    0.818512  0.814775   0.995264  ...  0.998424  0.947680  0.957401   \n",
              "4    0.716506  0.735448   0.995452  ...  0.923232  0.937966  0.929277   \n",
              "..        ...       ...        ...  ...       ...       ...       ...   \n",
              "176  0.903106  0.770862   0.995264  ...  0.936616  0.888118  0.922722   \n",
              "177  0.827623  0.616311   0.995452  ...  0.923690  0.920968  0.943441   \n",
              "178  0.716506  0.616311   0.995452  ...  0.925590  0.909762  0.967439   \n",
              "179  0.925370  0.797217   0.995452  ...  0.916987  0.947475  0.945322   \n",
              "180  0.761715  0.898352   1.000000  ...  0.905674  0.960602  0.938101   \n",
              "\n",
              "         ZXDB      ZXDC    ZYG11A    ZYG11B       ZYX     ZZEF1      ZZZ3  \n",
              "0    0.972976  0.978546  0.848241  0.937432  0.942274  0.971082  0.945915  \n",
              "1    0.962001  0.965852  0.690264  0.961500  0.933123  0.962010  0.945552  \n",
              "2    0.945750  0.971058  0.819073  0.954922  0.946943  0.979970  0.942541  \n",
              "3    0.991454  0.968886  0.888686  0.967635  0.930631  0.909769  0.982305  \n",
              "4    0.951184  0.955440  0.690264  0.927770  0.952357  0.949641  0.898182  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "176  0.948670  0.938640  0.874007  0.959509  0.948534  0.964075  0.961522  \n",
              "177  0.964207  0.951463  0.690264  0.946555  0.958705  0.957165  0.963836  \n",
              "178  0.954835  0.948239  1.000000  0.973222  0.934216  0.991729  0.965139  \n",
              "179  0.958759  0.954255  0.892336  0.949702  0.947260  0.960909  0.946979  \n",
              "180  0.943929  0.948142  0.862228  0.967281  0.937758  0.932678  0.947616  \n",
              "\n",
              "[181 rows x 43894 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f86a4436-cfa2-45d4-a3e1-d8c79db51d29\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>5S_rRNA</th>\n",
              "      <th>7SK</th>\n",
              "      <th>A1BG</th>\n",
              "      <th>A1BG-AS1</th>\n",
              "      <th>A1CF</th>\n",
              "      <th>A2M</th>\n",
              "      <th>A2M-AS1</th>\n",
              "      <th>A2ML1</th>\n",
              "      <th>A2ML1-AS1</th>\n",
              "      <th>...</th>\n",
              "      <th>ZWILCH</th>\n",
              "      <th>ZWINT</th>\n",
              "      <th>ZXDA</th>\n",
              "      <th>ZXDB</th>\n",
              "      <th>ZXDC</th>\n",
              "      <th>ZYG11A</th>\n",
              "      <th>ZYG11B</th>\n",
              "      <th>ZYX</th>\n",
              "      <th>ZZEF1</th>\n",
              "      <th>ZZZ3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NR</td>\n",
              "      <td>0.714991</td>\n",
              "      <td>0.932338</td>\n",
              "      <td>0.921685</td>\n",
              "      <td>0.918377</td>\n",
              "      <td>0.868816</td>\n",
              "      <td>0.920867</td>\n",
              "      <td>0.900190</td>\n",
              "      <td>0.777727</td>\n",
              "      <td>0.995452</td>\n",
              "      <td>...</td>\n",
              "      <td>0.923409</td>\n",
              "      <td>0.924559</td>\n",
              "      <td>0.959142</td>\n",
              "      <td>0.972976</td>\n",
              "      <td>0.978546</td>\n",
              "      <td>0.848241</td>\n",
              "      <td>0.937432</td>\n",
              "      <td>0.942274</td>\n",
              "      <td>0.971082</td>\n",
              "      <td>0.945915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NR</td>\n",
              "      <td>0.714991</td>\n",
              "      <td>0.956733</td>\n",
              "      <td>0.881020</td>\n",
              "      <td>0.758973</td>\n",
              "      <td>0.915812</td>\n",
              "      <td>0.961075</td>\n",
              "      <td>0.919822</td>\n",
              "      <td>0.734050</td>\n",
              "      <td>0.995452</td>\n",
              "      <td>...</td>\n",
              "      <td>0.909491</td>\n",
              "      <td>0.933381</td>\n",
              "      <td>0.970506</td>\n",
              "      <td>0.962001</td>\n",
              "      <td>0.965852</td>\n",
              "      <td>0.690264</td>\n",
              "      <td>0.961500</td>\n",
              "      <td>0.933123</td>\n",
              "      <td>0.962010</td>\n",
              "      <td>0.945552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NR</td>\n",
              "      <td>0.714991</td>\n",
              "      <td>0.955003</td>\n",
              "      <td>0.908466</td>\n",
              "      <td>0.906128</td>\n",
              "      <td>0.727340</td>\n",
              "      <td>0.922510</td>\n",
              "      <td>0.859118</td>\n",
              "      <td>0.843943</td>\n",
              "      <td>0.995452</td>\n",
              "      <td>...</td>\n",
              "      <td>0.920607</td>\n",
              "      <td>0.945140</td>\n",
              "      <td>0.961678</td>\n",
              "      <td>0.945750</td>\n",
              "      <td>0.971058</td>\n",
              "      <td>0.819073</td>\n",
              "      <td>0.954922</td>\n",
              "      <td>0.946943</td>\n",
              "      <td>0.979970</td>\n",
              "      <td>0.942541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NR</td>\n",
              "      <td>0.716540</td>\n",
              "      <td>0.915399</td>\n",
              "      <td>0.926004</td>\n",
              "      <td>0.846287</td>\n",
              "      <td>0.868152</td>\n",
              "      <td>0.943894</td>\n",
              "      <td>0.818512</td>\n",
              "      <td>0.814775</td>\n",
              "      <td>0.995264</td>\n",
              "      <td>...</td>\n",
              "      <td>0.998424</td>\n",
              "      <td>0.947680</td>\n",
              "      <td>0.957401</td>\n",
              "      <td>0.991454</td>\n",
              "      <td>0.968886</td>\n",
              "      <td>0.888686</td>\n",
              "      <td>0.967635</td>\n",
              "      <td>0.930631</td>\n",
              "      <td>0.909769</td>\n",
              "      <td>0.982305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NR</td>\n",
              "      <td>0.714991</td>\n",
              "      <td>0.896066</td>\n",
              "      <td>0.950472</td>\n",
              "      <td>0.792657</td>\n",
              "      <td>0.911017</td>\n",
              "      <td>0.987305</td>\n",
              "      <td>0.716506</td>\n",
              "      <td>0.735448</td>\n",
              "      <td>0.995452</td>\n",
              "      <td>...</td>\n",
              "      <td>0.923232</td>\n",
              "      <td>0.937966</td>\n",
              "      <td>0.929277</td>\n",
              "      <td>0.951184</td>\n",
              "      <td>0.955440</td>\n",
              "      <td>0.690264</td>\n",
              "      <td>0.927770</td>\n",
              "      <td>0.952357</td>\n",
              "      <td>0.949641</td>\n",
              "      <td>0.898182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>R</td>\n",
              "      <td>0.716540</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.894411</td>\n",
              "      <td>0.897777</td>\n",
              "      <td>0.813788</td>\n",
              "      <td>0.962628</td>\n",
              "      <td>0.903106</td>\n",
              "      <td>0.770862</td>\n",
              "      <td>0.995264</td>\n",
              "      <td>...</td>\n",
              "      <td>0.936616</td>\n",
              "      <td>0.888118</td>\n",
              "      <td>0.922722</td>\n",
              "      <td>0.948670</td>\n",
              "      <td>0.938640</td>\n",
              "      <td>0.874007</td>\n",
              "      <td>0.959509</td>\n",
              "      <td>0.948534</td>\n",
              "      <td>0.964075</td>\n",
              "      <td>0.961522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>R</td>\n",
              "      <td>0.714991</td>\n",
              "      <td>0.902858</td>\n",
              "      <td>0.903137</td>\n",
              "      <td>0.920815</td>\n",
              "      <td>0.752731</td>\n",
              "      <td>0.977965</td>\n",
              "      <td>0.827623</td>\n",
              "      <td>0.616311</td>\n",
              "      <td>0.995452</td>\n",
              "      <td>...</td>\n",
              "      <td>0.923690</td>\n",
              "      <td>0.920968</td>\n",
              "      <td>0.943441</td>\n",
              "      <td>0.964207</td>\n",
              "      <td>0.951463</td>\n",
              "      <td>0.690264</td>\n",
              "      <td>0.946555</td>\n",
              "      <td>0.958705</td>\n",
              "      <td>0.957165</td>\n",
              "      <td>0.963836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>R</td>\n",
              "      <td>0.714991</td>\n",
              "      <td>0.922765</td>\n",
              "      <td>0.695329</td>\n",
              "      <td>0.677136</td>\n",
              "      <td>0.955074</td>\n",
              "      <td>0.947624</td>\n",
              "      <td>0.716506</td>\n",
              "      <td>0.616311</td>\n",
              "      <td>0.995452</td>\n",
              "      <td>...</td>\n",
              "      <td>0.925590</td>\n",
              "      <td>0.909762</td>\n",
              "      <td>0.967439</td>\n",
              "      <td>0.954835</td>\n",
              "      <td>0.948239</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.973222</td>\n",
              "      <td>0.934216</td>\n",
              "      <td>0.991729</td>\n",
              "      <td>0.965139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>R</td>\n",
              "      <td>0.714991</td>\n",
              "      <td>0.912778</td>\n",
              "      <td>0.861078</td>\n",
              "      <td>0.803264</td>\n",
              "      <td>0.918416</td>\n",
              "      <td>0.940771</td>\n",
              "      <td>0.925370</td>\n",
              "      <td>0.797217</td>\n",
              "      <td>0.995452</td>\n",
              "      <td>...</td>\n",
              "      <td>0.916987</td>\n",
              "      <td>0.947475</td>\n",
              "      <td>0.945322</td>\n",
              "      <td>0.958759</td>\n",
              "      <td>0.954255</td>\n",
              "      <td>0.892336</td>\n",
              "      <td>0.949702</td>\n",
              "      <td>0.947260</td>\n",
              "      <td>0.960909</td>\n",
              "      <td>0.946979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>R</td>\n",
              "      <td>0.716961</td>\n",
              "      <td>0.928804</td>\n",
              "      <td>0.856192</td>\n",
              "      <td>0.856749</td>\n",
              "      <td>0.741666</td>\n",
              "      <td>0.943929</td>\n",
              "      <td>0.761715</td>\n",
              "      <td>0.898352</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.905674</td>\n",
              "      <td>0.960602</td>\n",
              "      <td>0.938101</td>\n",
              "      <td>0.943929</td>\n",
              "      <td>0.948142</td>\n",
              "      <td>0.862228</td>\n",
              "      <td>0.967281</td>\n",
              "      <td>0.937758</td>\n",
              "      <td>0.932678</td>\n",
              "      <td>0.947616</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>181 rows × 43894 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f86a4436-cfa2-45d4-a3e1-d8c79db51d29')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f86a4436-cfa2-45d4-a3e1-d8c79db51d29 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f86a4436-cfa2-45d4-a3e1-d8c79db51d29');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "sL8Nvj-o77gl"
      },
      "outputs": [],
      "source": [
        "data.to_csv(\"/content/drive/MyDrive/UPM/data_standarized.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iegQKieFOrME"
      },
      "source": [
        "# 4. Data split into Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "4fn8ILqh792r"
      },
      "outputs": [],
      "source": [
        "#144 NR\n",
        "#167 R\n",
        "X_train, X_test = train_test_split(data, test_size=0.2, random_state=125)\n",
        "X_train = X_train.drop(['Target'], axis=1)\n",
        "X_train = X_train.values\n",
        "\n",
        "Y_test = X_test['Target']\n",
        "X_test = X_test.drop(['Target'], axis=1)\n",
        "X_test = X_test.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVgwImz2EoV_",
        "outputId": "6aba3014-cc0f-47e3-e62f-34dbd0cfd06e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.71694061 0.87589736 0.77157163 ... 0.94260617 0.97703959 0.94053507]\n",
            " [0.71499066 0.92372434 0.87674569 ... 0.94166456 0.95621658 0.94721745]\n",
            " [0.71499066 0.92328074 0.87560615 ... 0.94956774 0.94415563 0.95044962]\n",
            " ...\n",
            " [0.71499066 0.89721919 0.80781399 ... 0.95826646 0.94562913 0.95655529]\n",
            " [0.71694061 0.91377997 0.91250475 ... 0.97155726 0.95168124 0.9087562 ]\n",
            " [0.71653993 0.92027979 0.88738279 ... 0.95222717 0.95548252 0.93984862]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "DYrvrbKdD_eK"
      },
      "outputs": [],
      "source": [
        "np.savetxt(\"/content/drive/MyDrive/UPM/input_data.csv\", X_train, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Qwk3oHO7HI"
      },
      "source": [
        "# 5. Autoencoder model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "WUYGDW1z7_f8"
      },
      "outputs": [],
      "source": [
        "dim_entrada = X_train.shape[1]\n",
        "capa_entrada = Input(shape=(dim_entrada,))\n",
        "#15-65 neuronas por llayer y entre 1 y 4 layers\n",
        "\n",
        "encoder = Dense(438, activation='relu')(capa_entrada) #relu for encoder sigmoid for decoders\n",
        "encoder = Dense(43, activation='relu')(encoder)\n",
        "decoder = Dense(438, activation='sigmoid')(encoder)\n",
        "decoder = Dense(43893 , activation='sigmoid')(decoder)\n",
        "autoencoder = Model(inputs=capa_entrada, outputs=decoder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TF_X4u0P72-"
      },
      "source": [
        "### 5.1. Loss function definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "uczyfAfu8A3X"
      },
      "outputs": [],
      "source": [
        "autoencoder.compile(optimizer='adam', loss='mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ8V-FZCPHWy"
      },
      "source": [
        "# 6. Autoencoder model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2n_6O_O8COj",
        "outputId": "b7755a73-dd5d-47ba-b388-19a3c20b7109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "5/5 [==============================] - 3s 412ms/step - loss: 0.0926 - val_loss: 0.0357\n",
            "Epoch 2/200\n",
            "5/5 [==============================] - 2s 410ms/step - loss: 0.0195 - val_loss: 0.0062\n",
            "Epoch 3/200\n",
            "5/5 [==============================] - 2s 394ms/step - loss: 0.0056 - val_loss: 0.0059\n",
            "Epoch 4/200\n",
            "5/5 [==============================] - 2s 363ms/step - loss: 0.0058 - val_loss: 0.0058\n",
            "Epoch 5/200\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0054 - val_loss: 0.0051\n",
            "Epoch 6/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0046 - val_loss: 0.0045\n",
            "Epoch 7/200\n",
            "5/5 [==============================] - 2s 354ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 8/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Epoch 9/200\n",
            "5/5 [==============================] - 2s 349ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 10/200\n",
            "5/5 [==============================] - 2s 350ms/step - loss: 0.0032 - val_loss: 0.0033\n",
            "Epoch 11/200\n",
            "5/5 [==============================] - 2s 349ms/step - loss: 0.0031 - val_loss: 0.0032\n",
            "Epoch 12/200\n",
            "5/5 [==============================] - 2s 351ms/step - loss: 0.0030 - val_loss: 0.0031\n",
            "Epoch 13/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0029 - val_loss: 0.0031\n",
            "Epoch 14/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0029 - val_loss: 0.0031\n",
            "Epoch 15/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0029 - val_loss: 0.0031\n",
            "Epoch 16/200\n",
            "5/5 [==============================] - 2s 354ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 17/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 18/200\n",
            "5/5 [==============================] - 2s 349ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 19/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 20/200\n",
            "5/5 [==============================] - 2s 354ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 21/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 22/200\n",
            "5/5 [==============================] - 2s 348ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 23/200\n",
            "5/5 [==============================] - 2s 351ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 24/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 25/200\n",
            "5/5 [==============================] - 2s 359ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 26/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 27/200\n",
            "5/5 [==============================] - 3s 553ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 28/200\n",
            "5/5 [==============================] - 3s 625ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 29/200\n",
            "5/5 [==============================] - 2s 349ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 30/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 31/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 32/200\n",
            "5/5 [==============================] - 2s 350ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 33/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 34/200\n",
            "5/5 [==============================] - 2s 359ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 35/200\n",
            "5/5 [==============================] - 2s 363ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 36/200\n",
            "5/5 [==============================] - 2s 370ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 37/200\n",
            "5/5 [==============================] - 2s 351ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 38/200\n",
            "5/5 [==============================] - 2s 482ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 39/200\n",
            "5/5 [==============================] - 2s 438ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 40/200\n",
            "5/5 [==============================] - 3s 604ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 41/200\n",
            "5/5 [==============================] - 2s 457ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 42/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 43/200\n",
            "5/5 [==============================] - 2s 359ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 44/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 45/200\n",
            "5/5 [==============================] - 2s 351ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 46/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 47/200\n",
            "5/5 [==============================] - 2s 347ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 48/200\n",
            "5/5 [==============================] - 2s 351ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 49/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 50/200\n",
            "5/5 [==============================] - 2s 347ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 51/200\n",
            "5/5 [==============================] - 2s 359ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 52/200\n",
            "5/5 [==============================] - 2s 367ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 53/200\n",
            "5/5 [==============================] - 2s 364ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 54/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 55/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 56/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 57/200\n",
            "5/5 [==============================] - 2s 354ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 58/200\n",
            "5/5 [==============================] - 2s 349ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 59/200\n",
            "5/5 [==============================] - 2s 362ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 60/200\n",
            "5/5 [==============================] - 2s 351ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 61/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 62/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 63/200\n",
            "5/5 [==============================] - 2s 361ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 64/200\n",
            "5/5 [==============================] - 2s 363ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 65/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 66/200\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 67/200\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 68/200\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 69/200\n",
            "5/5 [==============================] - 2s 354ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 70/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 71/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 72/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 73/200\n",
            "5/5 [==============================] - 2s 361ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 74/200\n",
            "5/5 [==============================] - 2s 362ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 75/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 76/200\n",
            "5/5 [==============================] - 2s 359ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 77/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 78/200\n",
            "5/5 [==============================] - 2s 349ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 79/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 80/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 81/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 82/200\n",
            "5/5 [==============================] - 2s 364ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 83/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 84/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 85/200\n",
            "5/5 [==============================] - 2s 365ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 86/200\n",
            "5/5 [==============================] - 2s 361ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 87/200\n",
            "5/5 [==============================] - 2s 359ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 88/200\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 89/200\n",
            "5/5 [==============================] - 2s 346ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 90/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 91/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 92/200\n",
            "5/5 [==============================] - 2s 350ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 93/200\n",
            "5/5 [==============================] - 2s 359ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 94/200\n",
            "5/5 [==============================] - 2s 367ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 95/200\n",
            "5/5 [==============================] - 2s 361ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 96/200\n",
            "5/5 [==============================] - 2s 366ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 97/200\n",
            "5/5 [==============================] - 2s 360ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 98/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 99/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 100/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 101/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 102/200\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 103/200\n",
            "5/5 [==============================] - 2s 365ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 104/200\n",
            "5/5 [==============================] - 2s 370ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 105/200\n",
            "5/5 [==============================] - 2s 368ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 106/200\n",
            "5/5 [==============================] - 2s 369ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 107/200\n",
            "5/5 [==============================] - 2s 366ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 108/200\n",
            "5/5 [==============================] - 2s 365ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 109/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 110/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 111/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 112/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 113/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 114/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 115/200\n",
            "5/5 [==============================] - 2s 366ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 116/200\n",
            "5/5 [==============================] - 2s 366ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 117/200\n",
            "5/5 [==============================] - 2s 363ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 118/200\n",
            "5/5 [==============================] - 2s 360ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 119/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 120/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 121/200\n",
            "5/5 [==============================] - 2s 360ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 122/200\n",
            "5/5 [==============================] - 2s 345ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 123/200\n",
            "5/5 [==============================] - 2s 354ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 124/200\n",
            "5/5 [==============================] - 2s 361ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 125/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 126/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 127/200\n",
            "5/5 [==============================] - 2s 363ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 128/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 129/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 130/200\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 131/200\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 132/200\n",
            "5/5 [==============================] - 2s 346ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 133/200\n",
            "5/5 [==============================] - 2s 349ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 134/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 135/200\n",
            "5/5 [==============================] - 2s 349ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 136/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 137/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 138/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 139/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 140/200\n",
            "5/5 [==============================] - 2s 350ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 141/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 142/200\n",
            "5/5 [==============================] - 2s 350ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 143/200\n",
            "5/5 [==============================] - 2s 346ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 144/200\n",
            "5/5 [==============================] - 2s 354ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 145/200\n",
            "5/5 [==============================] - 2s 349ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 146/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 147/200\n",
            "5/5 [==============================] - 2s 368ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 148/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 149/200\n",
            "5/5 [==============================] - 2s 361ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 150/200\n",
            "5/5 [==============================] - 2s 347ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 151/200\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 152/200\n",
            "5/5 [==============================] - 2s 344ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 153/200\n",
            "5/5 [==============================] - 2s 354ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 154/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 155/200\n",
            "5/5 [==============================] - 2s 351ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 156/200\n",
            "5/5 [==============================] - 2s 365ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 157/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 158/200\n",
            "5/5 [==============================] - 2s 354ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 159/200\n",
            "5/5 [==============================] - 2s 360ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 160/200\n",
            "5/5 [==============================] - 2s 350ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 161/200\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 162/200\n",
            "5/5 [==============================] - 2s 349ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 163/200\n",
            "5/5 [==============================] - 2s 350ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 164/200\n",
            "5/5 [==============================] - 2s 350ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 165/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 166/200\n",
            "5/5 [==============================] - 2s 361ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 167/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 168/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 169/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 170/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 171/200\n",
            "5/5 [==============================] - 2s 359ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 172/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 173/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 174/200\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 175/200\n",
            "5/5 [==============================] - 2s 350ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 176/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 177/200\n",
            "5/5 [==============================] - 2s 362ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 178/200\n",
            "5/5 [==============================] - 2s 364ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 179/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 180/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 181/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 182/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 183/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 184/200\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 185/200\n",
            "5/5 [==============================] - 2s 349ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 186/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 187/200\n",
            "5/5 [==============================] - 2s 364ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 188/200\n",
            "5/5 [==============================] - 2s 366ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 189/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 190/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 191/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 192/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 193/200\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 194/200\n",
            "5/5 [==============================] - 2s 359ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 195/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 196/200\n",
            "5/5 [==============================] - 2s 360ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 197/200\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 198/200\n",
            "5/5 [==============================] - 2s 358ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 199/200\n",
            "5/5 [==============================] - 2s 360ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 200/200\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0028 - val_loss: 0.0030\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2d2b9d3390>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "nits = 200 #1000-1500\n",
        "tam_lote = 32\n",
        "\n",
        "autoencoder.fit(X_train, X_train, epochs=nits, batch_size=tam_lote, shuffle=True, validation_data=(X_test,X_test), verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk_ie95gQLfd"
      },
      "source": [
        "# 7. Model validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBPki6PmQRq_"
      },
      "source": [
        "### 7.1 Generate prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "XNsHUGpr8Fpk"
      },
      "outputs": [],
      "source": [
        "X_pred = autoencoder.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8bt4yiiQWQy"
      },
      "source": [
        "### 7.2. Mean squared error (MSE) calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "s7_sYq6B8G4h"
      },
      "outputs": [],
      "source": [
        "MSE = np.mean(np.power(X_test-X_pred,2), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHNWuQr0FwWi",
        "outputId": "cf272697-0976-49ce-cfce-a88a204497c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "print(type(Y_test))\n",
        "Y_test=Y_test.to_numpy()\n",
        "print(type(Y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "565DKiotTBXX"
      },
      "outputs": [],
      "source": [
        "Y_test=np.array(Y_test, dtype=bool)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHlGK_DqpLoE",
        "outputId": "245bf401-6f94-46ba-de38-bdd675635e19"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "V1oK6G0E8LiF",
        "outputId": "38c14104-e4a6-44ca-c995-c6d4ad6287d1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from matplotlib import pyplot as plt\n",
        "precision, recall, umbral = precision_recall_curve(Y_test, MSE)\n",
        "\n",
        "plt.plot(umbral, precision[1:], label=\"Precision\",linewidth=5)\n",
        "plt.plot(umbral, recall[1:], label=\"Recall\",linewidth=5)\n",
        "plt.title('Precision y Recall para diferentes umbrales')\n",
        "plt.xlabel('Umbral')\n",
        "plt.ylabel('Precision/Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "m4PlO8zY8NHO"
      },
      "outputs": [],
      "source": [
        "threshold = 0.002\n",
        "Y_pred = [1 if e > threshold else 0 for e in MSE]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHepbr1F8O5l",
        "outputId": "1143fb46-d6b9-4ebf-9886-d204aa12a64f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy on the testing dataset: 0.8648648648648649\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "match = collections.Counter(Y_pred)[1]\n",
        "print('Final accuracy on the testing dataset: ' + str(match/len(Y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPDkCD4YT7uk",
        "outputId": "528d2c52-1928-4ef1-eacf-070addaeb2f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0]\n",
            " [ 5 32]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "FfRqteHkEHkB"
      },
      "outputs": [],
      "source": [
        "np.savetxt(\"/content/drive/MyDrive/UPM/output_data.csv\", X_train, delimiter=',')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Autoencoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}