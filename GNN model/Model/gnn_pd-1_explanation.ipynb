{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cpu\n",
      "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cpu.html\n",
      "Requirement already satisfied: torch-scatter in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (2.0.9)\n",
      "Requirement already satisfied: torch-sparse in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.6.15)\n",
      "Requirement already satisfied: scipy in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from torch-sparse) (1.7.1)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scipy->torch-sparse) (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cpu.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install -q captum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gene matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACPP</th>\n",
       "      <th>FOLH1</th>\n",
       "      <th>FRAT1</th>\n",
       "      <th>FRAT2</th>\n",
       "      <th>ICOS</th>\n",
       "      <th>ICOSLG</th>\n",
       "      <th>ITK</th>\n",
       "      <th>MTCP1</th>\n",
       "      <th>NFATC1</th>\n",
       "      <th>NFATC2</th>\n",
       "      <th>...</th>\n",
       "      <th>TCF7L1</th>\n",
       "      <th>TCIRG1</th>\n",
       "      <th>TCL1A</th>\n",
       "      <th>TCL1B</th>\n",
       "      <th>TLX1</th>\n",
       "      <th>TLX2</th>\n",
       "      <th>TLX3</th>\n",
       "      <th>WT1</th>\n",
       "      <th>WTAP</th>\n",
       "      <th>WTIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.643264</td>\n",
       "      <td>33.06366</td>\n",
       "      <td>22.86623</td>\n",
       "      <td>25.14807</td>\n",
       "      <td>23.66478</td>\n",
       "      <td>31.25529</td>\n",
       "      <td>31.97483</td>\n",
       "      <td>32.68788</td>\n",
       "      <td>32.00358</td>\n",
       "      <td>31.98820</td>\n",
       "      <td>...</td>\n",
       "      <td>29.73447</td>\n",
       "      <td>33.92966</td>\n",
       "      <td>21.65301</td>\n",
       "      <td>21.31325</td>\n",
       "      <td>25.58066</td>\n",
       "      <td>21.09375</td>\n",
       "      <td>21.21067</td>\n",
       "      <td>27.25894</td>\n",
       "      <td>32.30986</td>\n",
       "      <td>30.89343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.478681</td>\n",
       "      <td>33.34812</td>\n",
       "      <td>27.58122</td>\n",
       "      <td>27.19051</td>\n",
       "      <td>27.81065</td>\n",
       "      <td>29.88798</td>\n",
       "      <td>32.85944</td>\n",
       "      <td>34.62906</td>\n",
       "      <td>30.96356</td>\n",
       "      <td>31.94520</td>\n",
       "      <td>...</td>\n",
       "      <td>31.39434</td>\n",
       "      <td>33.85213</td>\n",
       "      <td>21.65301</td>\n",
       "      <td>21.31325</td>\n",
       "      <td>21.06706</td>\n",
       "      <td>24.61959</td>\n",
       "      <td>21.21067</td>\n",
       "      <td>22.90940</td>\n",
       "      <td>33.64920</td>\n",
       "      <td>32.19936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.556110</td>\n",
       "      <td>32.20052</td>\n",
       "      <td>25.38929</td>\n",
       "      <td>26.69664</td>\n",
       "      <td>26.66294</td>\n",
       "      <td>30.92857</td>\n",
       "      <td>31.09956</td>\n",
       "      <td>33.46376</td>\n",
       "      <td>31.30038</td>\n",
       "      <td>31.04482</td>\n",
       "      <td>...</td>\n",
       "      <td>30.94080</td>\n",
       "      <td>33.53312</td>\n",
       "      <td>26.38498</td>\n",
       "      <td>21.31325</td>\n",
       "      <td>21.06706</td>\n",
       "      <td>21.09375</td>\n",
       "      <td>21.21067</td>\n",
       "      <td>22.90940</td>\n",
       "      <td>35.26323</td>\n",
       "      <td>29.13798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.253267</td>\n",
       "      <td>34.48359</td>\n",
       "      <td>28.10294</td>\n",
       "      <td>26.49687</td>\n",
       "      <td>23.66478</td>\n",
       "      <td>29.47329</td>\n",
       "      <td>31.18813</td>\n",
       "      <td>33.10176</td>\n",
       "      <td>31.65882</td>\n",
       "      <td>32.62476</td>\n",
       "      <td>...</td>\n",
       "      <td>31.69465</td>\n",
       "      <td>32.51771</td>\n",
       "      <td>24.98799</td>\n",
       "      <td>21.31325</td>\n",
       "      <td>21.06706</td>\n",
       "      <td>26.34435</td>\n",
       "      <td>21.21067</td>\n",
       "      <td>30.72576</td>\n",
       "      <td>33.85052</td>\n",
       "      <td>32.61099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.643264</td>\n",
       "      <td>34.68356</td>\n",
       "      <td>27.09929</td>\n",
       "      <td>22.82728</td>\n",
       "      <td>23.66478</td>\n",
       "      <td>32.22363</td>\n",
       "      <td>33.25193</td>\n",
       "      <td>32.65197</td>\n",
       "      <td>32.94580</td>\n",
       "      <td>33.70037</td>\n",
       "      <td>...</td>\n",
       "      <td>30.41687</td>\n",
       "      <td>33.75841</td>\n",
       "      <td>21.65301</td>\n",
       "      <td>21.31325</td>\n",
       "      <td>21.06706</td>\n",
       "      <td>21.09375</td>\n",
       "      <td>21.21067</td>\n",
       "      <td>31.55831</td>\n",
       "      <td>33.25769</td>\n",
       "      <td>29.29437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>29.643765</td>\n",
       "      <td>33.58120</td>\n",
       "      <td>26.78622</td>\n",
       "      <td>27.10293</td>\n",
       "      <td>22.97275</td>\n",
       "      <td>30.82689</td>\n",
       "      <td>26.57659</td>\n",
       "      <td>33.91039</td>\n",
       "      <td>31.93211</td>\n",
       "      <td>31.61476</td>\n",
       "      <td>...</td>\n",
       "      <td>31.55301</td>\n",
       "      <td>32.80839</td>\n",
       "      <td>20.32437</td>\n",
       "      <td>21.31293</td>\n",
       "      <td>21.09326</td>\n",
       "      <td>21.18281</td>\n",
       "      <td>21.37595</td>\n",
       "      <td>33.76553</td>\n",
       "      <td>32.92597</td>\n",
       "      <td>33.36054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>32.256558</td>\n",
       "      <td>31.42896</td>\n",
       "      <td>21.76825</td>\n",
       "      <td>26.01421</td>\n",
       "      <td>29.53775</td>\n",
       "      <td>27.72514</td>\n",
       "      <td>30.11127</td>\n",
       "      <td>34.06419</td>\n",
       "      <td>29.13034</td>\n",
       "      <td>29.95589</td>\n",
       "      <td>...</td>\n",
       "      <td>27.36919</td>\n",
       "      <td>33.87425</td>\n",
       "      <td>25.58961</td>\n",
       "      <td>21.31293</td>\n",
       "      <td>21.09326</td>\n",
       "      <td>21.18281</td>\n",
       "      <td>21.37595</td>\n",
       "      <td>29.72826</td>\n",
       "      <td>35.27110</td>\n",
       "      <td>27.34363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>26.275234</td>\n",
       "      <td>34.13379</td>\n",
       "      <td>26.74567</td>\n",
       "      <td>26.15839</td>\n",
       "      <td>28.98194</td>\n",
       "      <td>31.82216</td>\n",
       "      <td>33.03207</td>\n",
       "      <td>32.87564</td>\n",
       "      <td>32.10593</td>\n",
       "      <td>32.57213</td>\n",
       "      <td>...</td>\n",
       "      <td>31.82891</td>\n",
       "      <td>33.58358</td>\n",
       "      <td>20.32437</td>\n",
       "      <td>21.31293</td>\n",
       "      <td>21.09326</td>\n",
       "      <td>26.68332</td>\n",
       "      <td>21.37595</td>\n",
       "      <td>24.56089</td>\n",
       "      <td>33.08478</td>\n",
       "      <td>32.23764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>31.822287</td>\n",
       "      <td>32.24862</td>\n",
       "      <td>25.46541</td>\n",
       "      <td>22.81349</td>\n",
       "      <td>27.50438</td>\n",
       "      <td>31.74571</td>\n",
       "      <td>31.83385</td>\n",
       "      <td>33.71362</td>\n",
       "      <td>31.09448</td>\n",
       "      <td>32.49485</td>\n",
       "      <td>...</td>\n",
       "      <td>30.01069</td>\n",
       "      <td>34.20252</td>\n",
       "      <td>20.32437</td>\n",
       "      <td>21.31293</td>\n",
       "      <td>21.09326</td>\n",
       "      <td>21.18281</td>\n",
       "      <td>21.37595</td>\n",
       "      <td>29.86003</td>\n",
       "      <td>32.15581</td>\n",
       "      <td>32.35205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>32.443022</td>\n",
       "      <td>26.45839</td>\n",
       "      <td>26.21286</td>\n",
       "      <td>26.00386</td>\n",
       "      <td>29.33309</td>\n",
       "      <td>29.34334</td>\n",
       "      <td>32.18353</td>\n",
       "      <td>32.85734</td>\n",
       "      <td>30.93542</td>\n",
       "      <td>30.80994</td>\n",
       "      <td>...</td>\n",
       "      <td>30.86975</td>\n",
       "      <td>35.10128</td>\n",
       "      <td>26.59326</td>\n",
       "      <td>21.31293</td>\n",
       "      <td>21.09326</td>\n",
       "      <td>27.56092</td>\n",
       "      <td>21.37595</td>\n",
       "      <td>34.54019</td>\n",
       "      <td>32.88606</td>\n",
       "      <td>27.34363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ACPP     FOLH1     FRAT1     FRAT2      ICOS    ICOSLG       ITK  \\\n",
       "0    24.643264  33.06366  22.86623  25.14807  23.66478  31.25529  31.97483   \n",
       "1    30.478681  33.34812  27.58122  27.19051  27.81065  29.88798  32.85944   \n",
       "2    30.556110  32.20052  25.38929  26.69664  26.66294  30.92857  31.09956   \n",
       "3    30.253267  34.48359  28.10294  26.49687  23.66478  29.47329  31.18813   \n",
       "4    24.643264  34.68356  27.09929  22.82728  23.66478  32.22363  33.25193   \n",
       "..         ...       ...       ...       ...       ...       ...       ...   \n",
       "176  29.643765  33.58120  26.78622  27.10293  22.97275  30.82689  26.57659   \n",
       "177  32.256558  31.42896  21.76825  26.01421  29.53775  27.72514  30.11127   \n",
       "178  26.275234  34.13379  26.74567  26.15839  28.98194  31.82216  33.03207   \n",
       "179  31.822287  32.24862  25.46541  22.81349  27.50438  31.74571  31.83385   \n",
       "180  32.443022  26.45839  26.21286  26.00386  29.33309  29.34334  32.18353   \n",
       "\n",
       "        MTCP1    NFATC1    NFATC2  ...    TCF7L1    TCIRG1     TCL1A  \\\n",
       "0    32.68788  32.00358  31.98820  ...  29.73447  33.92966  21.65301   \n",
       "1    34.62906  30.96356  31.94520  ...  31.39434  33.85213  21.65301   \n",
       "2    33.46376  31.30038  31.04482  ...  30.94080  33.53312  26.38498   \n",
       "3    33.10176  31.65882  32.62476  ...  31.69465  32.51771  24.98799   \n",
       "4    32.65197  32.94580  33.70037  ...  30.41687  33.75841  21.65301   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "176  33.91039  31.93211  31.61476  ...  31.55301  32.80839  20.32437   \n",
       "177  34.06419  29.13034  29.95589  ...  27.36919  33.87425  25.58961   \n",
       "178  32.87564  32.10593  32.57213  ...  31.82891  33.58358  20.32437   \n",
       "179  33.71362  31.09448  32.49485  ...  30.01069  34.20252  20.32437   \n",
       "180  32.85734  30.93542  30.80994  ...  30.86975  35.10128  26.59326   \n",
       "\n",
       "        TCL1B      TLX1      TLX2      TLX3       WT1      WTAP      WTIP  \n",
       "0    21.31325  25.58066  21.09375  21.21067  27.25894  32.30986  30.89343  \n",
       "1    21.31325  21.06706  24.61959  21.21067  22.90940  33.64920  32.19936  \n",
       "2    21.31325  21.06706  21.09375  21.21067  22.90940  35.26323  29.13798  \n",
       "3    21.31325  21.06706  26.34435  21.21067  30.72576  33.85052  32.61099  \n",
       "4    21.31325  21.06706  21.09375  21.21067  31.55831  33.25769  29.29437  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "176  21.31293  21.09326  21.18281  21.37595  33.76553  32.92597  33.36054  \n",
       "177  21.31293  21.09326  21.18281  21.37595  29.72826  35.27110  27.34363  \n",
       "178  21.31293  21.09326  26.68332  21.37595  24.56089  33.08478  32.23764  \n",
       "179  21.31293  21.09326  21.18281  21.37595  29.86003  32.15581  32.35205  \n",
       "180  21.31293  21.09326  27.56092  21.37595  34.54019  32.88606  27.34363  \n",
       "\n",
       "[181 rows x 38 columns]"
      ]
     },
     "execution_count": 1109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes = pd.read_csv('../Data/Programmed cell death protein/Programmed_cell_death_protein_matrix.csv')\n",
    "Y = genes.Y\n",
    "\n",
    "genes = genes.iloc[:,1:39] \n",
    "genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='../Data/Programmed cell death protein/network_edges_pd-1.tsv'\n",
    "data = pd.read_csv(path, delimiter='\\t')\n",
    "edge_index1=data['#node1'].to_numpy()\n",
    "edge_index2=data['node2'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 1111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(edge_index1)\n",
    "len(list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index1 = le.transform(edge_index1)\n",
    "edge_index2 = le.transform(edge_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = [edge_index1]+[edge_index2]\n",
    "edge_index = np.array(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  5,  5,  1,  1,  2,  3,  4,  4,  4,  4,  6,  7,  7,  8,\n",
       "         8,  8,  8,  9,  9,  9,  9, 10, 11, 11, 11, 12, 12, 13, 13, 13,\n",
       "        14, 14, 14, 14, 15, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 17,\n",
       "        17, 18, 18, 19, 20, 21, 22, 22, 22, 22, 22, 22, 23, 23, 24, 24,\n",
       "        25, 25, 25, 25, 26, 26, 27, 27, 27, 28, 29, 30, 30, 31, 31, 32,\n",
       "        32, 32, 33, 34, 34, 35, 35, 35, 36, 37],\n",
       "       [22, 19,  4, 17, 24, 22,  3,  2, 18,  6, 17,  5,  4, 31, 30, 29,\n",
       "        11, 17,  9, 11, 10, 12,  8,  9, 12,  8,  9, 11,  9, 16, 14, 15,\n",
       "        22, 13, 15, 16, 22, 13, 14, 16, 22, 13, 14, 15,  4,  8, 27,  5,\n",
       "        18,  4, 17,  0, 21, 20,  1, 24, 15, 14,  0, 16, 25, 27,  1, 22,\n",
       "        33, 23, 34, 32, 34, 32, 28, 17, 23, 27,  8, 31,  7,  7, 30, 25,\n",
       "        35, 26, 25, 25, 26, 32, 37, 36, 35, 35]])"
      ]
     },
     "execution_count": 1114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  5,  5,  1,  1,  2,  3,  4,  4,  4,  4,  6,  7,  7,  8,  8,  8,\n",
       "          8,  9,  9,  9,  9, 10, 11, 11, 11, 12, 12, 13, 13, 13, 14, 14, 14, 14,\n",
       "         15, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 17, 17, 18, 18, 19, 20, 21,\n",
       "         22, 22, 22, 22, 22, 22, 23, 23, 24, 24, 25, 25, 25, 25, 26, 26, 27, 27,\n",
       "         27, 28, 29, 30, 30, 31, 31, 32, 32, 32, 33, 34, 34, 35, 35, 35, 36, 37],\n",
       "        [22, 19,  4, 17, 24, 22,  3,  2, 18,  6, 17,  5,  4, 31, 30, 29, 11, 17,\n",
       "          9, 11, 10, 12,  8,  9, 12,  8,  9, 11,  9, 16, 14, 15, 22, 13, 15, 16,\n",
       "         22, 13, 14, 16, 22, 13, 14, 15,  4,  8, 27,  5, 18,  4, 17,  0, 21, 20,\n",
       "          1, 24, 15, 14,  0, 16, 25, 27,  1, 22, 33, 23, 34, 32, 34, 32, 28, 17,\n",
       "         23, 27,  8, 31,  7,  7, 30, 25, 35, 26, 25, 25, 26, 32, 37, 36, 35, 35]])"
      ]
     },
     "execution_count": 1115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edge_index, dtype=torch.int64)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[38], edge_index=[2, 90], y=[1, 1])\n"
     ]
    }
   ],
   "source": [
    "list_data=[]\n",
    "\n",
    "for g in range(len(genes)):\n",
    "  b=[]\n",
    "  for i in genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    a.append(i*10)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.float).reshape([-1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y.iloc[g]], dtype=torch.float).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  list_data.append(data)\n",
    "\n",
    "print(list_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([246.4326, 330.6366, 228.6623, 251.4807, 236.6478, 312.5529, 319.7483,\n",
       "        326.8788, 320.0358, 319.8820, 316.0401, 326.9664, 308.3347, 215.0007,\n",
       "        211.8392, 211.6984, 212.8552, 288.4472, 302.3462, 347.1497, 211.1579,\n",
       "        210.5495, 219.4579, 219.5682, 253.3625, 271.7087, 215.2616, 310.1699,\n",
       "        297.3447, 339.2966, 216.5301, 213.1325, 255.8066, 210.9375, 212.1067,\n",
       "        272.5894, 323.0986, 308.9343])"
      ]
     },
     "execution_count": 1117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_data[0].x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Patient sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 38\n",
      "Number of charcateristics per node: 1\n",
      "Number of edges: 90\n",
      "Average node degree: 2.37\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "data = list_data[0]\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of charcateristics per node: {data.num_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Train-Test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 124\n",
      "Number of test graphs: 57\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "# random.shuffle(list_data)\n",
    "train_dataset = list_data[0:124]\n",
    "test_dataset = list_data[124:182]\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[2432], edge_index=[2, 5760], y=[64, 1], batch=[2432], ptr=[65])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 60\n",
      "DataBatch(x=[2280], edge_index=[2, 5400], y=[60, 1], batch=[2280], ptr=[61])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv, global_add_pool\n",
    "from torch_geometric.nn import TopKPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch import nn\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 38\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.dim = dim\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GraphConv(embed_dim, dim)\n",
    "        self.pool1 = TopKPooling(dim, ratio=0.8)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.pool2 = TopKPooling(dim, ratio=0.8)\n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=368, embedding_dim=embed_dim)\n",
    "        self.lin1 = torch.nn.Linear(38, 19)\n",
    "        self.lin3 = torch.nn.Linear(19, 1)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = torch.tensor(x).to(torch.int)\n",
    "        x = self.item_embedding(x)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        # x = F.relu(self.conv2(x, edge_index))\n",
    "        # x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        # x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 #+ x2\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = torch.sigmoid(self.lin3(x)).squeeze(1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(output, data.y.squeeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        for i in range(len(output)):\n",
    "            if output[i]>0.5:\n",
    "                output[i]=1\n",
    "            else:\n",
    "                output[i]=0\n",
    "            if output[i]==data.y[i]:\n",
    "                correct=correct+1\n",
    "    # print(\"Correct: \"+str(correct) +\" of \"+str(len(loader.dataset)))\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): GraphConv(38, 19)\n",
      "  (pool1): TopKPooling(19, ratio=0.8, multiplier=1.0)\n",
      "  (conv2): GraphConv(19, 19)\n",
      "  (pool2): TopKPooling(19, ratio=0.8, multiplier=1.0)\n",
      "  (item_embedding): Embedding(368, 38)\n",
      "  (lin1): Linear(in_features=38, out_features=19, bias=True)\n",
      "  (lin3): Linear(in_features=19, out_features=1, bias=True)\n",
      "  (act1): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_15936/3374044401.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x).to(torch.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6928, Train Acc: 0.5403, Test Acc: 0.5088\n",
      "Epoch: 002, Loss: 0.7007, Train Acc: 0.5403, Test Acc: 0.5088\n",
      "Epoch: 003, Loss: 0.6970, Train Acc: 0.5403, Test Acc: 0.5088\n",
      "Epoch: 004, Loss: 0.6861, Train Acc: 0.5403, Test Acc: 0.5088\n",
      "Epoch: 005, Loss: 0.6783, Train Acc: 0.5403, Test Acc: 0.5088\n",
      "Epoch: 006, Loss: 0.6807, Train Acc: 0.5403, Test Acc: 0.5088\n",
      "Epoch: 007, Loss: 0.6756, Train Acc: 0.5403, Test Acc: 0.5088\n",
      "Epoch: 008, Loss: 0.6808, Train Acc: 0.5403, Test Acc: 0.5088\n",
      "Epoch: 009, Loss: 0.6753, Train Acc: 0.5484, Test Acc: 0.5088\n",
      "Epoch: 010, Loss: 0.6759, Train Acc: 0.5484, Test Acc: 0.5088\n",
      "Epoch: 011, Loss: 0.6786, Train Acc: 0.5484, Test Acc: 0.5088\n",
      "Epoch: 012, Loss: 0.6651, Train Acc: 0.5484, Test Acc: 0.5088\n",
      "Epoch: 013, Loss: 0.6655, Train Acc: 0.5484, Test Acc: 0.5088\n",
      "Epoch: 014, Loss: 0.6785, Train Acc: 0.5484, Test Acc: 0.5088\n",
      "Epoch: 015, Loss: 0.6720, Train Acc: 0.5484, Test Acc: 0.5088\n",
      "Epoch: 016, Loss: 0.6579, Train Acc: 0.5484, Test Acc: 0.5088\n",
      "Epoch: 017, Loss: 0.6657, Train Acc: 0.5726, Test Acc: 0.5088\n",
      "Epoch: 018, Loss: 0.6628, Train Acc: 0.5726, Test Acc: 0.5088\n",
      "Epoch: 019, Loss: 0.6582, Train Acc: 0.5887, Test Acc: 0.5088\n",
      "Epoch: 020, Loss: 0.6502, Train Acc: 0.6129, Test Acc: 0.5088\n",
      "Epoch: 021, Loss: 0.6430, Train Acc: 0.6129, Test Acc: 0.5088\n",
      "Epoch: 022, Loss: 0.6390, Train Acc: 0.6129, Test Acc: 0.5088\n",
      "Epoch: 023, Loss: 0.6425, Train Acc: 0.6129, Test Acc: 0.5088\n",
      "Epoch: 024, Loss: 0.6254, Train Acc: 0.6210, Test Acc: 0.5088\n",
      "Epoch: 025, Loss: 0.6246, Train Acc: 0.6290, Test Acc: 0.5088\n",
      "Epoch: 026, Loss: 0.6212, Train Acc: 0.6290, Test Acc: 0.5088\n",
      "Epoch: 027, Loss: 0.6086, Train Acc: 0.6290, Test Acc: 0.5088\n",
      "Epoch: 028, Loss: 0.6126, Train Acc: 0.6774, Test Acc: 0.5088\n",
      "Epoch: 029, Loss: 0.6101, Train Acc: 0.6935, Test Acc: 0.5439\n",
      "Epoch: 030, Loss: 0.5916, Train Acc: 0.7177, Test Acc: 0.5439\n",
      "Epoch: 031, Loss: 0.5951, Train Acc: 0.7661, Test Acc: 0.5439\n",
      "Epoch: 032, Loss: 0.5638, Train Acc: 0.7661, Test Acc: 0.5614\n",
      "Epoch: 033, Loss: 0.5694, Train Acc: 0.7742, Test Acc: 0.5614\n",
      "Epoch: 034, Loss: 0.5405, Train Acc: 0.7742, Test Acc: 0.5965\n",
      "Epoch: 035, Loss: 0.5396, Train Acc: 0.7903, Test Acc: 0.6140\n",
      "Epoch: 036, Loss: 0.5426, Train Acc: 0.8306, Test Acc: 0.6140\n",
      "Epoch: 037, Loss: 0.5150, Train Acc: 0.8790, Test Acc: 0.5965\n",
      "Epoch: 038, Loss: 0.5091, Train Acc: 0.8952, Test Acc: 0.6140\n",
      "Epoch: 039, Loss: 0.5289, Train Acc: 0.9355, Test Acc: 0.6140\n",
      "Epoch: 040, Loss: 0.4997, Train Acc: 0.9435, Test Acc: 0.6316\n",
      "Epoch: 041, Loss: 0.4734, Train Acc: 0.9435, Test Acc: 0.6140\n",
      "Epoch: 042, Loss: 0.4692, Train Acc: 0.9597, Test Acc: 0.5965\n",
      "Epoch: 043, Loss: 0.4507, Train Acc: 0.9597, Test Acc: 0.5965\n",
      "Epoch: 044, Loss: 0.4135, Train Acc: 0.9597, Test Acc: 0.5965\n",
      "Epoch: 045, Loss: 0.4182, Train Acc: 0.9516, Test Acc: 0.5965\n",
      "Epoch: 046, Loss: 0.4295, Train Acc: 0.9597, Test Acc: 0.6667\n",
      "Epoch: 047, Loss: 0.3842, Train Acc: 0.9597, Test Acc: 0.6667\n",
      "Epoch: 048, Loss: 0.3727, Train Acc: 0.9677, Test Acc: 0.6491\n",
      "Epoch: 049, Loss: 0.3508, Train Acc: 0.9677, Test Acc: 0.6667\n",
      "Epoch: 050, Loss: 0.3476, Train Acc: 0.9677, Test Acc: 0.6491\n",
      "Epoch: 051, Loss: 0.3549, Train Acc: 0.9758, Test Acc: 0.6491\n",
      "Epoch: 052, Loss: 0.3343, Train Acc: 0.9758, Test Acc: 0.6491\n",
      "Epoch: 053, Loss: 0.2958, Train Acc: 0.9839, Test Acc: 0.6491\n",
      "Epoch: 054, Loss: 0.2897, Train Acc: 0.9839, Test Acc: 0.6491\n",
      "Epoch: 055, Loss: 0.2635, Train Acc: 0.9839, Test Acc: 0.6316\n",
      "Epoch: 056, Loss: 0.2441, Train Acc: 0.9839, Test Acc: 0.6491\n",
      "Epoch: 057, Loss: 0.2385, Train Acc: 0.9839, Test Acc: 0.6842\n",
      "Epoch: 058, Loss: 0.2271, Train Acc: 0.9839, Test Acc: 0.6491\n",
      "Epoch: 059, Loss: 0.2528, Train Acc: 0.9839, Test Acc: 0.6667\n",
      "Epoch: 060, Loss: 0.2171, Train Acc: 0.9839, Test Acc: 0.6842\n",
      "Epoch: 061, Loss: 0.2169, Train Acc: 0.9839, Test Acc: 0.6842\n",
      "Epoch: 062, Loss: 0.1832, Train Acc: 0.9839, Test Acc: 0.6842\n",
      "Epoch: 063, Loss: 0.1725, Train Acc: 1.0000, Test Acc: 0.6842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20f421b9220>"
      ]
     },
     "execution_count": 1124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = Net(dim=19)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_epoch=[]\n",
    "test_epoch=[]\n",
    "epoch = 1\n",
    "train_acc=0\n",
    "while train_acc < 0.99 and epoch < 100:\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    train_epoch.append(train_acc)\n",
    "    test_epoch.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    epoch +=1\n",
    "\n",
    "plt.plot(train_epoch, color=\"red\", label='Train')\n",
    "plt.plot(test_epoch, color=\"blue\", label = 'Test')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb15f1e0f376981e7b6e1fc44ae8b8146823f10f258bcd6e448b0230b889fc06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
