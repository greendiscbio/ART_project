{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1690,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# os.environ['TORCH'] = torch.__version__\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# # !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cpu.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "# !pip install -q captum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1691,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gene matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1692,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACPP</th>\n",
       "      <th>FOLH1</th>\n",
       "      <th>FRAT1</th>\n",
       "      <th>FRAT2</th>\n",
       "      <th>ICOS</th>\n",
       "      <th>ICOSLG</th>\n",
       "      <th>ITK</th>\n",
       "      <th>MTCP1</th>\n",
       "      <th>NFATC1</th>\n",
       "      <th>NFATC2</th>\n",
       "      <th>...</th>\n",
       "      <th>TCF7L1</th>\n",
       "      <th>TCIRG1</th>\n",
       "      <th>TCL1A</th>\n",
       "      <th>TCL1B</th>\n",
       "      <th>TLX1</th>\n",
       "      <th>TLX2</th>\n",
       "      <th>TLX3</th>\n",
       "      <th>WT1</th>\n",
       "      <th>WTAP</th>\n",
       "      <th>WTIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.643264</td>\n",
       "      <td>33.06366</td>\n",
       "      <td>22.86623</td>\n",
       "      <td>25.14807</td>\n",
       "      <td>23.66478</td>\n",
       "      <td>31.25529</td>\n",
       "      <td>31.97483</td>\n",
       "      <td>32.68788</td>\n",
       "      <td>32.00358</td>\n",
       "      <td>31.98820</td>\n",
       "      <td>...</td>\n",
       "      <td>29.73447</td>\n",
       "      <td>33.92966</td>\n",
       "      <td>21.65301</td>\n",
       "      <td>21.31325</td>\n",
       "      <td>25.58066</td>\n",
       "      <td>21.09375</td>\n",
       "      <td>21.21067</td>\n",
       "      <td>27.25894</td>\n",
       "      <td>32.30986</td>\n",
       "      <td>30.89343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.478681</td>\n",
       "      <td>33.34812</td>\n",
       "      <td>27.58122</td>\n",
       "      <td>27.19051</td>\n",
       "      <td>27.81065</td>\n",
       "      <td>29.88798</td>\n",
       "      <td>32.85944</td>\n",
       "      <td>34.62906</td>\n",
       "      <td>30.96356</td>\n",
       "      <td>31.94520</td>\n",
       "      <td>...</td>\n",
       "      <td>31.39434</td>\n",
       "      <td>33.85213</td>\n",
       "      <td>21.65301</td>\n",
       "      <td>21.31325</td>\n",
       "      <td>21.06706</td>\n",
       "      <td>24.61959</td>\n",
       "      <td>21.21067</td>\n",
       "      <td>22.90940</td>\n",
       "      <td>33.64920</td>\n",
       "      <td>32.19936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.556110</td>\n",
       "      <td>32.20052</td>\n",
       "      <td>25.38929</td>\n",
       "      <td>26.69664</td>\n",
       "      <td>26.66294</td>\n",
       "      <td>30.92857</td>\n",
       "      <td>31.09956</td>\n",
       "      <td>33.46376</td>\n",
       "      <td>31.30038</td>\n",
       "      <td>31.04482</td>\n",
       "      <td>...</td>\n",
       "      <td>30.94080</td>\n",
       "      <td>33.53312</td>\n",
       "      <td>26.38498</td>\n",
       "      <td>21.31325</td>\n",
       "      <td>21.06706</td>\n",
       "      <td>21.09375</td>\n",
       "      <td>21.21067</td>\n",
       "      <td>22.90940</td>\n",
       "      <td>35.26323</td>\n",
       "      <td>29.13798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.253267</td>\n",
       "      <td>34.48359</td>\n",
       "      <td>28.10294</td>\n",
       "      <td>26.49687</td>\n",
       "      <td>23.66478</td>\n",
       "      <td>29.47329</td>\n",
       "      <td>31.18813</td>\n",
       "      <td>33.10176</td>\n",
       "      <td>31.65882</td>\n",
       "      <td>32.62476</td>\n",
       "      <td>...</td>\n",
       "      <td>31.69465</td>\n",
       "      <td>32.51771</td>\n",
       "      <td>24.98799</td>\n",
       "      <td>21.31325</td>\n",
       "      <td>21.06706</td>\n",
       "      <td>26.34435</td>\n",
       "      <td>21.21067</td>\n",
       "      <td>30.72576</td>\n",
       "      <td>33.85052</td>\n",
       "      <td>32.61099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.643264</td>\n",
       "      <td>34.68356</td>\n",
       "      <td>27.09929</td>\n",
       "      <td>22.82728</td>\n",
       "      <td>23.66478</td>\n",
       "      <td>32.22363</td>\n",
       "      <td>33.25193</td>\n",
       "      <td>32.65197</td>\n",
       "      <td>32.94580</td>\n",
       "      <td>33.70037</td>\n",
       "      <td>...</td>\n",
       "      <td>30.41687</td>\n",
       "      <td>33.75841</td>\n",
       "      <td>21.65301</td>\n",
       "      <td>21.31325</td>\n",
       "      <td>21.06706</td>\n",
       "      <td>21.09375</td>\n",
       "      <td>21.21067</td>\n",
       "      <td>31.55831</td>\n",
       "      <td>33.25769</td>\n",
       "      <td>29.29437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>29.643765</td>\n",
       "      <td>33.58120</td>\n",
       "      <td>26.78622</td>\n",
       "      <td>27.10293</td>\n",
       "      <td>22.97275</td>\n",
       "      <td>30.82689</td>\n",
       "      <td>26.57659</td>\n",
       "      <td>33.91039</td>\n",
       "      <td>31.93211</td>\n",
       "      <td>31.61476</td>\n",
       "      <td>...</td>\n",
       "      <td>31.55301</td>\n",
       "      <td>32.80839</td>\n",
       "      <td>20.32437</td>\n",
       "      <td>21.31293</td>\n",
       "      <td>21.09326</td>\n",
       "      <td>21.18281</td>\n",
       "      <td>21.37595</td>\n",
       "      <td>33.76553</td>\n",
       "      <td>32.92597</td>\n",
       "      <td>33.36054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>32.256558</td>\n",
       "      <td>31.42896</td>\n",
       "      <td>21.76825</td>\n",
       "      <td>26.01421</td>\n",
       "      <td>29.53775</td>\n",
       "      <td>27.72514</td>\n",
       "      <td>30.11127</td>\n",
       "      <td>34.06419</td>\n",
       "      <td>29.13034</td>\n",
       "      <td>29.95589</td>\n",
       "      <td>...</td>\n",
       "      <td>27.36919</td>\n",
       "      <td>33.87425</td>\n",
       "      <td>25.58961</td>\n",
       "      <td>21.31293</td>\n",
       "      <td>21.09326</td>\n",
       "      <td>21.18281</td>\n",
       "      <td>21.37595</td>\n",
       "      <td>29.72826</td>\n",
       "      <td>35.27110</td>\n",
       "      <td>27.34363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>26.275234</td>\n",
       "      <td>34.13379</td>\n",
       "      <td>26.74567</td>\n",
       "      <td>26.15839</td>\n",
       "      <td>28.98194</td>\n",
       "      <td>31.82216</td>\n",
       "      <td>33.03207</td>\n",
       "      <td>32.87564</td>\n",
       "      <td>32.10593</td>\n",
       "      <td>32.57213</td>\n",
       "      <td>...</td>\n",
       "      <td>31.82891</td>\n",
       "      <td>33.58358</td>\n",
       "      <td>20.32437</td>\n",
       "      <td>21.31293</td>\n",
       "      <td>21.09326</td>\n",
       "      <td>26.68332</td>\n",
       "      <td>21.37595</td>\n",
       "      <td>24.56089</td>\n",
       "      <td>33.08478</td>\n",
       "      <td>32.23764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>31.822287</td>\n",
       "      <td>32.24862</td>\n",
       "      <td>25.46541</td>\n",
       "      <td>22.81349</td>\n",
       "      <td>27.50438</td>\n",
       "      <td>31.74571</td>\n",
       "      <td>31.83385</td>\n",
       "      <td>33.71362</td>\n",
       "      <td>31.09448</td>\n",
       "      <td>32.49485</td>\n",
       "      <td>...</td>\n",
       "      <td>30.01069</td>\n",
       "      <td>34.20252</td>\n",
       "      <td>20.32437</td>\n",
       "      <td>21.31293</td>\n",
       "      <td>21.09326</td>\n",
       "      <td>21.18281</td>\n",
       "      <td>21.37595</td>\n",
       "      <td>29.86003</td>\n",
       "      <td>32.15581</td>\n",
       "      <td>32.35205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>32.443022</td>\n",
       "      <td>26.45839</td>\n",
       "      <td>26.21286</td>\n",
       "      <td>26.00386</td>\n",
       "      <td>29.33309</td>\n",
       "      <td>29.34334</td>\n",
       "      <td>32.18353</td>\n",
       "      <td>32.85734</td>\n",
       "      <td>30.93542</td>\n",
       "      <td>30.80994</td>\n",
       "      <td>...</td>\n",
       "      <td>30.86975</td>\n",
       "      <td>35.10128</td>\n",
       "      <td>26.59326</td>\n",
       "      <td>21.31293</td>\n",
       "      <td>21.09326</td>\n",
       "      <td>27.56092</td>\n",
       "      <td>21.37595</td>\n",
       "      <td>34.54019</td>\n",
       "      <td>32.88606</td>\n",
       "      <td>27.34363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ACPP     FOLH1     FRAT1     FRAT2      ICOS    ICOSLG       ITK  \\\n",
       "0    24.643264  33.06366  22.86623  25.14807  23.66478  31.25529  31.97483   \n",
       "1    30.478681  33.34812  27.58122  27.19051  27.81065  29.88798  32.85944   \n",
       "2    30.556110  32.20052  25.38929  26.69664  26.66294  30.92857  31.09956   \n",
       "3    30.253267  34.48359  28.10294  26.49687  23.66478  29.47329  31.18813   \n",
       "4    24.643264  34.68356  27.09929  22.82728  23.66478  32.22363  33.25193   \n",
       "..         ...       ...       ...       ...       ...       ...       ...   \n",
       "176  29.643765  33.58120  26.78622  27.10293  22.97275  30.82689  26.57659   \n",
       "177  32.256558  31.42896  21.76825  26.01421  29.53775  27.72514  30.11127   \n",
       "178  26.275234  34.13379  26.74567  26.15839  28.98194  31.82216  33.03207   \n",
       "179  31.822287  32.24862  25.46541  22.81349  27.50438  31.74571  31.83385   \n",
       "180  32.443022  26.45839  26.21286  26.00386  29.33309  29.34334  32.18353   \n",
       "\n",
       "        MTCP1    NFATC1    NFATC2  ...    TCF7L1    TCIRG1     TCL1A  \\\n",
       "0    32.68788  32.00358  31.98820  ...  29.73447  33.92966  21.65301   \n",
       "1    34.62906  30.96356  31.94520  ...  31.39434  33.85213  21.65301   \n",
       "2    33.46376  31.30038  31.04482  ...  30.94080  33.53312  26.38498   \n",
       "3    33.10176  31.65882  32.62476  ...  31.69465  32.51771  24.98799   \n",
       "4    32.65197  32.94580  33.70037  ...  30.41687  33.75841  21.65301   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "176  33.91039  31.93211  31.61476  ...  31.55301  32.80839  20.32437   \n",
       "177  34.06419  29.13034  29.95589  ...  27.36919  33.87425  25.58961   \n",
       "178  32.87564  32.10593  32.57213  ...  31.82891  33.58358  20.32437   \n",
       "179  33.71362  31.09448  32.49485  ...  30.01069  34.20252  20.32437   \n",
       "180  32.85734  30.93542  30.80994  ...  30.86975  35.10128  26.59326   \n",
       "\n",
       "        TCL1B      TLX1      TLX2      TLX3       WT1      WTAP      WTIP  \n",
       "0    21.31325  25.58066  21.09375  21.21067  27.25894  32.30986  30.89343  \n",
       "1    21.31325  21.06706  24.61959  21.21067  22.90940  33.64920  32.19936  \n",
       "2    21.31325  21.06706  21.09375  21.21067  22.90940  35.26323  29.13798  \n",
       "3    21.31325  21.06706  26.34435  21.21067  30.72576  33.85052  32.61099  \n",
       "4    21.31325  21.06706  21.09375  21.21067  31.55831  33.25769  29.29437  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "176  21.31293  21.09326  21.18281  21.37595  33.76553  32.92597  33.36054  \n",
       "177  21.31293  21.09326  21.18281  21.37595  29.72826  35.27110  27.34363  \n",
       "178  21.31293  21.09326  26.68332  21.37595  24.56089  33.08478  32.23764  \n",
       "179  21.31293  21.09326  21.18281  21.37595  29.86003  32.15581  32.35205  \n",
       "180  21.31293  21.09326  27.56092  21.37595  34.54019  32.88606  27.34363  \n",
       "\n",
       "[181 rows x 38 columns]"
      ]
     },
     "execution_count": 1692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes = pd.read_csv('../Data/Programmed cell death protein/Programmed_cell_death_protein_matrix.csv')\n",
    "Y = genes.Y\n",
    "\n",
    "genes = genes.iloc[:,1:39] \n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1693,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36.3834914, 35.6532   , 29.63795  , 29.85193  , 31.80307  ,\n",
       "       33.33913  , 35.58195  , 35.91798  , 34.16501  , 34.98087  ,\n",
       "       33.54234  , 35.44608  , 36.32668  , 31.89917  , 27.45466  ,\n",
       "       27.7493   , 29.68846  , 33.07908  , 32.17109  , 35.39935  ,\n",
       "       29.31901  , 22.79502  , 34.30242  , 30.99031  , 33.85911  ,\n",
       "       33.19399  , 30.59104  , 33.66715  , 34.88199  , 36.75667  ,\n",
       "       31.1586   , 27.90452  , 27.3604   , 31.07757  , 29.22178  ,\n",
       "       34.74902  , 35.37453  , 34.32126  ])"
      ]
     },
     "execution_count": 1693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "genes_ = scaler.fit_transform(genes)\n",
    "scaler.data_max_\n",
    "# genes = pd.DataFrame(genes_, columns=genes.columns)\n",
    "# genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1694,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for g in range(len(genes)):\n",
    "#     if Y[g]==0:\n",
    "#         for i in genes.iloc[g]:\n",
    "#             genes.iloc[g]=np.zeros((1, 38))\n",
    "#     else:\n",
    "#         for i in genes.iloc[g]:\n",
    "#             genes.iloc[g]=np.ones((1, 38))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1695,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='../Data/Programmed cell death protein/network_edges_pd-1.tsv'\n",
    "data = pd.read_csv(path, delimiter='\\t')\n",
    "edge_index1=data['#node1'].to_numpy()\n",
    "edge_index2=data['node2'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1696,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 1696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(edge_index1)\n",
    "len(list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1697,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index1 = le.transform(edge_index1)\n",
    "edge_index2 = le.transform(edge_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1698,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = [edge_index1]+[edge_index2]\n",
    "edge_index = np.array(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  5,  5,  1,  1,  2,  3,  4,  4,  4,  4,  6,  7,  7,  8,\n",
       "         8,  8,  8,  9,  9,  9,  9, 10, 11, 11, 11, 12, 12, 13, 13, 13,\n",
       "        14, 14, 14, 14, 15, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 17,\n",
       "        17, 18, 18, 19, 20, 21, 22, 22, 22, 22, 22, 22, 23, 23, 24, 24,\n",
       "        25, 25, 25, 25, 26, 26, 27, 27, 27, 28, 29, 30, 30, 31, 31, 32,\n",
       "        32, 32, 33, 34, 34, 35, 35, 35, 36, 37],\n",
       "       [22, 19,  4, 17, 24, 22,  3,  2, 18,  6, 17,  5,  4, 31, 30, 29,\n",
       "        11, 17,  9, 11, 10, 12,  8,  9, 12,  8,  9, 11,  9, 16, 14, 15,\n",
       "        22, 13, 15, 16, 22, 13, 14, 16, 22, 13, 14, 15,  4,  8, 27,  5,\n",
       "        18,  4, 17,  0, 21, 20,  1, 24, 15, 14,  0, 16, 25, 27,  1, 22,\n",
       "        33, 23, 34, 32, 34, 32, 28, 17, 23, 27,  8, 31,  7,  7, 30, 25,\n",
       "        35, 26, 25, 25, 26, 32, 37, 36, 35, 35]])"
      ]
     },
     "execution_count": 1699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  5,  5,  1,  1,  2,  3,  4,  4,  4,  4,  6,  7,  7,  8,  8,  8,\n",
       "          8,  9,  9,  9,  9, 10, 11, 11, 11, 12, 12, 13, 13, 13, 14, 14, 14, 14,\n",
       "         15, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 17, 17, 18, 18, 19, 20, 21,\n",
       "         22, 22, 22, 22, 22, 22, 23, 23, 24, 24, 25, 25, 25, 25, 26, 26, 27, 27,\n",
       "         27, 28, 29, 30, 30, 31, 31, 32, 32, 32, 33, 34, 34, 35, 35, 35, 36, 37],\n",
       "        [22, 19,  4, 17, 24, 22,  3,  2, 18,  6, 17,  5,  4, 31, 30, 29, 11, 17,\n",
       "          9, 11, 10, 12,  8,  9, 12,  8,  9, 11,  9, 16, 14, 15, 22, 13, 15, 16,\n",
       "         22, 13, 14, 16, 22, 13, 14, 15,  4,  8, 27,  5, 18,  4, 17,  0, 21, 20,\n",
       "          1, 24, 15, 14,  0, 16, 25, 27,  1, 22, 33, 23, 34, 32, 34, 32, 28, 17,\n",
       "         23, 27,  8, 31,  7,  7, 30, 25, 35, 26, 25, 25, 26, 32, 37, 36, 35, 35]])"
      ]
     },
     "execution_count": 1700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edge_index, dtype=torch.int64)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[38], edge_index=[2, 90], y=[1, 1])\n"
     ]
    }
   ],
   "source": [
    "list_data=[]\n",
    "\n",
    "for g in range(len(genes)):\n",
    "  b=[]\n",
    "  for i in genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    a.append(i*10)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.float).reshape([-1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y.iloc[g]], dtype=torch.float).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  list_data.append(data)\n",
    "\n",
    "print(list_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([246.4326, 330.6366, 228.6623, 251.4807, 236.6478, 312.5529, 319.7483,\n",
       "        326.8788, 320.0358, 319.8820, 316.0401, 326.9664, 308.3347, 215.0007,\n",
       "        211.8392, 211.6984, 212.8552, 288.4472, 302.3462, 347.1497, 211.1579,\n",
       "        210.5495, 219.4579, 219.5682, 253.3625, 271.7087, 215.2616, 310.1699,\n",
       "        297.3447, 339.2966, 216.5301, 213.1325, 255.8066, 210.9375, 212.1067,\n",
       "        272.5894, 323.0986, 308.9343])"
      ]
     },
     "execution_count": 1702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_data[0].x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Patient sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 38\n",
      "Number of charcateristics per node: 1\n",
      "Number of edges: 90\n",
      "Average node degree: 2.37\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "data = list_data[0]\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of charcateristics per node: {data.num_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Train-Test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 124\n",
      "Number of test graphs: 57\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "# random.shuffle(list_data)\n",
    "train_dataset = list_data[0:124]\n",
    "test_dataset = list_data[124:182]\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[1216], edge_index=[2, 2880], y=[32, 1], batch=[1216], ptr=[33])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[1216], edge_index=[2, 2880], y=[32, 1], batch=[1216], ptr=[33])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[1216], edge_index=[2, 2880], y=[32, 1], batch=[1216], ptr=[33])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of graphs in the current batch: 28\n",
      "DataBatch(x=[1064], edge_index=[2, 2520], y=[28, 1], batch=[1064], ptr=[29])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1706,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv, global_add_pool\n",
    "from torch_geometric.nn import TopKPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch import nn\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1707,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 38\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.dim = dim\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GraphConv(embed_dim, dim)\n",
    "        self.pool1 = TopKPooling(dim, ratio=0.8)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.pool2 = TopKPooling(dim, ratio=0.8)\n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=368, embedding_dim=embed_dim)\n",
    "        self.lin1 = torch.nn.Linear(38, 19)\n",
    "        self.lin3 = torch.nn.Linear(19, 1)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = torch.tensor(x).to(torch.int)\n",
    "        x = self.item_embedding(x)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        # x = F.relu(self.conv2(x, edge_index))\n",
    "        # x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        # x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 #+ x2\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = torch.sigmoid(self.lin3(x)).squeeze(1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1708,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(output, data.y.squeeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        for i in range(len(output)):\n",
    "            if output[i]>0.5:\n",
    "                output[i]=1\n",
    "            else:\n",
    "                output[i]=0\n",
    "            if output[i]==data.y[i]:\n",
    "                correct=correct+1\n",
    "    # print(\"Correct: \"+str(correct) +\" of \"+str(len(loader.dataset)))\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): GraphConv(38, 19)\n",
      "  (pool1): TopKPooling(19, ratio=0.8, multiplier=1.0)\n",
      "  (conv2): GraphConv(19, 19)\n",
      "  (pool2): TopKPooling(19, ratio=0.8, multiplier=1.0)\n",
      "  (item_embedding): Embedding(368, 38)\n",
      "  (lin1): Linear(in_features=38, out_features=19, bias=True)\n",
      "  (lin3): Linear(in_features=19, out_features=1, bias=True)\n",
      "  (act1): ReLU()\n",
      ")\n",
      "Epoch: 001, Loss: 0.7091, Train Acc: 0.5403, Test Acc: 0.5088\n",
      "Epoch: 002, Loss: 0.6799, Train Acc: 0.5403, Test Acc: 0.5088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_15936/3374044401.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x).to(torch.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.6850, Train Acc: 0.5403, Test Acc: 0.5088\n",
      "Epoch: 004, Loss: 0.6829, Train Acc: 0.5403, Test Acc: 0.5088\n",
      "Epoch: 005, Loss: 0.6779, Train Acc: 0.5484, Test Acc: 0.5088\n",
      "Epoch: 006, Loss: 0.6830, Train Acc: 0.5565, Test Acc: 0.5088\n",
      "Epoch: 007, Loss: 0.6727, Train Acc: 0.5565, Test Acc: 0.5088\n",
      "Epoch: 008, Loss: 0.6800, Train Acc: 0.5726, Test Acc: 0.5088\n",
      "Epoch: 009, Loss: 0.6746, Train Acc: 0.5887, Test Acc: 0.5088\n",
      "Epoch: 010, Loss: 0.6652, Train Acc: 0.5887, Test Acc: 0.5088\n",
      "Epoch: 011, Loss: 0.6586, Train Acc: 0.5887, Test Acc: 0.5088\n",
      "Epoch: 012, Loss: 0.6639, Train Acc: 0.5887, Test Acc: 0.5088\n",
      "Epoch: 013, Loss: 0.6566, Train Acc: 0.5887, Test Acc: 0.5088\n",
      "Epoch: 014, Loss: 0.6408, Train Acc: 0.5887, Test Acc: 0.5088\n",
      "Epoch: 015, Loss: 0.6290, Train Acc: 0.6048, Test Acc: 0.5088\n",
      "Epoch: 016, Loss: 0.6451, Train Acc: 0.6210, Test Acc: 0.5088\n",
      "Epoch: 017, Loss: 0.6273, Train Acc: 0.6694, Test Acc: 0.5088\n",
      "Epoch: 018, Loss: 0.6155, Train Acc: 0.7016, Test Acc: 0.5088\n",
      "Epoch: 019, Loss: 0.6166, Train Acc: 0.7258, Test Acc: 0.5088\n",
      "Epoch: 020, Loss: 0.5952, Train Acc: 0.7419, Test Acc: 0.5614\n",
      "Epoch: 021, Loss: 0.5782, Train Acc: 0.7903, Test Acc: 0.5789\n",
      "Epoch: 022, Loss: 0.5736, Train Acc: 0.8065, Test Acc: 0.6140\n",
      "Epoch: 023, Loss: 0.5467, Train Acc: 0.8548, Test Acc: 0.6140\n",
      "Epoch: 024, Loss: 0.5502, Train Acc: 0.8629, Test Acc: 0.6140\n",
      "Epoch: 025, Loss: 0.5431, Train Acc: 0.8952, Test Acc: 0.6491\n",
      "Epoch: 026, Loss: 0.5192, Train Acc: 0.9032, Test Acc: 0.6667\n",
      "Epoch: 027, Loss: 0.5031, Train Acc: 0.9113, Test Acc: 0.6667\n",
      "Epoch: 028, Loss: 0.4639, Train Acc: 0.9113, Test Acc: 0.6491\n",
      "Epoch: 029, Loss: 0.4427, Train Acc: 0.9355, Test Acc: 0.6316\n",
      "Epoch: 030, Loss: 0.4178, Train Acc: 0.9355, Test Acc: 0.6316\n",
      "Epoch: 031, Loss: 0.4136, Train Acc: 0.9516, Test Acc: 0.6316\n",
      "Epoch: 032, Loss: 0.4071, Train Acc: 0.9435, Test Acc: 0.6491\n",
      "Epoch: 033, Loss: 0.3795, Train Acc: 0.9435, Test Acc: 0.6316\n",
      "Epoch: 034, Loss: 0.3366, Train Acc: 0.9516, Test Acc: 0.6491\n",
      "Epoch: 035, Loss: 0.3408, Train Acc: 0.9677, Test Acc: 0.6491\n",
      "Epoch: 036, Loss: 0.3239, Train Acc: 0.9758, Test Acc: 0.6491\n",
      "Epoch: 037, Loss: 0.2658, Train Acc: 0.9839, Test Acc: 0.6667\n",
      "Epoch: 038, Loss: 0.2920, Train Acc: 0.9839, Test Acc: 0.7018\n",
      "Epoch: 039, Loss: 0.2619, Train Acc: 0.9839, Test Acc: 0.6842\n",
      "Epoch: 040, Loss: 0.1985, Train Acc: 0.9919, Test Acc: 0.7018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20f44e987f0>"
      ]
     },
     "execution_count": 1709,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = Net(dim=19)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_epoch=[]\n",
    "test_epoch=[]\n",
    "epoch = 1\n",
    "train_acc=0\n",
    "while train_acc < 0.99 and epoch < 100:\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    train_epoch.append(train_acc)\n",
    "    test_epoch.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    epoch +=1\n",
    "\n",
    "plt.plot(train_epoch, color=\"red\", label='Train')\n",
    "plt.plot(test_epoch, color=\"blue\", label = 'Test')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb15f1e0f376981e7b6e1fc44ae8b8146823f10f258bcd6e448b0230b889fc06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
