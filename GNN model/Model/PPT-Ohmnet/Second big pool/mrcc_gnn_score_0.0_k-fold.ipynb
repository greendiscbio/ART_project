{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cpu.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gene matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABCB1</th>\n",
       "      <th>ABCB6</th>\n",
       "      <th>ABCG2</th>\n",
       "      <th>ABL2</th>\n",
       "      <th>ACADM</th>\n",
       "      <th>ACD</th>\n",
       "      <th>ACE</th>\n",
       "      <th>ACE2</th>\n",
       "      <th>ACHE</th>\n",
       "      <th>ACO2</th>\n",
       "      <th>...</th>\n",
       "      <th>XRCC4</th>\n",
       "      <th>XRCC6</th>\n",
       "      <th>YAP1</th>\n",
       "      <th>YBX1</th>\n",
       "      <th>YBX3</th>\n",
       "      <th>YY1</th>\n",
       "      <th>ZBTB17</th>\n",
       "      <th>ZHX2</th>\n",
       "      <th>ZMIZ1</th>\n",
       "      <th>ZMPSTE24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.716012</td>\n",
       "      <td>33.867624</td>\n",
       "      <td>29.676682</td>\n",
       "      <td>32.862716</td>\n",
       "      <td>35.061520</td>\n",
       "      <td>31.801427</td>\n",
       "      <td>31.988036</td>\n",
       "      <td>30.172489</td>\n",
       "      <td>29.912204</td>\n",
       "      <td>35.812113</td>\n",
       "      <td>...</td>\n",
       "      <td>29.79709</td>\n",
       "      <td>35.24634</td>\n",
       "      <td>33.97677</td>\n",
       "      <td>37.51551</td>\n",
       "      <td>35.75761</td>\n",
       "      <td>33.53412</td>\n",
       "      <td>31.72285</td>\n",
       "      <td>33.23150</td>\n",
       "      <td>32.38760</td>\n",
       "      <td>32.22373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.174577</td>\n",
       "      <td>32.703008</td>\n",
       "      <td>31.318871</td>\n",
       "      <td>33.061603</td>\n",
       "      <td>35.908450</td>\n",
       "      <td>31.878071</td>\n",
       "      <td>33.015718</td>\n",
       "      <td>33.634947</td>\n",
       "      <td>26.076400</td>\n",
       "      <td>34.834669</td>\n",
       "      <td>...</td>\n",
       "      <td>31.29674</td>\n",
       "      <td>35.91455</td>\n",
       "      <td>34.75610</td>\n",
       "      <td>36.67356</td>\n",
       "      <td>35.85355</td>\n",
       "      <td>34.44291</td>\n",
       "      <td>31.63512</td>\n",
       "      <td>32.63491</td>\n",
       "      <td>33.78434</td>\n",
       "      <td>32.19111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.225510</td>\n",
       "      <td>34.522351</td>\n",
       "      <td>30.612181</td>\n",
       "      <td>32.658413</td>\n",
       "      <td>34.370223</td>\n",
       "      <td>31.425178</td>\n",
       "      <td>31.714695</td>\n",
       "      <td>26.858096</td>\n",
       "      <td>27.589284</td>\n",
       "      <td>34.449024</td>\n",
       "      <td>...</td>\n",
       "      <td>31.37668</td>\n",
       "      <td>36.05801</td>\n",
       "      <td>34.48484</td>\n",
       "      <td>36.41164</td>\n",
       "      <td>35.24518</td>\n",
       "      <td>35.14050</td>\n",
       "      <td>31.99899</td>\n",
       "      <td>34.31359</td>\n",
       "      <td>32.95630</td>\n",
       "      <td>32.77568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.988641</td>\n",
       "      <td>33.059035</td>\n",
       "      <td>31.973489</td>\n",
       "      <td>33.014431</td>\n",
       "      <td>35.824161</td>\n",
       "      <td>31.829645</td>\n",
       "      <td>32.916062</td>\n",
       "      <td>29.565514</td>\n",
       "      <td>28.143610</td>\n",
       "      <td>35.401370</td>\n",
       "      <td>...</td>\n",
       "      <td>30.65271</td>\n",
       "      <td>35.75676</td>\n",
       "      <td>35.27953</td>\n",
       "      <td>36.58061</td>\n",
       "      <td>36.19379</td>\n",
       "      <td>34.96911</td>\n",
       "      <td>31.28469</td>\n",
       "      <td>33.03073</td>\n",
       "      <td>33.88207</td>\n",
       "      <td>32.46805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.284849</td>\n",
       "      <td>31.488348</td>\n",
       "      <td>30.774368</td>\n",
       "      <td>33.793065</td>\n",
       "      <td>34.332987</td>\n",
       "      <td>32.020127</td>\n",
       "      <td>31.824945</td>\n",
       "      <td>28.767942</td>\n",
       "      <td>26.537023</td>\n",
       "      <td>33.068712</td>\n",
       "      <td>...</td>\n",
       "      <td>31.10007</td>\n",
       "      <td>36.33015</td>\n",
       "      <td>34.50745</td>\n",
       "      <td>36.89459</td>\n",
       "      <td>34.95292</td>\n",
       "      <td>34.76483</td>\n",
       "      <td>33.40109</td>\n",
       "      <td>32.15993</td>\n",
       "      <td>33.35772</td>\n",
       "      <td>31.64157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>33.838323</td>\n",
       "      <td>32.296025</td>\n",
       "      <td>30.740420</td>\n",
       "      <td>33.040376</td>\n",
       "      <td>34.981970</td>\n",
       "      <td>31.823358</td>\n",
       "      <td>32.748254</td>\n",
       "      <td>25.870831</td>\n",
       "      <td>27.355529</td>\n",
       "      <td>34.695167</td>\n",
       "      <td>...</td>\n",
       "      <td>30.75886</td>\n",
       "      <td>35.53767</td>\n",
       "      <td>34.82946</td>\n",
       "      <td>37.20378</td>\n",
       "      <td>36.41314</td>\n",
       "      <td>34.69950</td>\n",
       "      <td>32.89423</td>\n",
       "      <td>33.07640</td>\n",
       "      <td>33.73563</td>\n",
       "      <td>32.02075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>31.895951</td>\n",
       "      <td>33.784466</td>\n",
       "      <td>29.347511</td>\n",
       "      <td>31.571621</td>\n",
       "      <td>34.865097</td>\n",
       "      <td>29.786302</td>\n",
       "      <td>29.720601</td>\n",
       "      <td>30.945513</td>\n",
       "      <td>23.149098</td>\n",
       "      <td>32.541787</td>\n",
       "      <td>...</td>\n",
       "      <td>32.96204</td>\n",
       "      <td>35.50224</td>\n",
       "      <td>33.69401</td>\n",
       "      <td>36.55929</td>\n",
       "      <td>35.16855</td>\n",
       "      <td>34.89385</td>\n",
       "      <td>33.76237</td>\n",
       "      <td>31.38631</td>\n",
       "      <td>30.87456</td>\n",
       "      <td>32.16958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>33.842290</td>\n",
       "      <td>32.838072</td>\n",
       "      <td>29.862937</td>\n",
       "      <td>33.491465</td>\n",
       "      <td>34.379400</td>\n",
       "      <td>31.246333</td>\n",
       "      <td>33.114433</td>\n",
       "      <td>25.870831</td>\n",
       "      <td>28.926076</td>\n",
       "      <td>33.864699</td>\n",
       "      <td>...</td>\n",
       "      <td>29.63276</td>\n",
       "      <td>35.22805</td>\n",
       "      <td>34.52318</td>\n",
       "      <td>36.62523</td>\n",
       "      <td>35.18621</td>\n",
       "      <td>34.88021</td>\n",
       "      <td>31.47358</td>\n",
       "      <td>31.68824</td>\n",
       "      <td>33.42545</td>\n",
       "      <td>32.62316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>34.715293</td>\n",
       "      <td>32.561403</td>\n",
       "      <td>29.292590</td>\n",
       "      <td>33.151894</td>\n",
       "      <td>34.802123</td>\n",
       "      <td>32.344355</td>\n",
       "      <td>33.543569</td>\n",
       "      <td>32.700981</td>\n",
       "      <td>29.967641</td>\n",
       "      <td>35.307358</td>\n",
       "      <td>...</td>\n",
       "      <td>31.09475</td>\n",
       "      <td>34.81409</td>\n",
       "      <td>34.59853</td>\n",
       "      <td>34.81241</td>\n",
       "      <td>35.19760</td>\n",
       "      <td>34.37022</td>\n",
       "      <td>31.75344</td>\n",
       "      <td>33.62427</td>\n",
       "      <td>33.09948</td>\n",
       "      <td>31.49508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>32.476530</td>\n",
       "      <td>32.316413</td>\n",
       "      <td>26.957145</td>\n",
       "      <td>33.580139</td>\n",
       "      <td>34.950670</td>\n",
       "      <td>32.959610</td>\n",
       "      <td>32.203161</td>\n",
       "      <td>25.870831</td>\n",
       "      <td>29.189065</td>\n",
       "      <td>35.778200</td>\n",
       "      <td>...</td>\n",
       "      <td>31.01456</td>\n",
       "      <td>37.01634</td>\n",
       "      <td>35.21349</td>\n",
       "      <td>36.59243</td>\n",
       "      <td>36.28946</td>\n",
       "      <td>35.12411</td>\n",
       "      <td>32.07098</td>\n",
       "      <td>33.59060</td>\n",
       "      <td>33.46466</td>\n",
       "      <td>31.84658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 838 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ABCB1      ABCB6      ABCG2       ABL2      ACADM        ACD  \\\n",
       "0    36.716012  33.867624  29.676682  32.862716  35.061520  31.801427   \n",
       "1    34.174577  32.703008  31.318871  33.061603  35.908450  31.878071   \n",
       "2    31.225510  34.522351  30.612181  32.658413  34.370223  31.425178   \n",
       "3    32.988641  33.059035  31.973489  33.014431  35.824161  31.829645   \n",
       "4    33.284849  31.488348  30.774368  33.793065  34.332987  32.020127   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "176  33.838323  32.296025  30.740420  33.040376  34.981970  31.823358   \n",
       "177  31.895951  33.784466  29.347511  31.571621  34.865097  29.786302   \n",
       "178  33.842290  32.838072  29.862937  33.491465  34.379400  31.246333   \n",
       "179  34.715293  32.561403  29.292590  33.151894  34.802123  32.344355   \n",
       "180  32.476530  32.316413  26.957145  33.580139  34.950670  32.959610   \n",
       "\n",
       "           ACE       ACE2       ACHE       ACO2  ...     XRCC4     XRCC6  \\\n",
       "0    31.988036  30.172489  29.912204  35.812113  ...  29.79709  35.24634   \n",
       "1    33.015718  33.634947  26.076400  34.834669  ...  31.29674  35.91455   \n",
       "2    31.714695  26.858096  27.589284  34.449024  ...  31.37668  36.05801   \n",
       "3    32.916062  29.565514  28.143610  35.401370  ...  30.65271  35.75676   \n",
       "4    31.824945  28.767942  26.537023  33.068712  ...  31.10007  36.33015   \n",
       "..         ...        ...        ...        ...  ...       ...       ...   \n",
       "176  32.748254  25.870831  27.355529  34.695167  ...  30.75886  35.53767   \n",
       "177  29.720601  30.945513  23.149098  32.541787  ...  32.96204  35.50224   \n",
       "178  33.114433  25.870831  28.926076  33.864699  ...  29.63276  35.22805   \n",
       "179  33.543569  32.700981  29.967641  35.307358  ...  31.09475  34.81409   \n",
       "180  32.203161  25.870831  29.189065  35.778200  ...  31.01456  37.01634   \n",
       "\n",
       "         YAP1      YBX1      YBX3       YY1    ZBTB17      ZHX2     ZMIZ1  \\\n",
       "0    33.97677  37.51551  35.75761  33.53412  31.72285  33.23150  32.38760   \n",
       "1    34.75610  36.67356  35.85355  34.44291  31.63512  32.63491  33.78434   \n",
       "2    34.48484  36.41164  35.24518  35.14050  31.99899  34.31359  32.95630   \n",
       "3    35.27953  36.58061  36.19379  34.96911  31.28469  33.03073  33.88207   \n",
       "4    34.50745  36.89459  34.95292  34.76483  33.40109  32.15993  33.35772   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "176  34.82946  37.20378  36.41314  34.69950  32.89423  33.07640  33.73563   \n",
       "177  33.69401  36.55929  35.16855  34.89385  33.76237  31.38631  30.87456   \n",
       "178  34.52318  36.62523  35.18621  34.88021  31.47358  31.68824  33.42545   \n",
       "179  34.59853  34.81241  35.19760  34.37022  31.75344  33.62427  33.09948   \n",
       "180  35.21349  36.59243  36.28946  35.12411  32.07098  33.59060  33.46466   \n",
       "\n",
       "     ZMPSTE24  \n",
       "0    32.22373  \n",
       "1    32.19111  \n",
       "2    32.77568  \n",
       "3    32.46805  \n",
       "4    31.64157  \n",
       "..        ...  \n",
       "176  32.02075  \n",
       "177  32.16958  \n",
       "178  32.62316  \n",
       "179  31.49508  \n",
       "180  31.84658  \n",
       "\n",
       "[181 rows x 838 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes = pd.read_csv('../../../Data\\PPT-Ohmnet/mRCC_big_pool/Second big pool/mrcc_protein_matrix_2776_genes_839_nodes.csv')\n",
    "Y = genes.Y\n",
    "\n",
    "genes = genes.iloc[:,1:839] \n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABCB1</th>\n",
       "      <th>ABCB6</th>\n",
       "      <th>ABCG2</th>\n",
       "      <th>ABL2</th>\n",
       "      <th>ACADM</th>\n",
       "      <th>ACD</th>\n",
       "      <th>ACE</th>\n",
       "      <th>ACE2</th>\n",
       "      <th>ACHE</th>\n",
       "      <th>ACO2</th>\n",
       "      <th>...</th>\n",
       "      <th>XRCC4</th>\n",
       "      <th>XRCC6</th>\n",
       "      <th>YAP1</th>\n",
       "      <th>YBX1</th>\n",
       "      <th>YBX3</th>\n",
       "      <th>YY1</th>\n",
       "      <th>ZBTB17</th>\n",
       "      <th>ZHX2</th>\n",
       "      <th>ZMIZ1</th>\n",
       "      <th>ZMPSTE24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.913055</td>\n",
       "      <td>0.671207</td>\n",
       "      <td>0.629264</td>\n",
       "      <td>0.440491</td>\n",
       "      <td>0.462644</td>\n",
       "      <td>0.610824</td>\n",
       "      <td>0.385954</td>\n",
       "      <td>0.599127</td>\n",
       "      <td>0.716215</td>\n",
       "      <td>0.805240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590076</td>\n",
       "      <td>0.495999</td>\n",
       "      <td>0.350519</td>\n",
       "      <td>0.653839</td>\n",
       "      <td>0.589186</td>\n",
       "      <td>0.283179</td>\n",
       "      <td>0.394627</td>\n",
       "      <td>0.559013</td>\n",
       "      <td>0.297360</td>\n",
       "      <td>0.527435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.611207</td>\n",
       "      <td>0.444017</td>\n",
       "      <td>0.793442</td>\n",
       "      <td>0.498055</td>\n",
       "      <td>0.651154</td>\n",
       "      <td>0.619572</td>\n",
       "      <td>0.560882</td>\n",
       "      <td>0.864018</td>\n",
       "      <td>0.399519</td>\n",
       "      <td>0.596648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766505</td>\n",
       "      <td>0.686270</td>\n",
       "      <td>0.533618</td>\n",
       "      <td>0.470341</td>\n",
       "      <td>0.610968</td>\n",
       "      <td>0.578967</td>\n",
       "      <td>0.378959</td>\n",
       "      <td>0.378272</td>\n",
       "      <td>0.571863</td>\n",
       "      <td>0.520119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.260945</td>\n",
       "      <td>0.798929</td>\n",
       "      <td>0.722790</td>\n",
       "      <td>0.381359</td>\n",
       "      <td>0.308776</td>\n",
       "      <td>0.567881</td>\n",
       "      <td>0.339427</td>\n",
       "      <td>0.345563</td>\n",
       "      <td>0.524427</td>\n",
       "      <td>0.514349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775910</td>\n",
       "      <td>0.727119</td>\n",
       "      <td>0.469887</td>\n",
       "      <td>0.413257</td>\n",
       "      <td>0.472842</td>\n",
       "      <td>0.806014</td>\n",
       "      <td>0.443946</td>\n",
       "      <td>0.886840</td>\n",
       "      <td>0.409127</td>\n",
       "      <td>0.651230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.470353</td>\n",
       "      <td>0.513469</td>\n",
       "      <td>0.858887</td>\n",
       "      <td>0.484402</td>\n",
       "      <td>0.632393</td>\n",
       "      <td>0.614045</td>\n",
       "      <td>0.543919</td>\n",
       "      <td>0.552691</td>\n",
       "      <td>0.570194</td>\n",
       "      <td>0.717585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690737</td>\n",
       "      <td>0.641339</td>\n",
       "      <td>0.656595</td>\n",
       "      <td>0.450083</td>\n",
       "      <td>0.688218</td>\n",
       "      <td>0.750231</td>\n",
       "      <td>0.316372</td>\n",
       "      <td>0.498188</td>\n",
       "      <td>0.591070</td>\n",
       "      <td>0.582232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.505534</td>\n",
       "      <td>0.207065</td>\n",
       "      <td>0.739005</td>\n",
       "      <td>0.709763</td>\n",
       "      <td>0.300488</td>\n",
       "      <td>0.635785</td>\n",
       "      <td>0.358194</td>\n",
       "      <td>0.491674</td>\n",
       "      <td>0.437550</td>\n",
       "      <td>0.219782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743368</td>\n",
       "      <td>0.804610</td>\n",
       "      <td>0.475199</td>\n",
       "      <td>0.518513</td>\n",
       "      <td>0.406487</td>\n",
       "      <td>0.683743</td>\n",
       "      <td>0.694362</td>\n",
       "      <td>0.234373</td>\n",
       "      <td>0.488018</td>\n",
       "      <td>0.396864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.571270</td>\n",
       "      <td>0.364624</td>\n",
       "      <td>0.735611</td>\n",
       "      <td>0.491911</td>\n",
       "      <td>0.444938</td>\n",
       "      <td>0.613327</td>\n",
       "      <td>0.515356</td>\n",
       "      <td>0.270034</td>\n",
       "      <td>0.505128</td>\n",
       "      <td>0.566877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.703225</td>\n",
       "      <td>0.578954</td>\n",
       "      <td>0.550854</td>\n",
       "      <td>0.585899</td>\n",
       "      <td>0.738020</td>\n",
       "      <td>0.662480</td>\n",
       "      <td>0.603837</td>\n",
       "      <td>0.512024</td>\n",
       "      <td>0.562290</td>\n",
       "      <td>0.481909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.340574</td>\n",
       "      <td>0.654984</td>\n",
       "      <td>0.596355</td>\n",
       "      <td>0.066808</td>\n",
       "      <td>0.418925</td>\n",
       "      <td>0.380828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.658266</td>\n",
       "      <td>0.157832</td>\n",
       "      <td>0.107333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962423</td>\n",
       "      <td>0.568866</td>\n",
       "      <td>0.284087</td>\n",
       "      <td>0.445437</td>\n",
       "      <td>0.455444</td>\n",
       "      <td>0.725736</td>\n",
       "      <td>0.758887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.571741</td>\n",
       "      <td>0.470365</td>\n",
       "      <td>0.647884</td>\n",
       "      <td>0.622471</td>\n",
       "      <td>0.310818</td>\n",
       "      <td>0.547469</td>\n",
       "      <td>0.577685</td>\n",
       "      <td>0.270034</td>\n",
       "      <td>0.634797</td>\n",
       "      <td>0.389650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570743</td>\n",
       "      <td>0.490791</td>\n",
       "      <td>0.478895</td>\n",
       "      <td>0.459808</td>\n",
       "      <td>0.459454</td>\n",
       "      <td>0.721296</td>\n",
       "      <td>0.350108</td>\n",
       "      <td>0.091472</td>\n",
       "      <td>0.501330</td>\n",
       "      <td>0.617022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.675428</td>\n",
       "      <td>0.416393</td>\n",
       "      <td>0.590864</td>\n",
       "      <td>0.524188</td>\n",
       "      <td>0.404908</td>\n",
       "      <td>0.672791</td>\n",
       "      <td>0.650731</td>\n",
       "      <td>0.792566</td>\n",
       "      <td>0.720792</td>\n",
       "      <td>0.697522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742742</td>\n",
       "      <td>0.372918</td>\n",
       "      <td>0.496598</td>\n",
       "      <td>0.064714</td>\n",
       "      <td>0.462040</td>\n",
       "      <td>0.555308</td>\n",
       "      <td>0.400091</td>\n",
       "      <td>0.678005</td>\n",
       "      <td>0.437266</td>\n",
       "      <td>0.364008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.409530</td>\n",
       "      <td>0.368601</td>\n",
       "      <td>0.357377</td>\n",
       "      <td>0.648136</td>\n",
       "      <td>0.437972</td>\n",
       "      <td>0.743013</td>\n",
       "      <td>0.422572</td>\n",
       "      <td>0.270034</td>\n",
       "      <td>0.656510</td>\n",
       "      <td>0.798003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.733308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641079</td>\n",
       "      <td>0.452659</td>\n",
       "      <td>0.709939</td>\n",
       "      <td>0.800680</td>\n",
       "      <td>0.456804</td>\n",
       "      <td>0.667805</td>\n",
       "      <td>0.509036</td>\n",
       "      <td>0.442845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 838 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ABCB1     ABCB6     ABCG2      ABL2     ACADM       ACD       ACE  \\\n",
       "0    0.913055  0.671207  0.629264  0.440491  0.462644  0.610824  0.385954   \n",
       "1    0.611207  0.444017  0.793442  0.498055  0.651154  0.619572  0.560882   \n",
       "2    0.260945  0.798929  0.722790  0.381359  0.308776  0.567881  0.339427   \n",
       "3    0.470353  0.513469  0.858887  0.484402  0.632393  0.614045  0.543919   \n",
       "4    0.505534  0.207065  0.739005  0.709763  0.300488  0.635785  0.358194   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "176  0.571270  0.364624  0.735611  0.491911  0.444938  0.613327  0.515356   \n",
       "177  0.340574  0.654984  0.596355  0.066808  0.418925  0.380828  0.000000   \n",
       "178  0.571741  0.470365  0.647884  0.622471  0.310818  0.547469  0.577685   \n",
       "179  0.675428  0.416393  0.590864  0.524188  0.404908  0.672791  0.650731   \n",
       "180  0.409530  0.368601  0.357377  0.648136  0.437972  0.743013  0.422572   \n",
       "\n",
       "         ACE2      ACHE      ACO2  ...     XRCC4     XRCC6      YAP1  \\\n",
       "0    0.599127  0.716215  0.805240  ...  0.590076  0.495999  0.350519   \n",
       "1    0.864018  0.399519  0.596648  ...  0.766505  0.686270  0.533618   \n",
       "2    0.345563  0.524427  0.514349  ...  0.775910  0.727119  0.469887   \n",
       "3    0.552691  0.570194  0.717585  ...  0.690737  0.641339  0.656595   \n",
       "4    0.491674  0.437550  0.219782  ...  0.743368  0.804610  0.475199   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "176  0.270034  0.505128  0.566877  ...  0.703225  0.578954  0.550854   \n",
       "177  0.658266  0.157832  0.107333  ...  0.962423  0.568866  0.284087   \n",
       "178  0.270034  0.634797  0.389650  ...  0.570743  0.490791  0.478895   \n",
       "179  0.792566  0.720792  0.697522  ...  0.742742  0.372918  0.496598   \n",
       "180  0.270034  0.656510  0.798003  ...  0.733308  1.000000  0.641079   \n",
       "\n",
       "         YBX1      YBX3       YY1    ZBTB17      ZHX2     ZMIZ1  ZMPSTE24  \n",
       "0    0.653839  0.589186  0.283179  0.394627  0.559013  0.297360  0.527435  \n",
       "1    0.470341  0.610968  0.578967  0.378959  0.378272  0.571863  0.520119  \n",
       "2    0.413257  0.472842  0.806014  0.443946  0.886840  0.409127  0.651230  \n",
       "3    0.450083  0.688218  0.750231  0.316372  0.498188  0.591070  0.582232  \n",
       "4    0.518513  0.406487  0.683743  0.694362  0.234373  0.488018  0.396864  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "176  0.585899  0.738020  0.662480  0.603837  0.512024  0.562290  0.481909  \n",
       "177  0.445437  0.455444  0.725736  0.758887  0.000000  0.000000  0.515290  \n",
       "178  0.459808  0.459454  0.721296  0.350108  0.091472  0.501330  0.617022  \n",
       "179  0.064714  0.462040  0.555308  0.400091  0.678005  0.437266  0.364008  \n",
       "180  0.452659  0.709939  0.800680  0.456804  0.667805  0.509036  0.442845  \n",
       "\n",
       "[181 rows x 838 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = genes.columns\n",
    "d = scaler.fit_transform(genes)\n",
    "genes = pd.DataFrame(d, columns=names)\n",
    "genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='../../../Data/PPT-Ohmnet/mRCC_big_pool/Second big pool/network_edges_mrcc_2776_genes_839_nodes.tsv'\n",
    "data = pd.read_csv(path, delimiter='\\t')\n",
    "edge_index1=data[data.columns[1]].to_numpy()\n",
    "edge_index2=data[data.columns[2]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = np.concatenate((edge_index1, edge_index2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EPO', 'EPO', 'EPO', ..., 'SMAD4', 'SMAD4', 'GATA3'], dtype=object)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "838"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(edge_index)\n",
    "len(list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index1 = le.transform(edge_index1)\n",
    "edge_index2 = le.transform(edge_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = [edge_index1]+[edge_index2]\n",
    "edge_index = np.array(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[245, 245, 245, ..., 692, 693, 245],\n",
       "       [342, 301, 246, ..., 694, 694, 302]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[245, 245, 245,  ..., 692, 693, 245],\n",
       "        [342, 301, 246,  ..., 694, 694, 302]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edge_index, dtype=torch.int64)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[838], edge_index=[2, 8425], y=[1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_2952/1747583445.py:11: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  x = torch.tensor([b], dtype=torch.long).reshape([-1])\n"
     ]
    }
   ],
   "source": [
    "list_data_0=[]\n",
    "list_data_1=[]\n",
    "total_data=[]\n",
    "for g in range(len(genes)):\n",
    "  b=[]\n",
    "  for i in genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    # a.append(Y[g])\n",
    "    a.append(i*100)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.long).reshape([-1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y.iloc[g]], dtype=torch.float).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  total_data.append(data)\n",
    "  if y == 0:\n",
    "    list_data_0.append(data)\n",
    "  else:\n",
    "    list_data_1.append(data)\n",
    "\n",
    "print(list_data_0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algunas estadísticas del grafo:\n",
      "Número de Características: 1\n",
      "Número de Nodos: 838\n",
      "Número de bordes: 8425\n",
      "Grado promedio de nodos: 10.05\n",
      "¿Contiene nodos aislados?: False\n",
      "¿Contiene autoloops?: False\n",
      "¿Es no dirigido?: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'contains_isolated_nodes' is deprecated, use 'has_isolated_nodes' instead\n",
      "  warnings.warn(out)\n",
      "C:\\Users\\sandr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'contains_self_loops' is deprecated, use 'has_self_loops' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "def plot_graph(data,description=True):\n",
    "    edges_raw = data.edge_index.numpy()\n",
    "    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
    "    labels = data.x.numpy()\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(list(range(np.max(edges_raw))))\n",
    "    G.add_edges_from(edges)\n",
    "    plt.subplot(111)\n",
    "    options = {\n",
    "       'node_size': 500,\n",
    "       'width': 1,\n",
    "    }\n",
    "    nx.draw(G, with_labels=description, node_color=labels.tolist(), cmap=plt.cm.tab10, font_weight='bold', **options)\n",
    "    plt.show()\n",
    "    # print(len(nx.node_connected_component(G, 127)))\n",
    "    # print((nx.node_connected_component(G, 665)))\n",
    "    # print((nx.node_connected_component(G, 778)))\n",
    "\n",
    "\n",
    "plot_graph(data,True)\n",
    "print(\"Algunas estadísticas del grafo:\")\n",
    "print(f'Número de Características: {data.num_features}')\n",
    "print(f'Número de Nodos: {data.num_nodes}')\n",
    "print(f'Número de bordes: {data.num_edges}')\n",
    "print(f'Grado promedio de nodos: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'¿Contiene nodos aislados?: {data.contains_isolated_nodes()}')\n",
    "print(f'¿Contiene autoloops?: {data.contains_self_loops()}')\n",
    "print(f'¿Es no dirigido?: {data.is_undirected()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Patient sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 838\n",
      "Number of charcateristics per node: 1\n",
      "Number of edges: 8425\n",
      "Average node degree: 10.05\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n"
     ]
    }
   ],
   "source": [
    "data = list_data_0[0]\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of charcateristics per node: {data.num_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import SAGPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 6\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.dim = dim\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GraphConv(embed_dim, dim)\n",
    "        self.pool1 = SAGPooling(dim, ratio=0.1)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.pool2 = SAGPooling(dim, ratio=0.1)\n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=101, embedding_dim=embed_dim)\n",
    "        self.lin1 = torch.nn.Linear(1200, 500)\n",
    "        self.lin2 = torch.nn.Linear(500, 10)\n",
    "        self.lin3 = torch.nn.Linear(500, 1)\n",
    "        self.act1 = torch.nn.RReLU()\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = torch.tensor(x) #.to(torch.int)\n",
    "        # print(x.long())\n",
    "        x = self.item_embedding(x)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        # x = F.relu(self.conv2(x, edge_index))\n",
    "        # x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        # x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 #+ x2\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        # x = self.lin2(x)\n",
    "        # x = self.act1(x)\n",
    "        x = F.dropout(x, p=0.8, training=self.training)\n",
    "        x = torch.sigmoid(self.lin3(x)).squeeze(1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(output, data.y.squeeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        for i in range(len(output)):\n",
    "            if output[i]>0.5:\n",
    "                output[i]=1\n",
    "            else:\n",
    "                output[i]=0\n",
    "            if output[i]==data.y[i]:\n",
    "                correct=correct+1\n",
    "    # print(\"Correct: \"+str(correct) +\" of \"+str(len(loader.dataset)))\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "kf=StratifiedKFold(n_splits=5, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  [ 30  32  33  34  40  41  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 31 35 36 37 38 39 42]\n",
      "144\n",
      "37\n",
      "Net(\n",
      "  (conv1): GraphConv(6, 600)\n",
      "  (pool1): SAGPooling(GraphConv, 600, ratio=0.1, multiplier=1.0)\n",
      "  (conv2): GraphConv(600, 600)\n",
      "  (pool2): SAGPooling(GraphConv, 600, ratio=0.1, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 6)\n",
      "  (lin1): Linear(in_features=1200, out_features=500, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=500, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_2952/922151203.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x) #.to(torch.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.9369, Train Acc: 0.4722, Test Acc: 0.4595\n",
      "Epoch: 002, Loss: 0.8724, Train Acc: 0.4722, Test Acc: 0.4595\n",
      "Epoch: 003, Loss: 0.7807, Train Acc: 0.4722, Test Acc: 0.4595\n",
      "Epoch: 004, Loss: 0.8597, Train Acc: 0.4722, Test Acc: 0.4595\n",
      "Epoch: 005, Loss: 0.7641, Train Acc: 0.4722, Test Acc: 0.4595\n",
      "Epoch: 006, Loss: 0.7719, Train Acc: 0.5000, Test Acc: 0.4595\n",
      "Epoch: 007, Loss: 0.7161, Train Acc: 0.5278, Test Acc: 0.5405\n",
      "Epoch: 008, Loss: 0.7174, Train Acc: 0.4722, Test Acc: 0.4595\n",
      "Epoch: 009, Loss: 0.7438, Train Acc: 0.5556, Test Acc: 0.4324\n",
      "Epoch: 010, Loss: 0.7171, Train Acc: 0.4792, Test Acc: 0.4595\n",
      "Epoch: 011, Loss: 0.7407, Train Acc: 0.5208, Test Acc: 0.4054\n",
      "Epoch: 012, Loss: 0.7010, Train Acc: 0.4722, Test Acc: 0.4595\n",
      "Epoch: 013, Loss: 0.7375, Train Acc: 0.5069, Test Acc: 0.4324\n",
      "Epoch: 014, Loss: 0.6939, Train Acc: 0.5069, Test Acc: 0.4324\n",
      "Epoch: 015, Loss: 0.7089, Train Acc: 0.5903, Test Acc: 0.4324\n",
      "Epoch: 016, Loss: 0.7271, Train Acc: 0.5139, Test Acc: 0.4324\n",
      "Epoch: 017, Loss: 0.7193, Train Acc: 0.5000, Test Acc: 0.4324\n",
      "Epoch: 018, Loss: 0.7072, Train Acc: 0.5625, Test Acc: 0.3784\n",
      "Epoch: 019, Loss: 0.7342, Train Acc: 0.5764, Test Acc: 0.4324\n",
      "Epoch: 020, Loss: 0.7182, Train Acc: 0.4722, Test Acc: 0.4595\n",
      "Epoch: 021, Loss: 0.6816, Train Acc: 0.4792, Test Acc: 0.4595\n",
      "Epoch: 022, Loss: 0.7094, Train Acc: 0.4722, Test Acc: 0.4595\n",
      "Epoch: 023, Loss: 0.7225, Train Acc: 0.5764, Test Acc: 0.3784\n",
      "Epoch: 024, Loss: 0.7116, Train Acc: 0.5694, Test Acc: 0.4054\n",
      "Epoch: 025, Loss: 0.6932, Train Acc: 0.5764, Test Acc: 0.4324\n",
      "Epoch: 026, Loss: 0.7044, Train Acc: 0.5417, Test Acc: 0.3784\n",
      "Epoch: 027, Loss: 0.7319, Train Acc: 0.4722, Test Acc: 0.4595\n",
      "Epoch: 028, Loss: 0.7129, Train Acc: 0.5417, Test Acc: 0.3514\n",
      "Epoch: 029, Loss: 0.7115, Train Acc: 0.5278, Test Acc: 0.4324\n",
      "Epoch: 030, Loss: 0.6945, Train Acc: 0.5417, Test Acc: 0.4324\n",
      "Epoch: 031, Loss: 0.7019, Train Acc: 0.5417, Test Acc: 0.4324\n",
      "Epoch: 032, Loss: 0.7019, Train Acc: 0.5347, Test Acc: 0.4054\n",
      "Epoch: 033, Loss: 0.7082, Train Acc: 0.4861, Test Acc: 0.4595\n",
      "Epoch: 034, Loss: 0.6913, Train Acc: 0.5278, Test Acc: 0.4054\n",
      "Epoch: 035, Loss: 0.7107, Train Acc: 0.5486, Test Acc: 0.3514\n",
      "Epoch: 036, Loss: 0.6939, Train Acc: 0.5208, Test Acc: 0.4865\n",
      "Epoch: 037, Loss: 0.7052, Train Acc: 0.5486, Test Acc: 0.3514\n",
      "Epoch: 038, Loss: 0.7037, Train Acc: 0.5069, Test Acc: 0.4595\n",
      "Epoch: 039, Loss: 0.7144, Train Acc: 0.5208, Test Acc: 0.4865\n",
      "Epoch: 040, Loss: 0.7008, Train Acc: 0.5486, Test Acc: 0.3784\n",
      "Epoch: 041, Loss: 0.6964, Train Acc: 0.5208, Test Acc: 0.5135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2952/3620620967.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2952/2743475341.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(loader)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2952/922151203.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgmp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_geometric\\nn\\pool\\sag_pool.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, edge_index, edge_attr, batch, attn)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mattn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mattn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mattn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mattn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_score\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_geometric\\nn\\conv\\graph_conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, edge_index, edge_weight, size)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# propagate_type: (x: OptPairTensor, edge_weight: OptTensor)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n\u001b[0m\u001b[0;32m     80\u001b[0m                              size=size)\n\u001b[0;32m     81\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin_rel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    399\u001b[0m                         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecomp_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m                 coll_dict = self.__collect__(self.__user_args__, edge_index,\n\u001b[0m\u001b[0;32m    402\u001b[0m                                              size, kwargs)\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36m__collect__\u001b[1;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__set_size__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__lift__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m                 \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36m__lift__\u001b[1;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m'CUDA'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "test_avg = []\n",
    "for train_index, test_index in kf.split(total_data, Y):\n",
    "    train_dataset=[]\n",
    "    test_dataset=[]\n",
    "    print(\"TRAIN: \", train_index, \"TEST:\", test_index)\n",
    "    for i in train_index:\n",
    "        train_dataset.append(total_data[i])\n",
    "    for i in test_index:\n",
    "        test_dataset.append(total_data[i])\n",
    "\n",
    "    print(len(train_dataset))\n",
    "    print(len(test_dataset))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = Net(dim=600)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "    # optimizer = torch.optim.Adadelta(model.parameters(), lr=0.7) # 35 epochs\n",
    "\n",
    "    train_epoch=[]\n",
    "    test_epoch=[]\n",
    "    epoch = 1\n",
    "    train_acc=0\n",
    "    while epoch < 150:\n",
    "        loss = train(epoch)\n",
    "        train_acc = test(train_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        train_epoch.append(train_acc)\n",
    "        test_epoch.append(test_acc)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "            f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        epoch +=1\n",
    "\n",
    "    plt.plot(train_epoch, color=\"red\")\n",
    "    plt.plot(test_epoch, color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    test_avg.append(test_acc)\n",
    "\n",
    "print('Test accuracy: '+ str(np.array(test_avg).mean()))\n",
    "print('Test stv: '+ str(np.array(test_avg).std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb15f1e0f376981e7b6e1fc44ae8b8146823f10f258bcd6e448b0230b889fc06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
