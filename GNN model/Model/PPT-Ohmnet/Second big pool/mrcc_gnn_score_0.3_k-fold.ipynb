{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cpu.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gene matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APAF1</th>\n",
       "      <th>ARID1A</th>\n",
       "      <th>ATM</th>\n",
       "      <th>BAP1</th>\n",
       "      <th>EPAS1</th>\n",
       "      <th>ERBB2</th>\n",
       "      <th>FLT1</th>\n",
       "      <th>FLT4</th>\n",
       "      <th>GSTP1</th>\n",
       "      <th>HSPB1</th>\n",
       "      <th>...</th>\n",
       "      <th>RNF139</th>\n",
       "      <th>SETD2</th>\n",
       "      <th>SLC2A1</th>\n",
       "      <th>SOD2</th>\n",
       "      <th>TGM2</th>\n",
       "      <th>TP53</th>\n",
       "      <th>TSC1</th>\n",
       "      <th>TSC2</th>\n",
       "      <th>VEGFA</th>\n",
       "      <th>VHL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.668769</td>\n",
       "      <td>33.848026</td>\n",
       "      <td>35.942429</td>\n",
       "      <td>33.677294</td>\n",
       "      <td>37.95811</td>\n",
       "      <td>35.32243</td>\n",
       "      <td>33.69326</td>\n",
       "      <td>30.79376</td>\n",
       "      <td>36.48088</td>\n",
       "      <td>38.25591</td>\n",
       "      <td>...</td>\n",
       "      <td>32.46554</td>\n",
       "      <td>32.58565</td>\n",
       "      <td>33.38586</td>\n",
       "      <td>38.67433</td>\n",
       "      <td>38.50142</td>\n",
       "      <td>33.83518</td>\n",
       "      <td>32.93402</td>\n",
       "      <td>34.93520</td>\n",
       "      <td>37.79678</td>\n",
       "      <td>32.30615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.337493</td>\n",
       "      <td>33.843513</td>\n",
       "      <td>35.988225</td>\n",
       "      <td>32.643149</td>\n",
       "      <td>38.83281</td>\n",
       "      <td>33.71706</td>\n",
       "      <td>35.56873</td>\n",
       "      <td>33.38444</td>\n",
       "      <td>36.21403</td>\n",
       "      <td>37.41814</td>\n",
       "      <td>...</td>\n",
       "      <td>32.27190</td>\n",
       "      <td>33.19915</td>\n",
       "      <td>33.69538</td>\n",
       "      <td>38.64559</td>\n",
       "      <td>34.33752</td>\n",
       "      <td>34.44810</td>\n",
       "      <td>33.16630</td>\n",
       "      <td>35.08304</td>\n",
       "      <td>40.09193</td>\n",
       "      <td>32.19988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.818198</td>\n",
       "      <td>33.516005</td>\n",
       "      <td>36.193587</td>\n",
       "      <td>32.368866</td>\n",
       "      <td>37.19345</td>\n",
       "      <td>33.38917</td>\n",
       "      <td>34.21918</td>\n",
       "      <td>33.34670</td>\n",
       "      <td>35.34069</td>\n",
       "      <td>37.94992</td>\n",
       "      <td>...</td>\n",
       "      <td>32.55514</td>\n",
       "      <td>32.84628</td>\n",
       "      <td>36.23588</td>\n",
       "      <td>40.50559</td>\n",
       "      <td>35.50178</td>\n",
       "      <td>35.41980</td>\n",
       "      <td>33.63282</td>\n",
       "      <td>34.79244</td>\n",
       "      <td>38.22308</td>\n",
       "      <td>31.49147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.601293</td>\n",
       "      <td>34.197698</td>\n",
       "      <td>36.578348</td>\n",
       "      <td>31.895400</td>\n",
       "      <td>39.46713</td>\n",
       "      <td>33.22340</td>\n",
       "      <td>36.25593</td>\n",
       "      <td>34.21029</td>\n",
       "      <td>35.36208</td>\n",
       "      <td>37.86790</td>\n",
       "      <td>...</td>\n",
       "      <td>33.19823</td>\n",
       "      <td>33.68316</td>\n",
       "      <td>34.41938</td>\n",
       "      <td>38.99231</td>\n",
       "      <td>35.77236</td>\n",
       "      <td>34.18862</td>\n",
       "      <td>32.88250</td>\n",
       "      <td>35.02014</td>\n",
       "      <td>39.94908</td>\n",
       "      <td>32.11538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.593121</td>\n",
       "      <td>33.351460</td>\n",
       "      <td>36.807497</td>\n",
       "      <td>33.968348</td>\n",
       "      <td>38.49884</td>\n",
       "      <td>33.40876</td>\n",
       "      <td>35.39769</td>\n",
       "      <td>34.92401</td>\n",
       "      <td>34.26885</td>\n",
       "      <td>35.26187</td>\n",
       "      <td>...</td>\n",
       "      <td>30.89813</td>\n",
       "      <td>34.63036</td>\n",
       "      <td>34.59911</td>\n",
       "      <td>38.41437</td>\n",
       "      <td>33.47112</td>\n",
       "      <td>34.91241</td>\n",
       "      <td>33.44515</td>\n",
       "      <td>35.01310</td>\n",
       "      <td>39.31564</td>\n",
       "      <td>33.33646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>32.603769</td>\n",
       "      <td>34.133940</td>\n",
       "      <td>35.318612</td>\n",
       "      <td>33.843872</td>\n",
       "      <td>39.13826</td>\n",
       "      <td>33.62978</td>\n",
       "      <td>35.17642</td>\n",
       "      <td>33.60519</td>\n",
       "      <td>35.75912</td>\n",
       "      <td>37.34151</td>\n",
       "      <td>...</td>\n",
       "      <td>32.12573</td>\n",
       "      <td>33.34867</td>\n",
       "      <td>36.50807</td>\n",
       "      <td>35.15898</td>\n",
       "      <td>34.57504</td>\n",
       "      <td>35.39631</td>\n",
       "      <td>32.93248</td>\n",
       "      <td>35.12781</td>\n",
       "      <td>40.48054</td>\n",
       "      <td>31.79913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>33.619701</td>\n",
       "      <td>32.373330</td>\n",
       "      <td>35.771711</td>\n",
       "      <td>32.519967</td>\n",
       "      <td>35.86338</td>\n",
       "      <td>31.25871</td>\n",
       "      <td>32.24347</td>\n",
       "      <td>31.63139</td>\n",
       "      <td>37.02994</td>\n",
       "      <td>38.71080</td>\n",
       "      <td>...</td>\n",
       "      <td>34.27276</td>\n",
       "      <td>32.16275</td>\n",
       "      <td>33.97705</td>\n",
       "      <td>38.85295</td>\n",
       "      <td>32.38354</td>\n",
       "      <td>32.04003</td>\n",
       "      <td>32.62658</td>\n",
       "      <td>33.78873</td>\n",
       "      <td>37.41392</td>\n",
       "      <td>31.66344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>33.316811</td>\n",
       "      <td>34.118843</td>\n",
       "      <td>36.008091</td>\n",
       "      <td>33.115209</td>\n",
       "      <td>37.91340</td>\n",
       "      <td>32.66502</td>\n",
       "      <td>35.55199</td>\n",
       "      <td>33.43254</td>\n",
       "      <td>35.47039</td>\n",
       "      <td>38.35448</td>\n",
       "      <td>...</td>\n",
       "      <td>32.92305</td>\n",
       "      <td>34.01015</td>\n",
       "      <td>34.85694</td>\n",
       "      <td>37.96021</td>\n",
       "      <td>36.65499</td>\n",
       "      <td>33.34126</td>\n",
       "      <td>32.81059</td>\n",
       "      <td>35.24316</td>\n",
       "      <td>38.72091</td>\n",
       "      <td>32.39461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>33.046782</td>\n",
       "      <td>33.833796</td>\n",
       "      <td>37.008936</td>\n",
       "      <td>32.895151</td>\n",
       "      <td>37.96870</td>\n",
       "      <td>33.57688</td>\n",
       "      <td>35.18870</td>\n",
       "      <td>33.74302</td>\n",
       "      <td>33.76634</td>\n",
       "      <td>36.74006</td>\n",
       "      <td>...</td>\n",
       "      <td>31.87160</td>\n",
       "      <td>33.23246</td>\n",
       "      <td>34.24055</td>\n",
       "      <td>37.24924</td>\n",
       "      <td>36.84744</td>\n",
       "      <td>34.98283</td>\n",
       "      <td>34.04810</td>\n",
       "      <td>35.60526</td>\n",
       "      <td>40.53108</td>\n",
       "      <td>32.34561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>32.170042</td>\n",
       "      <td>33.739764</td>\n",
       "      <td>35.937812</td>\n",
       "      <td>33.404526</td>\n",
       "      <td>38.75226</td>\n",
       "      <td>32.10887</td>\n",
       "      <td>32.99715</td>\n",
       "      <td>28.89508</td>\n",
       "      <td>33.23928</td>\n",
       "      <td>37.84644</td>\n",
       "      <td>...</td>\n",
       "      <td>32.47268</td>\n",
       "      <td>32.81781</td>\n",
       "      <td>35.99620</td>\n",
       "      <td>38.54211</td>\n",
       "      <td>37.23935</td>\n",
       "      <td>33.82151</td>\n",
       "      <td>33.82576</td>\n",
       "      <td>35.13995</td>\n",
       "      <td>40.81516</td>\n",
       "      <td>30.34566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         APAF1     ARID1A        ATM       BAP1     EPAS1     ERBB2      FLT1  \\\n",
       "0    32.668769  33.848026  35.942429  33.677294  37.95811  35.32243  33.69326   \n",
       "1    32.337493  33.843513  35.988225  32.643149  38.83281  33.71706  35.56873   \n",
       "2    31.818198  33.516005  36.193587  32.368866  37.19345  33.38917  34.21918   \n",
       "3    32.601293  34.197698  36.578348  31.895400  39.46713  33.22340  36.25593   \n",
       "4    33.593121  33.351460  36.807497  33.968348  38.49884  33.40876  35.39769   \n",
       "..         ...        ...        ...        ...       ...       ...       ...   \n",
       "176  32.603769  34.133940  35.318612  33.843872  39.13826  33.62978  35.17642   \n",
       "177  33.619701  32.373330  35.771711  32.519967  35.86338  31.25871  32.24347   \n",
       "178  33.316811  34.118843  36.008091  33.115209  37.91340  32.66502  35.55199   \n",
       "179  33.046782  33.833796  37.008936  32.895151  37.96870  33.57688  35.18870   \n",
       "180  32.170042  33.739764  35.937812  33.404526  38.75226  32.10887  32.99715   \n",
       "\n",
       "         FLT4     GSTP1     HSPB1  ...    RNF139     SETD2    SLC2A1  \\\n",
       "0    30.79376  36.48088  38.25591  ...  32.46554  32.58565  33.38586   \n",
       "1    33.38444  36.21403  37.41814  ...  32.27190  33.19915  33.69538   \n",
       "2    33.34670  35.34069  37.94992  ...  32.55514  32.84628  36.23588   \n",
       "3    34.21029  35.36208  37.86790  ...  33.19823  33.68316  34.41938   \n",
       "4    34.92401  34.26885  35.26187  ...  30.89813  34.63036  34.59911   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "176  33.60519  35.75912  37.34151  ...  32.12573  33.34867  36.50807   \n",
       "177  31.63139  37.02994  38.71080  ...  34.27276  32.16275  33.97705   \n",
       "178  33.43254  35.47039  38.35448  ...  32.92305  34.01015  34.85694   \n",
       "179  33.74302  33.76634  36.74006  ...  31.87160  33.23246  34.24055   \n",
       "180  28.89508  33.23928  37.84644  ...  32.47268  32.81781  35.99620   \n",
       "\n",
       "         SOD2      TGM2      TP53      TSC1      TSC2     VEGFA       VHL  \n",
       "0    38.67433  38.50142  33.83518  32.93402  34.93520  37.79678  32.30615  \n",
       "1    38.64559  34.33752  34.44810  33.16630  35.08304  40.09193  32.19988  \n",
       "2    40.50559  35.50178  35.41980  33.63282  34.79244  38.22308  31.49147  \n",
       "3    38.99231  35.77236  34.18862  32.88250  35.02014  39.94908  32.11538  \n",
       "4    38.41437  33.47112  34.91241  33.44515  35.01310  39.31564  33.33646  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "176  35.15898  34.57504  35.39631  32.93248  35.12781  40.48054  31.79913  \n",
       "177  38.85295  32.38354  32.04003  32.62658  33.78873  37.41392  31.66344  \n",
       "178  37.96021  36.65499  33.34126  32.81059  35.24316  38.72091  32.39461  \n",
       "179  37.24924  36.84744  34.98283  34.04810  35.60526  40.53108  32.34561  \n",
       "180  38.54211  37.23935  33.82151  33.82576  35.13995  40.81516  30.34566  \n",
       "\n",
       "[181 rows x 32 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes = pd.read_csv('..\\..\\..\\Data\\PPT-Ohmnet\\mRCC_big_pool\\Second big pool/mrcc_protein_matrix_84_genes_32_nodes.csv')\n",
    "Y = genes.Y\n",
    "\n",
    "genes = genes.iloc[:,1:33] \n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APAF1</th>\n",
       "      <th>ARID1A</th>\n",
       "      <th>ATM</th>\n",
       "      <th>BAP1</th>\n",
       "      <th>EPAS1</th>\n",
       "      <th>ERBB2</th>\n",
       "      <th>FLT1</th>\n",
       "      <th>FLT4</th>\n",
       "      <th>GSTP1</th>\n",
       "      <th>HSPB1</th>\n",
       "      <th>...</th>\n",
       "      <th>RNF139</th>\n",
       "      <th>SETD2</th>\n",
       "      <th>SLC2A1</th>\n",
       "      <th>SOD2</th>\n",
       "      <th>TGM2</th>\n",
       "      <th>TP53</th>\n",
       "      <th>TSC1</th>\n",
       "      <th>TSC2</th>\n",
       "      <th>VEGFA</th>\n",
       "      <th>VHL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.610274</td>\n",
       "      <td>0.474298</td>\n",
       "      <td>0.551095</td>\n",
       "      <td>0.703386</td>\n",
       "      <td>0.614968</td>\n",
       "      <td>0.879366</td>\n",
       "      <td>0.485379</td>\n",
       "      <td>0.345731</td>\n",
       "      <td>0.697909</td>\n",
       "      <td>0.655601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547741</td>\n",
       "      <td>0.361620</td>\n",
       "      <td>0.420160</td>\n",
       "      <td>0.542412</td>\n",
       "      <td>0.945549</td>\n",
       "      <td>0.403803</td>\n",
       "      <td>0.411780</td>\n",
       "      <td>0.408244</td>\n",
       "      <td>0.439826</td>\n",
       "      <td>0.681580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.536117</td>\n",
       "      <td>0.472846</td>\n",
       "      <td>0.561963</td>\n",
       "      <td>0.465055</td>\n",
       "      <td>0.796869</td>\n",
       "      <td>0.573713</td>\n",
       "      <td>0.753386</td>\n",
       "      <td>0.679993</td>\n",
       "      <td>0.653713</td>\n",
       "      <td>0.472156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504091</td>\n",
       "      <td>0.518369</td>\n",
       "      <td>0.458930</td>\n",
       "      <td>0.538144</td>\n",
       "      <td>0.301997</td>\n",
       "      <td>0.538341</td>\n",
       "      <td>0.474109</td>\n",
       "      <td>0.451980</td>\n",
       "      <td>0.760074</td>\n",
       "      <td>0.664154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.419872</td>\n",
       "      <td>0.367512</td>\n",
       "      <td>0.610698</td>\n",
       "      <td>0.401843</td>\n",
       "      <td>0.455951</td>\n",
       "      <td>0.511285</td>\n",
       "      <td>0.560534</td>\n",
       "      <td>0.675123</td>\n",
       "      <td>0.509070</td>\n",
       "      <td>0.588599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567938</td>\n",
       "      <td>0.428211</td>\n",
       "      <td>0.777154</td>\n",
       "      <td>0.814317</td>\n",
       "      <td>0.481939</td>\n",
       "      <td>0.751632</td>\n",
       "      <td>0.599295</td>\n",
       "      <td>0.366011</td>\n",
       "      <td>0.499309</td>\n",
       "      <td>0.547991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.595169</td>\n",
       "      <td>0.586761</td>\n",
       "      <td>0.702007</td>\n",
       "      <td>0.292727</td>\n",
       "      <td>0.928781</td>\n",
       "      <td>0.479723</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.786548</td>\n",
       "      <td>0.512613</td>\n",
       "      <td>0.570639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712900</td>\n",
       "      <td>0.642034</td>\n",
       "      <td>0.549619</td>\n",
       "      <td>0.589625</td>\n",
       "      <td>0.523759</td>\n",
       "      <td>0.481384</td>\n",
       "      <td>0.397955</td>\n",
       "      <td>0.433372</td>\n",
       "      <td>0.740142</td>\n",
       "      <td>0.650298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.817191</td>\n",
       "      <td>0.314590</td>\n",
       "      <td>0.756387</td>\n",
       "      <td>0.770463</td>\n",
       "      <td>0.727417</td>\n",
       "      <td>0.515014</td>\n",
       "      <td>0.728944</td>\n",
       "      <td>0.878635</td>\n",
       "      <td>0.331552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194423</td>\n",
       "      <td>0.884044</td>\n",
       "      <td>0.572132</td>\n",
       "      <td>0.503813</td>\n",
       "      <td>0.168091</td>\n",
       "      <td>0.640258</td>\n",
       "      <td>0.548936</td>\n",
       "      <td>0.431290</td>\n",
       "      <td>0.651756</td>\n",
       "      <td>0.850528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.595723</td>\n",
       "      <td>0.566255</td>\n",
       "      <td>0.403055</td>\n",
       "      <td>0.741776</td>\n",
       "      <td>0.860390</td>\n",
       "      <td>0.557095</td>\n",
       "      <td>0.697324</td>\n",
       "      <td>0.708475</td>\n",
       "      <td>0.578371</td>\n",
       "      <td>0.455376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471142</td>\n",
       "      <td>0.556572</td>\n",
       "      <td>0.811248</td>\n",
       "      <td>0.020453</td>\n",
       "      <td>0.338707</td>\n",
       "      <td>0.746476</td>\n",
       "      <td>0.411366</td>\n",
       "      <td>0.465225</td>\n",
       "      <td>0.814298</td>\n",
       "      <td>0.598440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.823141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510581</td>\n",
       "      <td>0.436667</td>\n",
       "      <td>0.179353</td>\n",
       "      <td>0.105657</td>\n",
       "      <td>0.278203</td>\n",
       "      <td>0.453806</td>\n",
       "      <td>0.788844</td>\n",
       "      <td>0.755208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955115</td>\n",
       "      <td>0.253569</td>\n",
       "      <td>0.494212</td>\n",
       "      <td>0.568933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.329281</td>\n",
       "      <td>0.069080</td>\n",
       "      <td>0.386405</td>\n",
       "      <td>0.576190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.755339</td>\n",
       "      <td>0.561399</td>\n",
       "      <td>0.566677</td>\n",
       "      <td>0.573847</td>\n",
       "      <td>0.605671</td>\n",
       "      <td>0.373411</td>\n",
       "      <td>0.750994</td>\n",
       "      <td>0.686199</td>\n",
       "      <td>0.530551</td>\n",
       "      <td>0.677185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650870</td>\n",
       "      <td>0.725580</td>\n",
       "      <td>0.604427</td>\n",
       "      <td>0.436379</td>\n",
       "      <td>0.660174</td>\n",
       "      <td>0.295386</td>\n",
       "      <td>0.378658</td>\n",
       "      <td>0.499349</td>\n",
       "      <td>0.568772</td>\n",
       "      <td>0.696085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.694893</td>\n",
       "      <td>0.469721</td>\n",
       "      <td>0.804191</td>\n",
       "      <td>0.523132</td>\n",
       "      <td>0.617171</td>\n",
       "      <td>0.547024</td>\n",
       "      <td>0.699079</td>\n",
       "      <td>0.726258</td>\n",
       "      <td>0.248326</td>\n",
       "      <td>0.323678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413858</td>\n",
       "      <td>0.526880</td>\n",
       "      <td>0.527218</td>\n",
       "      <td>0.330815</td>\n",
       "      <td>0.689918</td>\n",
       "      <td>0.655716</td>\n",
       "      <td>0.710731</td>\n",
       "      <td>0.606470</td>\n",
       "      <td>0.821350</td>\n",
       "      <td>0.688050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.498633</td>\n",
       "      <td>0.439478</td>\n",
       "      <td>0.549999</td>\n",
       "      <td>0.640524</td>\n",
       "      <td>0.780118</td>\n",
       "      <td>0.267523</td>\n",
       "      <td>0.385904</td>\n",
       "      <td>0.100754</td>\n",
       "      <td>0.161034</td>\n",
       "      <td>0.565940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549350</td>\n",
       "      <td>0.420937</td>\n",
       "      <td>0.747131</td>\n",
       "      <td>0.522780</td>\n",
       "      <td>0.750490</td>\n",
       "      <td>0.400802</td>\n",
       "      <td>0.651068</td>\n",
       "      <td>0.468816</td>\n",
       "      <td>0.860988</td>\n",
       "      <td>0.360103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        APAF1    ARID1A       ATM      BAP1     EPAS1     ERBB2      FLT1  \\\n",
       "0    0.610274  0.474298  0.551095  0.703386  0.614968  0.879366  0.485379   \n",
       "1    0.536117  0.472846  0.561963  0.465055  0.796869  0.573713  0.753386   \n",
       "2    0.419872  0.367512  0.610698  0.401843  0.455951  0.511285  0.560534   \n",
       "3    0.595169  0.586761  0.702007  0.292727  0.928781  0.479723  0.851587   \n",
       "4    0.817191  0.314590  0.756387  0.770463  0.727417  0.515014  0.728944   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "176  0.595723  0.566255  0.403055  0.741776  0.860390  0.557095  0.697324   \n",
       "177  0.823141  0.000000  0.510581  0.436667  0.179353  0.105657  0.278203   \n",
       "178  0.755339  0.561399  0.566677  0.573847  0.605671  0.373411  0.750994   \n",
       "179  0.694893  0.469721  0.804191  0.523132  0.617171  0.547024  0.699079   \n",
       "180  0.498633  0.439478  0.549999  0.640524  0.780118  0.267523  0.385904   \n",
       "\n",
       "         FLT4     GSTP1     HSPB1  ...    RNF139     SETD2    SLC2A1  \\\n",
       "0    0.345731  0.697909  0.655601  ...  0.547741  0.361620  0.420160   \n",
       "1    0.679993  0.653713  0.472156  ...  0.504091  0.518369  0.458930   \n",
       "2    0.675123  0.509070  0.588599  ...  0.567938  0.428211  0.777154   \n",
       "3    0.786548  0.512613  0.570639  ...  0.712900  0.642034  0.549619   \n",
       "4    0.878635  0.331552  0.000000  ...  0.194423  0.884044  0.572132   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "176  0.708475  0.578371  0.455376  ...  0.471142  0.556572  0.811248   \n",
       "177  0.453806  0.788844  0.755208  ...  0.955115  0.253569  0.494212   \n",
       "178  0.686199  0.530551  0.677185  ...  0.650870  0.725580  0.604427   \n",
       "179  0.726258  0.248326  0.323678  ...  0.413858  0.526880  0.527218   \n",
       "180  0.100754  0.161034  0.565940  ...  0.549350  0.420937  0.747131   \n",
       "\n",
       "         SOD2      TGM2      TP53      TSC1      TSC2     VEGFA       VHL  \n",
       "0    0.542412  0.945549  0.403803  0.411780  0.408244  0.439826  0.681580  \n",
       "1    0.538144  0.301997  0.538341  0.474109  0.451980  0.760074  0.664154  \n",
       "2    0.814317  0.481939  0.751632  0.599295  0.366011  0.499309  0.547991  \n",
       "3    0.589625  0.523759  0.481384  0.397955  0.433372  0.740142  0.650298  \n",
       "4    0.503813  0.168091  0.640258  0.548936  0.431290  0.651756  0.850528  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "176  0.020453  0.338707  0.746476  0.411366  0.465225  0.814298  0.598440  \n",
       "177  0.568933  0.000000  0.009761  0.329281  0.069080  0.386405  0.576190  \n",
       "178  0.436379  0.660174  0.295386  0.378658  0.499349  0.568772  0.696085  \n",
       "179  0.330815  0.689918  0.655716  0.710731  0.606470  0.821350  0.688050  \n",
       "180  0.522780  0.750490  0.400802  0.651068  0.468816  0.860988  0.360103  \n",
       "\n",
       "[181 rows x 32 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = genes.columns\n",
    "d = scaler.fit_transform(genes)\n",
    "genes = pd.DataFrame(d, columns=names)\n",
    "genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='../../../Data/PPT-Ohmnet/mRCC_big_pool/Second big pool/network_edges_mrcc_84_genes_32_nodes.tsv'\n",
    "data = pd.read_csv(path, delimiter='\\t')\n",
    "edge_index1=data[data.columns[1]].to_numpy()\n",
    "edge_index2=data[data.columns[2]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = np.concatenate((edge_index1, edge_index2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['VHL', 'VHL', 'VHL', 'VHL', 'VHL', 'VHL', 'VHL', 'GSTP1', 'GSTP1',\n",
       "       'TGM2', 'TGM2', 'SETD2', 'ERBB2', 'ERBB2', 'FLT1', 'FLT1', 'FLT4',\n",
       "       'NDRG1', 'NF2', 'NF2', 'PIK3CA', 'MTOR', 'MTOR', 'APAF1', 'KDR',\n",
       "       'TSC1', 'RELA', 'RELA', 'RELA', 'ATM', 'MAPK8', 'PTEN', 'PTEN',\n",
       "       'ARID1A', 'PTGS2', 'HSPB1', 'HSPD1', 'SLC2A1', 'RNF139', 'ATM',\n",
       "       'TGM2', 'TP53', 'SOD2', 'EPAS1', 'TGM2', 'MAPK8', 'RELA', 'PAK1',\n",
       "       'TP53', 'NF2', 'PAK1', 'VEGFA', 'KDR', 'KDR', 'TP53', 'PAK1',\n",
       "       'TSC1', 'PTEN', 'TP53', 'MAPK8', 'TP53', 'VEGFA', 'TSC2', 'IL6',\n",
       "       'TP53', 'ATM', 'TP53', 'TP53', 'TP53', 'BAP1', 'TP53', 'TP53',\n",
       "       'TP53', 'PTEN'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(edge_index)\n",
    "len(list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index1 = le.transform(edge_index1)\n",
    "edge_index2 = le.transform(edge_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = [edge_index1]+[edge_index2]\n",
    "edge_index = np.array(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31, 31, 31, 31, 31, 31, 31,  8,  8, 26, 26, 23,  5,  5,  6,  6,\n",
       "         7, 15, 16, 16, 18, 14, 14,  0, 12, 28, 21, 21, 21,  2, 13, 19,\n",
       "        19,  1, 20,  9, 10],\n",
       "       [24, 22,  2, 26, 27, 25,  4, 26, 13, 21, 17, 27, 16, 17, 30, 12,\n",
       "        12, 27, 17, 28, 19, 27, 13, 27, 30, 29, 11, 27,  2, 27, 27, 27,\n",
       "         3, 27, 27, 27, 19]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31, 31, 31, 31, 31, 31, 31,  8,  8, 26, 26, 23,  5,  5,  6,  6,  7, 15,\n",
       "         16, 16, 18, 14, 14,  0, 12, 28, 21, 21, 21,  2, 13, 19, 19,  1, 20,  9,\n",
       "         10],\n",
       "        [24, 22,  2, 26, 27, 25,  4, 26, 13, 21, 17, 27, 16, 17, 30, 12, 12, 27,\n",
       "         17, 28, 19, 27, 13, 27, 30, 29, 11, 27,  2, 27, 27, 27,  3, 27, 27, 27,\n",
       "         19]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edge_index, dtype=torch.int64)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[32], edge_index=[2, 37], y=[1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_14916/1747583445.py:11: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  x = torch.tensor([b], dtype=torch.long).reshape([-1])\n"
     ]
    }
   ],
   "source": [
    "list_data_0=[]\n",
    "list_data_1=[]\n",
    "total_data=[]\n",
    "for g in range(len(genes)):\n",
    "  b=[]\n",
    "  for i in genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    # a.append(Y[g])\n",
    "    a.append(i*100)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.long).reshape([-1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y.iloc[g]], dtype=torch.float).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  total_data.append(data)\n",
    "  if y == 0:\n",
    "    list_data_0.append(data)\n",
    "  else:\n",
    "    list_data_1.append(data)\n",
    "\n",
    "print(list_data_0[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Patient sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 32\n",
      "Number of charcateristics per node: 1\n",
      "Number of edges: 37\n",
      "Average node degree: 1.16\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n"
     ]
    }
   ],
   "source": [
    "data = list_data_0[0]\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of charcateristics per node: {data.num_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import SAGPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.dim = dim\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GraphConv(embed_dim, dim)\n",
    "        self.pool1 = SAGPooling(dim, ratio=0.5)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.pool2 = SAGPooling(dim, ratio=0.5)\n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=101, embedding_dim=embed_dim)\n",
    "        self.lin1 = torch.nn.Linear(512, 50)\n",
    "        self.lin2 = torch.nn.Linear(100, 10)\n",
    "        self.lin3 = torch.nn.Linear(50, 1)\n",
    "        self.act1 = torch.nn.RReLU()\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = torch.tensor(x) #.to(torch.int)\n",
    "        # print(x.long())\n",
    "        x = self.item_embedding(x)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        # x = F.relu(self.conv2(x, edge_index))\n",
    "        # x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        # x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 #+ x2\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        # x = self.lin2(x)\n",
    "        # x = self.act1(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = torch.sigmoid(self.lin3(x)).squeeze(1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(output, data.y.squeeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        for i in range(len(output)):\n",
    "            if output[i]>0.5:\n",
    "                output[i]=1\n",
    "            else:\n",
    "                output[i]=0\n",
    "            if output[i]==data.y[i]:\n",
    "                correct=correct+1\n",
    "    # print(\"Correct: \"+str(correct) +\" of \"+str(len(loader.dataset)))\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "kf=StratifiedKFold(n_splits=5, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  [ 30  32  33  34  40  41  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 31 35 36 37 38 39 42]\n",
      "144\n",
      "37\n",
      "Net(\n",
      "  (conv1): GraphConv(32, 256)\n",
      "  (pool1): SAGPooling(GraphConv, 256, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(256, 256)\n",
      "  (pool2): SAGPooling(GraphConv, 256, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 32)\n",
      "  (lin1): Linear(in_features=512, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.6974, Train Acc: 0.5972, Test Acc: 0.5946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_14916/1956110764.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x) #.to(torch.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.6862, Train Acc: 0.6667, Test Acc: 0.4595\n",
      "Epoch: 003, Loss: 0.6861, Train Acc: 0.6250, Test Acc: 0.5405\n",
      "Epoch: 004, Loss: 0.6662, Train Acc: 0.6458, Test Acc: 0.6216\n",
      "Epoch: 005, Loss: 0.6791, Train Acc: 0.6528, Test Acc: 0.5676\n",
      "Epoch: 006, Loss: 0.6507, Train Acc: 0.7014, Test Acc: 0.6486\n",
      "Epoch: 007, Loss: 0.6544, Train Acc: 0.7292, Test Acc: 0.5676\n",
      "Epoch: 008, Loss: 0.6363, Train Acc: 0.7431, Test Acc: 0.5405\n",
      "Epoch: 009, Loss: 0.6325, Train Acc: 0.7361, Test Acc: 0.5135\n",
      "Epoch: 010, Loss: 0.5743, Train Acc: 0.7778, Test Acc: 0.6757\n",
      "Epoch: 011, Loss: 0.5584, Train Acc: 0.8194, Test Acc: 0.6757\n",
      "Epoch: 012, Loss: 0.5415, Train Acc: 0.7986, Test Acc: 0.6757\n",
      "Epoch: 013, Loss: 0.5037, Train Acc: 0.6944, Test Acc: 0.5946\n",
      "Epoch: 014, Loss: 0.4727, Train Acc: 0.7778, Test Acc: 0.4595\n",
      "Epoch: 015, Loss: 0.5356, Train Acc: 0.8681, Test Acc: 0.5946\n",
      "Epoch: 016, Loss: 0.3681, Train Acc: 0.5417, Test Acc: 0.4324\n",
      "Epoch: 017, Loss: 0.7708, Train Acc: 0.8889, Test Acc: 0.5135\n",
      "Epoch: 018, Loss: 0.3251, Train Acc: 0.9167, Test Acc: 0.6486\n",
      "Epoch: 019, Loss: 0.2817, Train Acc: 0.9236, Test Acc: 0.6216\n",
      "Epoch: 020, Loss: 0.2516, Train Acc: 0.9514, Test Acc: 0.6216\n",
      "Epoch: 021, Loss: 0.2227, Train Acc: 0.9514, Test Acc: 0.6486\n",
      "Epoch: 022, Loss: 0.1809, Train Acc: 0.9583, Test Acc: 0.6216\n",
      "Epoch: 023, Loss: 0.1773, Train Acc: 0.9653, Test Acc: 0.6216\n",
      "Epoch: 024, Loss: 0.1315, Train Acc: 0.9722, Test Acc: 0.5676\n",
      "Epoch: 025, Loss: 0.1210, Train Acc: 0.9792, Test Acc: 0.5405\n",
      "Epoch: 026, Loss: 0.1131, Train Acc: 0.9861, Test Acc: 0.5405\n",
      "Epoch: 027, Loss: 0.0873, Train Acc: 0.9931, Test Acc: 0.5676\n",
      "Epoch: 028, Loss: 0.0853, Train Acc: 0.9931, Test Acc: 0.6757\n",
      "Epoch: 029, Loss: 0.0767, Train Acc: 1.0000, Test Acc: 0.6757\n",
      "Epoch: 030, Loss: 0.0536, Train Acc: 1.0000, Test Acc: 0.6216\n",
      "Epoch: 031, Loss: 0.0393, Train Acc: 1.0000, Test Acc: 0.6216\n",
      "Epoch: 032, Loss: 0.0312, Train Acc: 1.0000, Test Acc: 0.6216\n",
      "Epoch: 033, Loss: 0.0241, Train Acc: 1.0000, Test Acc: 0.6216\n",
      "Epoch: 034, Loss: 0.0226, Train Acc: 1.0000, Test Acc: 0.5676\n",
      "Epoch: 035, Loss: 0.0162, Train Acc: 1.0000, Test Acc: 0.5946\n",
      "Epoch: 036, Loss: 0.0152, Train Acc: 1.0000, Test Acc: 0.5676\n",
      "Epoch: 037, Loss: 0.0103, Train Acc: 1.0000, Test Acc: 0.5676\n",
      "Epoch: 038, Loss: 0.0099, Train Acc: 1.0000, Test Acc: 0.5676\n",
      "Epoch: 039, Loss: 0.0094, Train Acc: 1.0000, Test Acc: 0.6486\n",
      "Epoch: 040, Loss: 0.0102, Train Acc: 1.0000, Test Acc: 0.5405\n",
      "Epoch: 041, Loss: 0.0062, Train Acc: 1.0000, Test Acc: 0.5405\n",
      "Epoch: 042, Loss: 0.0068, Train Acc: 1.0000, Test Acc: 0.5676\n",
      "Epoch: 043, Loss: 0.0068, Train Acc: 0.9931, Test Acc: 0.5676\n",
      "Epoch: 044, Loss: 0.0966, Train Acc: 1.0000, Test Acc: 0.6486\n",
      "Epoch: 045, Loss: 0.0151, Train Acc: 1.0000, Test Acc: 0.6216\n",
      "Epoch: 046, Loss: 0.0054, Train Acc: 1.0000, Test Acc: 0.5946\n",
      "Epoch: 047, Loss: 0.0048, Train Acc: 1.0000, Test Acc: 0.5946\n",
      "Epoch: 048, Loss: 0.0069, Train Acc: 1.0000, Test Acc: 0.5946\n",
      "Epoch: 049, Loss: 0.0041, Train Acc: 1.0000, Test Acc: 0.6216\n",
      "Epoch: 050, Loss: 0.0053, Train Acc: 1.0000, Test Acc: 0.5676\n",
      "Epoch: 051, Loss: 0.0030, Train Acc: 1.0000, Test Acc: 0.5946\n",
      "Epoch: 052, Loss: 0.0035, Train Acc: 1.0000, Test Acc: 0.5676\n",
      "Epoch: 053, Loss: 0.0036, Train Acc: 1.0000, Test Acc: 0.5405\n",
      "Epoch: 054, Loss: 0.0044, Train Acc: 1.0000, Test Acc: 0.5405\n",
      "Epoch: 055, Loss: 0.0045, Train Acc: 1.0000, Test Acc: 0.5676\n",
      "Epoch: 056, Loss: 0.0015, Train Acc: 1.0000, Test Acc: 0.5135\n",
      "Epoch: 057, Loss: 0.0016, Train Acc: 1.0000, Test Acc: 0.5135\n",
      "Epoch: 058, Loss: 0.0016, Train Acc: 1.0000, Test Acc: 0.5135\n",
      "Epoch: 059, Loss: 0.0026, Train Acc: 1.0000, Test Acc: 0.5135\n",
      "Epoch: 060, Loss: 0.0018, Train Acc: 1.0000, Test Acc: 0.5135\n",
      "Epoch: 061, Loss: 0.0027, Train Acc: 1.0000, Test Acc: 0.5405\n",
      "Epoch: 062, Loss: 0.0020, Train Acc: 1.0000, Test Acc: 0.5676\n",
      "Epoch: 063, Loss: 0.0025, Train Acc: 1.0000, Test Acc: 0.5405\n",
      "Epoch: 064, Loss: 0.0009, Train Acc: 1.0000, Test Acc: 0.5405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  31  35  36  37  38  39\n",
      "  42  70  72  73  75  76  77  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180] TEST: [30 32 33 34 40 41 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n",
      " 61 62 63 64 65 66 67 68 69 71 74 78]\n",
      "145\n",
      "36\n",
      "Net(\n",
      "  (conv1): GraphConv(32, 256)\n",
      "  (pool1): SAGPooling(GraphConv, 256, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(256, 256)\n",
      "  (pool2): SAGPooling(GraphConv, 256, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 32)\n",
      "  (lin1): Linear(in_features=512, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.7787, Train Acc: 0.4690, Test Acc: 0.4722\n",
      "Epoch: 002, Loss: 0.6968, Train Acc: 0.5379, Test Acc: 0.4167\n",
      "Epoch: 003, Loss: 0.6823, Train Acc: 0.4690, Test Acc: 0.4722\n",
      "Epoch: 004, Loss: 0.6860, Train Acc: 0.6069, Test Acc: 0.3889\n",
      "Epoch: 005, Loss: 0.6855, Train Acc: 0.6276, Test Acc: 0.5556\n",
      "Epoch: 006, Loss: 0.6778, Train Acc: 0.5379, Test Acc: 0.5000\n",
      "Epoch: 007, Loss: 0.6756, Train Acc: 0.6966, Test Acc: 0.4722\n",
      "Epoch: 008, Loss: 0.6526, Train Acc: 0.6000, Test Acc: 0.4722\n",
      "Epoch: 009, Loss: 0.6421, Train Acc: 0.7172, Test Acc: 0.5000\n",
      "Epoch: 010, Loss: 0.6502, Train Acc: 0.7379, Test Acc: 0.4444\n",
      "Epoch: 011, Loss: 0.6234, Train Acc: 0.6828, Test Acc: 0.4722\n",
      "Epoch: 012, Loss: 0.5673, Train Acc: 0.7448, Test Acc: 0.5278\n",
      "Epoch: 013, Loss: 0.5573, Train Acc: 0.8069, Test Acc: 0.5556\n",
      "Epoch: 014, Loss: 0.5189, Train Acc: 0.8000, Test Acc: 0.5833\n",
      "Epoch: 015, Loss: 0.4734, Train Acc: 0.8621, Test Acc: 0.5278\n",
      "Epoch: 016, Loss: 0.4285, Train Acc: 0.9172, Test Acc: 0.5556\n",
      "Epoch: 017, Loss: 0.3374, Train Acc: 0.9241, Test Acc: 0.5000\n",
      "Epoch: 018, Loss: 0.3191, Train Acc: 0.9793, Test Acc: 0.5556\n",
      "Epoch: 019, Loss: 0.2275, Train Acc: 0.9862, Test Acc: 0.5000\n",
      "Epoch: 020, Loss: 0.2841, Train Acc: 0.9931, Test Acc: 0.5000\n",
      "Epoch: 021, Loss: 0.1826, Train Acc: 0.9931, Test Acc: 0.5278\n",
      "Epoch: 022, Loss: 0.0984, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 023, Loss: 0.1070, Train Acc: 1.0000, Test Acc: 0.5833\n",
      "Epoch: 024, Loss: 0.0578, Train Acc: 0.9862, Test Acc: 0.5833\n",
      "Epoch: 025, Loss: 0.0495, Train Acc: 1.0000, Test Acc: 0.5278\n",
      "Epoch: 026, Loss: 0.0385, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 027, Loss: 0.0402, Train Acc: 1.0000, Test Acc: 0.5278\n",
      "Epoch: 028, Loss: 0.0230, Train Acc: 1.0000, Test Acc: 0.5556\n",
      "Epoch: 029, Loss: 0.0239, Train Acc: 1.0000, Test Acc: 0.5278\n",
      "Epoch: 030, Loss: 0.0283, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 031, Loss: 0.0114, Train Acc: 1.0000, Test Acc: 0.5278\n",
      "Epoch: 032, Loss: 0.0177, Train Acc: 1.0000, Test Acc: 0.5278\n",
      "Epoch: 033, Loss: 0.0107, Train Acc: 1.0000, Test Acc: 0.5833\n",
      "Epoch: 034, Loss: 0.0104, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 035, Loss: 0.0121, Train Acc: 1.0000, Test Acc: 0.5278\n",
      "Epoch: 036, Loss: 0.0083, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 037, Loss: 0.0106, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 038, Loss: 0.0079, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 039, Loss: 0.0069, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 040, Loss: 0.0057, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 041, Loss: 0.0026, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 042, Loss: 0.0035, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 043, Loss: 0.0095, Train Acc: 1.0000, Test Acc: 0.5278\n",
      "Epoch: 044, Loss: 0.0038, Train Acc: 1.0000, Test Acc: 0.5278\n",
      "Epoch: 045, Loss: 0.0030, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 046, Loss: 0.0046, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 047, Loss: 0.0028, Train Acc: 1.0000, Test Acc: 0.5278\n",
      "Epoch: 048, Loss: 0.0045, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 049, Loss: 0.0030, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 050, Loss: 0.0017, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 051, Loss: 0.0018, Train Acc: 1.0000, Test Acc: 0.5278\n",
      "Epoch: 052, Loss: 0.0011, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 053, Loss: 0.0018, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 054, Loss: 0.0046, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 055, Loss: 0.0033, Train Acc: 1.0000, Test Acc: 0.5278\n",
      "Epoch: 056, Loss: 0.0011, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 057, Loss: 0.0014, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 058, Loss: 0.0028, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 059, Loss: 0.0013, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 060, Loss: 0.0012, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 061, Loss: 0.0006, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 062, Loss: 0.0005, Train Acc: 1.0000, Test Acc: 0.4722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 063, Loss: 0.0017, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 064, Loss: 0.0005, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  71  74\n",
      "  78 107 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180] TEST: [ 70  72  73  75  76  77  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 108 109]\n",
      "145\n",
      "36\n",
      "Net(\n",
      "  (conv1): GraphConv(32, 256)\n",
      "  (pool1): SAGPooling(GraphConv, 256, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(256, 256)\n",
      "  (pool2): SAGPooling(GraphConv, 256, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 32)\n",
      "  (lin1): Linear(in_features=512, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.8388, Train Acc: 0.5310, Test Acc: 0.5278\n",
      "Epoch: 002, Loss: 0.6912, Train Acc: 0.5310, Test Acc: 0.5278\n",
      "Epoch: 003, Loss: 0.6912, Train Acc: 0.5310, Test Acc: 0.5278\n",
      "Epoch: 004, Loss: 0.6912, Train Acc: 0.5310, Test Acc: 0.5278\n",
      "Epoch: 005, Loss: 0.6878, Train Acc: 0.5448, Test Acc: 0.5278\n",
      "Epoch: 006, Loss: 0.6880, Train Acc: 0.5379, Test Acc: 0.5278\n",
      "Epoch: 007, Loss: 0.6855, Train Acc: 0.5586, Test Acc: 0.5000\n",
      "Epoch: 008, Loss: 0.6741, Train Acc: 0.7034, Test Acc: 0.4167\n",
      "Epoch: 009, Loss: 0.6754, Train Acc: 0.7034, Test Acc: 0.4444\n",
      "Epoch: 010, Loss: 0.6523, Train Acc: 0.7103, Test Acc: 0.5278\n",
      "Epoch: 011, Loss: 0.6522, Train Acc: 0.7448, Test Acc: 0.4722\n",
      "Epoch: 012, Loss: 0.6196, Train Acc: 0.6828, Test Acc: 0.5556\n",
      "Epoch: 013, Loss: 0.6049, Train Acc: 0.7034, Test Acc: 0.5000\n",
      "Epoch: 014, Loss: 0.6084, Train Acc: 0.7241, Test Acc: 0.4722\n",
      "Epoch: 015, Loss: 0.5546, Train Acc: 0.6690, Test Acc: 0.5556\n",
      "Epoch: 016, Loss: 0.5232, Train Acc: 0.6897, Test Acc: 0.5278\n",
      "Epoch: 017, Loss: 0.5576, Train Acc: 0.7931, Test Acc: 0.4722\n",
      "Epoch: 018, Loss: 0.5766, Train Acc: 0.8345, Test Acc: 0.4722\n",
      "Epoch: 019, Loss: 0.4491, Train Acc: 0.8759, Test Acc: 0.4167\n",
      "Epoch: 020, Loss: 0.3799, Train Acc: 0.7103, Test Acc: 0.5278\n",
      "Epoch: 021, Loss: 0.3967, Train Acc: 0.9034, Test Acc: 0.3611\n",
      "Epoch: 022, Loss: 0.4609, Train Acc: 0.9379, Test Acc: 0.4444\n",
      "Epoch: 023, Loss: 0.3502, Train Acc: 0.8828, Test Acc: 0.4444\n",
      "Epoch: 024, Loss: 0.5232, Train Acc: 0.9310, Test Acc: 0.4444\n",
      "Epoch: 025, Loss: 0.2479, Train Acc: 0.9655, Test Acc: 0.4167\n",
      "Epoch: 026, Loss: 0.1601, Train Acc: 0.9655, Test Acc: 0.4444\n",
      "Epoch: 027, Loss: 0.1516, Train Acc: 0.9862, Test Acc: 0.4444\n",
      "Epoch: 028, Loss: 0.1174, Train Acc: 0.9793, Test Acc: 0.4444\n",
      "Epoch: 029, Loss: 0.1106, Train Acc: 0.9793, Test Acc: 0.5000\n",
      "Epoch: 030, Loss: 0.0848, Train Acc: 0.9862, Test Acc: 0.4722\n",
      "Epoch: 031, Loss: 0.1005, Train Acc: 0.9862, Test Acc: 0.4444\n",
      "Epoch: 032, Loss: 0.0789, Train Acc: 0.9862, Test Acc: 0.3889\n",
      "Epoch: 033, Loss: 0.0752, Train Acc: 0.9862, Test Acc: 0.4167\n",
      "Epoch: 034, Loss: 0.0688, Train Acc: 0.9862, Test Acc: 0.3889\n",
      "Epoch: 035, Loss: 0.0728, Train Acc: 0.9862, Test Acc: 0.3611\n",
      "Epoch: 036, Loss: 0.0620, Train Acc: 0.9862, Test Acc: 0.3611\n",
      "Epoch: 037, Loss: 0.0690, Train Acc: 0.9931, Test Acc: 0.3611\n",
      "Epoch: 038, Loss: 0.0447, Train Acc: 0.9931, Test Acc: 0.3333\n",
      "Epoch: 039, Loss: 0.0402, Train Acc: 0.9931, Test Acc: 0.3611\n",
      "Epoch: 040, Loss: 0.0305, Train Acc: 0.9931, Test Acc: 0.3611\n",
      "Epoch: 041, Loss: 0.0335, Train Acc: 0.9931, Test Acc: 0.3889\n",
      "Epoch: 042, Loss: 0.0317, Train Acc: 0.9931, Test Acc: 0.3333\n",
      "Epoch: 043, Loss: 0.0388, Train Acc: 0.9931, Test Acc: 0.3611\n",
      "Epoch: 044, Loss: 0.0722, Train Acc: 1.0000, Test Acc: 0.4167\n",
      "Epoch: 045, Loss: 0.0150, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 046, Loss: 0.0084, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 047, Loss: 0.0093, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 048, Loss: 0.0138, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 049, Loss: 0.0078, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 050, Loss: 0.0050, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 051, Loss: 0.0047, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 052, Loss: 0.0043, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 053, Loss: 0.0038, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 054, Loss: 0.0044, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 055, Loss: 0.0063, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 056, Loss: 0.0039, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 057, Loss: 0.0019, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 058, Loss: 0.0015, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 059, Loss: 0.0020, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 060, Loss: 0.0040, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 061, Loss: 0.0026, Train Acc: 1.0000, Test Acc: 0.3889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 062, Loss: 0.0011, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 063, Loss: 0.0020, Train Acc: 1.0000, Test Acc: 0.4167\n",
      "Epoch: 064, Loss: 0.0009, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 108\n",
      " 109 140 142 144 145 146 147 148 149 150 151 154 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180] TEST: [107 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 141 143 152 153 155]\n",
      "145\n",
      "36\n",
      "Net(\n",
      "  (conv1): GraphConv(32, 256)\n",
      "  (pool1): SAGPooling(GraphConv, 256, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(256, 256)\n",
      "  (pool2): SAGPooling(GraphConv, 256, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 32)\n",
      "  (lin1): Linear(in_features=512, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.7778, Train Acc: 0.4828, Test Acc: 0.5000\n",
      "Epoch: 002, Loss: 0.6948, Train Acc: 0.4759, Test Acc: 0.4722\n",
      "Epoch: 003, Loss: 0.6958, Train Acc: 0.4690, Test Acc: 0.4722\n",
      "Epoch: 004, Loss: 0.6935, Train Acc: 0.5586, Test Acc: 0.4722\n",
      "Epoch: 005, Loss: 0.6837, Train Acc: 0.5655, Test Acc: 0.4722\n",
      "Epoch: 006, Loss: 0.6857, Train Acc: 0.5931, Test Acc: 0.4444\n",
      "Epoch: 007, Loss: 0.6780, Train Acc: 0.6345, Test Acc: 0.4444\n",
      "Epoch: 008, Loss: 0.6615, Train Acc: 0.6138, Test Acc: 0.4722\n",
      "Epoch: 009, Loss: 0.6457, Train Acc: 0.6690, Test Acc: 0.4167\n",
      "Epoch: 010, Loss: 0.6367, Train Acc: 0.6759, Test Acc: 0.4722\n",
      "Epoch: 011, Loss: 0.5909, Train Acc: 0.6897, Test Acc: 0.4722\n",
      "Epoch: 012, Loss: 0.6432, Train Acc: 0.6276, Test Acc: 0.4722\n",
      "Epoch: 013, Loss: 0.5430, Train Acc: 0.7172, Test Acc: 0.4167\n",
      "Epoch: 014, Loss: 0.5334, Train Acc: 0.7793, Test Acc: 0.4722\n",
      "Epoch: 015, Loss: 0.4996, Train Acc: 0.8138, Test Acc: 0.3889\n",
      "Epoch: 016, Loss: 0.4818, Train Acc: 0.8000, Test Acc: 0.3889\n",
      "Epoch: 017, Loss: 0.4328, Train Acc: 0.8483, Test Acc: 0.4167\n",
      "Epoch: 018, Loss: 0.4436, Train Acc: 0.8000, Test Acc: 0.4444\n",
      "Epoch: 019, Loss: 0.4035, Train Acc: 0.8621, Test Acc: 0.4722\n",
      "Epoch: 020, Loss: 0.3465, Train Acc: 0.8828, Test Acc: 0.4444\n",
      "Epoch: 021, Loss: 0.3314, Train Acc: 0.9103, Test Acc: 0.4444\n",
      "Epoch: 022, Loss: 0.4040, Train Acc: 0.8966, Test Acc: 0.3611\n",
      "Epoch: 023, Loss: 0.2927, Train Acc: 0.9310, Test Acc: 0.3611\n",
      "Epoch: 024, Loss: 0.3713, Train Acc: 0.9241, Test Acc: 0.4167\n",
      "Epoch: 025, Loss: 0.2952, Train Acc: 0.9448, Test Acc: 0.4167\n",
      "Epoch: 026, Loss: 0.2114, Train Acc: 0.9448, Test Acc: 0.3889\n",
      "Epoch: 027, Loss: 0.1683, Train Acc: 0.9655, Test Acc: 0.3611\n",
      "Epoch: 028, Loss: 0.1313, Train Acc: 0.9655, Test Acc: 0.4167\n",
      "Epoch: 029, Loss: 0.1452, Train Acc: 0.9655, Test Acc: 0.4167\n",
      "Epoch: 030, Loss: 0.1234, Train Acc: 0.9655, Test Acc: 0.4167\n",
      "Epoch: 031, Loss: 0.1350, Train Acc: 0.9655, Test Acc: 0.3889\n",
      "Epoch: 032, Loss: 0.1377, Train Acc: 0.9655, Test Acc: 0.3889\n",
      "Epoch: 033, Loss: 0.1383, Train Acc: 0.9655, Test Acc: 0.3889\n",
      "Epoch: 034, Loss: 0.1248, Train Acc: 0.9793, Test Acc: 0.4167\n",
      "Epoch: 035, Loss: 0.0822, Train Acc: 0.9793, Test Acc: 0.4444\n",
      "Epoch: 036, Loss: 0.0796, Train Acc: 0.9793, Test Acc: 0.4444\n",
      "Epoch: 037, Loss: 0.1315, Train Acc: 0.9793, Test Acc: 0.3611\n",
      "Epoch: 038, Loss: 0.1403, Train Acc: 0.9931, Test Acc: 0.4444\n",
      "Epoch: 039, Loss: 0.0581, Train Acc: 0.9931, Test Acc: 0.4444\n",
      "Epoch: 040, Loss: 0.0476, Train Acc: 0.9931, Test Acc: 0.4722\n",
      "Epoch: 041, Loss: 0.0415, Train Acc: 0.9931, Test Acc: 0.4444\n",
      "Epoch: 042, Loss: 0.0399, Train Acc: 0.9931, Test Acc: 0.4722\n",
      "Epoch: 043, Loss: 0.0380, Train Acc: 0.9931, Test Acc: 0.4167\n",
      "Epoch: 044, Loss: 0.0221, Train Acc: 1.0000, Test Acc: 0.4167\n",
      "Epoch: 045, Loss: 0.0169, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 046, Loss: 0.0143, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 047, Loss: 0.0139, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 048, Loss: 0.0102, Train Acc: 1.0000, Test Acc: 0.4167\n",
      "Epoch: 049, Loss: 0.0081, Train Acc: 1.0000, Test Acc: 0.4444\n",
      "Epoch: 050, Loss: 0.0073, Train Acc: 1.0000, Test Acc: 0.4167\n",
      "Epoch: 051, Loss: 0.0102, Train Acc: 0.9931, Test Acc: 0.4722\n",
      "Epoch: 052, Loss: 0.0669, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 053, Loss: 0.0071, Train Acc: 1.0000, Test Acc: 0.4444\n",
      "Epoch: 054, Loss: 0.0044, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 055, Loss: 0.0045, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 056, Loss: 0.0059, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 057, Loss: 0.0047, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 058, Loss: 0.0046, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 059, Loss: 0.0023, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 060, Loss: 0.0029, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "Epoch: 061, Loss: 0.0023, Train Acc: 1.0000, Test Acc: 0.5000\n",
      "Epoch: 062, Loss: 0.0024, Train Acc: 1.0000, Test Acc: 0.4444\n",
      "Epoch: 063, Loss: 0.0018, Train Acc: 1.0000, Test Acc: 0.4722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 064, Loss: 0.0027, Train Acc: 1.0000, Test Acc: 0.4722\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 141 143 152 153\n",
      " 155] TEST: [140 142 144 145 146 147 148 149 150 151 154 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180]\n",
      "145\n",
      "36\n",
      "Net(\n",
      "  (conv1): GraphConv(32, 256)\n",
      "  (pool1): SAGPooling(GraphConv, 256, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(256, 256)\n",
      "  (pool2): SAGPooling(GraphConv, 256, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 32)\n",
      "  (lin1): Linear(in_features=512, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.7205, Train Acc: 0.5931, Test Acc: 0.5000\n",
      "Epoch: 002, Loss: 0.6863, Train Acc: 0.5379, Test Acc: 0.5278\n",
      "Epoch: 003, Loss: 0.6949, Train Acc: 0.5448, Test Acc: 0.5278\n",
      "Epoch: 004, Loss: 0.6578, Train Acc: 0.7586, Test Acc: 0.4722\n",
      "Epoch: 005, Loss: 0.6697, Train Acc: 0.6414, Test Acc: 0.5000\n",
      "Epoch: 006, Loss: 0.6199, Train Acc: 0.6069, Test Acc: 0.5556\n",
      "Epoch: 007, Loss: 0.5897, Train Acc: 0.7931, Test Acc: 0.5556\n",
      "Epoch: 008, Loss: 0.5919, Train Acc: 0.6966, Test Acc: 0.5000\n",
      "Epoch: 009, Loss: 0.6781, Train Acc: 0.6759, Test Acc: 0.5278\n",
      "Epoch: 010, Loss: 0.5565, Train Acc: 0.9034, Test Acc: 0.4444\n",
      "Epoch: 011, Loss: 0.4582, Train Acc: 0.8690, Test Acc: 0.3889\n",
      "Epoch: 012, Loss: 0.4172, Train Acc: 0.8552, Test Acc: 0.3611\n",
      "Epoch: 013, Loss: 0.3446, Train Acc: 0.9103, Test Acc: 0.4444\n",
      "Epoch: 014, Loss: 0.3971, Train Acc: 0.9517, Test Acc: 0.3056\n",
      "Epoch: 015, Loss: 0.2873, Train Acc: 0.9793, Test Acc: 0.3333\n",
      "Epoch: 016, Loss: 0.1899, Train Acc: 1.0000, Test Acc: 0.2778\n",
      "Epoch: 017, Loss: 0.2041, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 018, Loss: 0.1135, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 019, Loss: 0.0843, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 020, Loss: 0.0599, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 021, Loss: 0.0405, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 022, Loss: 0.0413, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 023, Loss: 0.0284, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 024, Loss: 0.0234, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 025, Loss: 0.0206, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 026, Loss: 0.0199, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 027, Loss: 0.0173, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 028, Loss: 0.0255, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 029, Loss: 0.0440, Train Acc: 1.0000, Test Acc: 0.4167\n",
      "Epoch: 030, Loss: 0.0138, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 031, Loss: 0.0142, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 032, Loss: 0.0171, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 033, Loss: 0.0088, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 034, Loss: 0.0059, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 035, Loss: 0.0058, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 036, Loss: 0.0045, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 037, Loss: 0.0061, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 038, Loss: 0.0030, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 039, Loss: 0.0055, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 040, Loss: 0.0074, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 041, Loss: 0.0061, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 042, Loss: 0.0065, Train Acc: 1.0000, Test Acc: 0.4167\n",
      "Epoch: 043, Loss: 0.0039, Train Acc: 1.0000, Test Acc: 0.3056\n",
      "Epoch: 044, Loss: 0.0032, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 045, Loss: 0.0030, Train Acc: 1.0000, Test Acc: 0.4167\n",
      "Epoch: 046, Loss: 0.0024, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 047, Loss: 0.0029, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 048, Loss: 0.0016, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 049, Loss: 0.0018, Train Acc: 1.0000, Test Acc: 0.4167\n",
      "Epoch: 050, Loss: 0.0018, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 051, Loss: 0.0010, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 052, Loss: 0.0016, Train Acc: 1.0000, Test Acc: 0.3056\n",
      "Epoch: 053, Loss: 0.0019, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 054, Loss: 0.0008, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Epoch: 055, Loss: 0.0007, Train Acc: 1.0000, Test Acc: 0.3611\n",
      "Epoch: 056, Loss: 0.0009, Train Acc: 1.0000, Test Acc: 0.3056\n",
      "Epoch: 057, Loss: 0.0011, Train Acc: 1.0000, Test Acc: 0.3056\n",
      "Epoch: 058, Loss: 0.0015, Train Acc: 1.0000, Test Acc: 0.3056\n",
      "Epoch: 059, Loss: 0.0021, Train Acc: 1.0000, Test Acc: 0.3889\n",
      "Epoch: 060, Loss: 0.0011, Train Acc: 1.0000, Test Acc: 0.3056\n",
      "Epoch: 061, Loss: 0.0003, Train Acc: 1.0000, Test Acc: 0.3056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 062, Loss: 0.0009, Train Acc: 1.0000, Test Acc: 0.3056\n",
      "Epoch: 063, Loss: 0.0004, Train Acc: 1.0000, Test Acc: 0.3056\n",
      "Epoch: 064, Loss: 0.0009, Train Acc: 1.0000, Test Acc: 0.3333\n",
      "Test accuracy: 0.4414414414414415\n",
      "Test stv: 0.07233903260578652\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "test_avg = []\n",
    "for train_index, test_index in kf.split(total_data, Y):\n",
    "    train_dataset=[]\n",
    "    test_dataset=[]\n",
    "    print(\"TRAIN: \", train_index, \"TEST:\", test_index)\n",
    "    for i in train_index:\n",
    "        train_dataset.append(total_data[i])\n",
    "    for i in test_index:\n",
    "        test_dataset.append(total_data[i])\n",
    "\n",
    "    print(len(train_dataset))\n",
    "    print(len(test_dataset))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = Net(dim=256)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.6)\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(), lr=0.7)\n",
    "\n",
    "    train_epoch=[]\n",
    "    test_epoch=[]\n",
    "    epoch = 1\n",
    "    train_acc=0\n",
    "    while epoch < 65:\n",
    "        loss = train(epoch)\n",
    "        train_acc = test(train_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        train_epoch.append(train_acc)\n",
    "        test_epoch.append(test_acc)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "            f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        epoch +=1\n",
    "\n",
    "    plt.plot(train_epoch, color=\"red\")\n",
    "    plt.plot(test_epoch, color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    test_avg.append(test_acc)\n",
    "\n",
    "print('Test accuracy: '+ str(np.array(test_avg).mean()))\n",
    "print('Test stv: '+ str(np.array(test_avg).std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb15f1e0f376981e7b6e1fc44ae8b8146823f10f258bcd6e448b0230b889fc06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
