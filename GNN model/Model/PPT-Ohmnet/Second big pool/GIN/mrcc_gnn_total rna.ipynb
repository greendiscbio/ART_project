{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cpu.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gene matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AAMP</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA7</th>\n",
       "      <th>ABCB1</th>\n",
       "      <th>ABCB6</th>\n",
       "      <th>...</th>\n",
       "      <th>ZMPSTE24</th>\n",
       "      <th>ZNF106</th>\n",
       "      <th>ZNF24</th>\n",
       "      <th>ZNF274</th>\n",
       "      <th>ZNF473</th>\n",
       "      <th>ZNF513</th>\n",
       "      <th>ZRANB1</th>\n",
       "      <th>ZRSR2</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWINT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.643965</td>\n",
       "      <td>33.279532</td>\n",
       "      <td>34.479355</td>\n",
       "      <td>31.093560</td>\n",
       "      <td>34.740118</td>\n",
       "      <td>35.452910</td>\n",
       "      <td>34.080431</td>\n",
       "      <td>33.456188</td>\n",
       "      <td>36.716012</td>\n",
       "      <td>33.867624</td>\n",
       "      <td>...</td>\n",
       "      <td>32.22373</td>\n",
       "      <td>34.12471</td>\n",
       "      <td>32.12065</td>\n",
       "      <td>30.73579</td>\n",
       "      <td>29.56513</td>\n",
       "      <td>33.18436</td>\n",
       "      <td>31.24377</td>\n",
       "      <td>33.55092</td>\n",
       "      <td>31.66007</td>\n",
       "      <td>30.04056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.866645</td>\n",
       "      <td>33.019435</td>\n",
       "      <td>34.074284</td>\n",
       "      <td>30.162233</td>\n",
       "      <td>34.265360</td>\n",
       "      <td>34.490012</td>\n",
       "      <td>34.191456</td>\n",
       "      <td>32.075265</td>\n",
       "      <td>34.174577</td>\n",
       "      <td>32.703008</td>\n",
       "      <td>...</td>\n",
       "      <td>32.19111</td>\n",
       "      <td>34.06288</td>\n",
       "      <td>32.43414</td>\n",
       "      <td>31.30495</td>\n",
       "      <td>30.12425</td>\n",
       "      <td>31.55522</td>\n",
       "      <td>31.86911</td>\n",
       "      <td>34.40793</td>\n",
       "      <td>31.68666</td>\n",
       "      <td>29.94062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.684228</td>\n",
       "      <td>31.033210</td>\n",
       "      <td>33.878221</td>\n",
       "      <td>31.323227</td>\n",
       "      <td>34.708417</td>\n",
       "      <td>36.122945</td>\n",
       "      <td>33.077576</td>\n",
       "      <td>33.196599</td>\n",
       "      <td>31.225510</td>\n",
       "      <td>34.522351</td>\n",
       "      <td>...</td>\n",
       "      <td>32.77568</td>\n",
       "      <td>36.59738</td>\n",
       "      <td>32.53318</td>\n",
       "      <td>31.43622</td>\n",
       "      <td>30.34865</td>\n",
       "      <td>31.99790</td>\n",
       "      <td>32.58640</td>\n",
       "      <td>34.19171</td>\n",
       "      <td>31.38916</td>\n",
       "      <td>31.95812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.521495</td>\n",
       "      <td>33.009203</td>\n",
       "      <td>34.196192</td>\n",
       "      <td>30.243072</td>\n",
       "      <td>35.123918</td>\n",
       "      <td>34.631350</td>\n",
       "      <td>32.862723</td>\n",
       "      <td>30.327194</td>\n",
       "      <td>32.988641</td>\n",
       "      <td>33.059035</td>\n",
       "      <td>...</td>\n",
       "      <td>32.46805</td>\n",
       "      <td>34.27609</td>\n",
       "      <td>33.50006</td>\n",
       "      <td>30.39592</td>\n",
       "      <td>30.97037</td>\n",
       "      <td>32.64170</td>\n",
       "      <td>32.44003</td>\n",
       "      <td>33.54101</td>\n",
       "      <td>31.71354</td>\n",
       "      <td>30.71732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.219176</td>\n",
       "      <td>31.447751</td>\n",
       "      <td>34.867916</td>\n",
       "      <td>30.632459</td>\n",
       "      <td>33.197341</td>\n",
       "      <td>34.885088</td>\n",
       "      <td>33.046511</td>\n",
       "      <td>32.147897</td>\n",
       "      <td>33.284849</td>\n",
       "      <td>31.488348</td>\n",
       "      <td>...</td>\n",
       "      <td>31.64157</td>\n",
       "      <td>36.30242</td>\n",
       "      <td>32.41124</td>\n",
       "      <td>31.68776</td>\n",
       "      <td>31.55637</td>\n",
       "      <td>34.70525</td>\n",
       "      <td>31.95751</td>\n",
       "      <td>32.64282</td>\n",
       "      <td>33.38645</td>\n",
       "      <td>30.75149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>26.161567</td>\n",
       "      <td>32.365074</td>\n",
       "      <td>34.742167</td>\n",
       "      <td>31.307773</td>\n",
       "      <td>33.113039</td>\n",
       "      <td>33.747546</td>\n",
       "      <td>33.931629</td>\n",
       "      <td>31.740490</td>\n",
       "      <td>33.838323</td>\n",
       "      <td>32.296025</td>\n",
       "      <td>...</td>\n",
       "      <td>32.02075</td>\n",
       "      <td>34.82837</td>\n",
       "      <td>33.17326</td>\n",
       "      <td>31.86836</td>\n",
       "      <td>31.17820</td>\n",
       "      <td>33.59099</td>\n",
       "      <td>32.57067</td>\n",
       "      <td>33.63338</td>\n",
       "      <td>31.81453</td>\n",
       "      <td>30.78532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>26.161567</td>\n",
       "      <td>30.121368</td>\n",
       "      <td>33.792409</td>\n",
       "      <td>31.192059</td>\n",
       "      <td>34.450999</td>\n",
       "      <td>34.371220</td>\n",
       "      <td>31.799811</td>\n",
       "      <td>30.626834</td>\n",
       "      <td>31.895951</td>\n",
       "      <td>33.784466</td>\n",
       "      <td>...</td>\n",
       "      <td>32.16958</td>\n",
       "      <td>33.61960</td>\n",
       "      <td>31.56047</td>\n",
       "      <td>31.59540</td>\n",
       "      <td>29.23127</td>\n",
       "      <td>32.77207</td>\n",
       "      <td>30.78896</td>\n",
       "      <td>31.31709</td>\n",
       "      <td>30.65842</td>\n",
       "      <td>31.50200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>31.822942</td>\n",
       "      <td>31.953984</td>\n",
       "      <td>34.146861</td>\n",
       "      <td>30.360677</td>\n",
       "      <td>33.676334</td>\n",
       "      <td>35.386781</td>\n",
       "      <td>32.611501</td>\n",
       "      <td>31.681398</td>\n",
       "      <td>33.842290</td>\n",
       "      <td>32.838072</td>\n",
       "      <td>...</td>\n",
       "      <td>32.62316</td>\n",
       "      <td>35.20787</td>\n",
       "      <td>32.46942</td>\n",
       "      <td>31.20456</td>\n",
       "      <td>30.95466</td>\n",
       "      <td>32.07850</td>\n",
       "      <td>31.45327</td>\n",
       "      <td>34.13179</td>\n",
       "      <td>32.56414</td>\n",
       "      <td>30.91066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>32.930512</td>\n",
       "      <td>33.255145</td>\n",
       "      <td>35.138362</td>\n",
       "      <td>30.881980</td>\n",
       "      <td>33.808583</td>\n",
       "      <td>36.072286</td>\n",
       "      <td>34.598098</td>\n",
       "      <td>32.909022</td>\n",
       "      <td>34.715293</td>\n",
       "      <td>32.561403</td>\n",
       "      <td>...</td>\n",
       "      <td>31.49508</td>\n",
       "      <td>34.76557</td>\n",
       "      <td>32.63131</td>\n",
       "      <td>32.87016</td>\n",
       "      <td>31.17809</td>\n",
       "      <td>33.03248</td>\n",
       "      <td>31.54103</td>\n",
       "      <td>33.80478</td>\n",
       "      <td>30.91711</td>\n",
       "      <td>28.76312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>33.128697</td>\n",
       "      <td>33.373644</td>\n",
       "      <td>34.747629</td>\n",
       "      <td>27.534864</td>\n",
       "      <td>35.757142</td>\n",
       "      <td>33.916591</td>\n",
       "      <td>34.169145</td>\n",
       "      <td>31.904643</td>\n",
       "      <td>32.476530</td>\n",
       "      <td>32.316413</td>\n",
       "      <td>...</td>\n",
       "      <td>31.84658</td>\n",
       "      <td>34.15309</td>\n",
       "      <td>32.27888</td>\n",
       "      <td>32.26490</td>\n",
       "      <td>32.89932</td>\n",
       "      <td>32.82486</td>\n",
       "      <td>31.10034</td>\n",
       "      <td>33.16714</td>\n",
       "      <td>32.10715</td>\n",
       "      <td>33.26885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 3187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          A1CF     A4GALT       AAAS      AADAT       AAMP      ABCA1  \\\n",
       "0    33.643965  33.279532  34.479355  31.093560  34.740118  35.452910   \n",
       "1    28.866645  33.019435  34.074284  30.162233  34.265360  34.490012   \n",
       "2    26.684228  31.033210  33.878221  31.323227  34.708417  36.122945   \n",
       "3    32.521495  33.009203  34.196192  30.243072  35.123918  34.631350   \n",
       "4    23.219176  31.447751  34.867916  30.632459  33.197341  34.885088   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "176  26.161567  32.365074  34.742167  31.307773  33.113039  33.747546   \n",
       "177  26.161567  30.121368  33.792409  31.192059  34.450999  34.371220   \n",
       "178  31.822942  31.953984  34.146861  30.360677  33.676334  35.386781   \n",
       "179  32.930512  33.255145  35.138362  30.881980  33.808583  36.072286   \n",
       "180  33.128697  33.373644  34.747629  27.534864  35.757142  33.916591   \n",
       "\n",
       "         ABCA2      ABCA7      ABCB1      ABCB6  ...  ZMPSTE24    ZNF106  \\\n",
       "0    34.080431  33.456188  36.716012  33.867624  ...  32.22373  34.12471   \n",
       "1    34.191456  32.075265  34.174577  32.703008  ...  32.19111  34.06288   \n",
       "2    33.077576  33.196599  31.225510  34.522351  ...  32.77568  36.59738   \n",
       "3    32.862723  30.327194  32.988641  33.059035  ...  32.46805  34.27609   \n",
       "4    33.046511  32.147897  33.284849  31.488348  ...  31.64157  36.30242   \n",
       "..         ...        ...        ...        ...  ...       ...       ...   \n",
       "176  33.931629  31.740490  33.838323  32.296025  ...  32.02075  34.82837   \n",
       "177  31.799811  30.626834  31.895951  33.784466  ...  32.16958  33.61960   \n",
       "178  32.611501  31.681398  33.842290  32.838072  ...  32.62316  35.20787   \n",
       "179  34.598098  32.909022  34.715293  32.561403  ...  31.49508  34.76557   \n",
       "180  34.169145  31.904643  32.476530  32.316413  ...  31.84658  34.15309   \n",
       "\n",
       "        ZNF24    ZNF274    ZNF473    ZNF513    ZRANB1     ZRSR2      ZW10  \\\n",
       "0    32.12065  30.73579  29.56513  33.18436  31.24377  33.55092  31.66007   \n",
       "1    32.43414  31.30495  30.12425  31.55522  31.86911  34.40793  31.68666   \n",
       "2    32.53318  31.43622  30.34865  31.99790  32.58640  34.19171  31.38916   \n",
       "3    33.50006  30.39592  30.97037  32.64170  32.44003  33.54101  31.71354   \n",
       "4    32.41124  31.68776  31.55637  34.70525  31.95751  32.64282  33.38645   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "176  33.17326  31.86836  31.17820  33.59099  32.57067  33.63338  31.81453   \n",
       "177  31.56047  31.59540  29.23127  32.77207  30.78896  31.31709  30.65842   \n",
       "178  32.46942  31.20456  30.95466  32.07850  31.45327  34.13179  32.56414   \n",
       "179  32.63131  32.87016  31.17809  33.03248  31.54103  33.80478  30.91711   \n",
       "180  32.27888  32.26490  32.89932  32.82486  31.10034  33.16714  32.10715   \n",
       "\n",
       "        ZWINT  \n",
       "0    30.04056  \n",
       "1    29.94062  \n",
       "2    31.95812  \n",
       "3    30.71732  \n",
       "4    30.75149  \n",
       "..        ...  \n",
       "176  30.78532  \n",
       "177  31.50200  \n",
       "178  30.91066  \n",
       "179  28.76312  \n",
       "180  33.26885  \n",
       "\n",
       "[181 rows x 3187 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes = pd.read_csv('C:/Users/sandr/Documents/ART_project/GNN model/Data/PPT-Ohmnet/mRCC_big_pool/Second big pool/mrcc_protein_matrix_total_RNA_nodes.csv')\n",
    "Y = genes.Y\n",
    "\n",
    "genes = genes.iloc[:,1:3188] \n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AAMP</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA7</th>\n",
       "      <th>ABCB1</th>\n",
       "      <th>ABCB6</th>\n",
       "      <th>...</th>\n",
       "      <th>ZMPSTE24</th>\n",
       "      <th>ZNF106</th>\n",
       "      <th>ZNF24</th>\n",
       "      <th>ZNF274</th>\n",
       "      <th>ZNF473</th>\n",
       "      <th>ZNF513</th>\n",
       "      <th>ZRANB1</th>\n",
       "      <th>ZRSR2</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWINT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.845335</td>\n",
       "      <td>0.649994</td>\n",
       "      <td>0.561587</td>\n",
       "      <td>0.845382</td>\n",
       "      <td>0.653407</td>\n",
       "      <td>0.725634</td>\n",
       "      <td>0.563173</td>\n",
       "      <td>0.874550</td>\n",
       "      <td>0.913055</td>\n",
       "      <td>0.671207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527435</td>\n",
       "      <td>0.426953</td>\n",
       "      <td>0.270376</td>\n",
       "      <td>0.234004</td>\n",
       "      <td>0.606001</td>\n",
       "      <td>0.607362</td>\n",
       "      <td>0.137365</td>\n",
       "      <td>0.530862</td>\n",
       "      <td>0.572566</td>\n",
       "      <td>0.707656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.528869</td>\n",
       "      <td>0.611402</td>\n",
       "      <td>0.433718</td>\n",
       "      <td>0.759865</td>\n",
       "      <td>0.564839</td>\n",
       "      <td>0.444908</td>\n",
       "      <td>0.586608</td>\n",
       "      <td>0.596735</td>\n",
       "      <td>0.611207</td>\n",
       "      <td>0.444017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520119</td>\n",
       "      <td>0.412624</td>\n",
       "      <td>0.363550</td>\n",
       "      <td>0.379268</td>\n",
       "      <td>0.668237</td>\n",
       "      <td>0.356705</td>\n",
       "      <td>0.304874</td>\n",
       "      <td>0.680442</td>\n",
       "      <td>0.579149</td>\n",
       "      <td>0.698606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.384299</td>\n",
       "      <td>0.316698</td>\n",
       "      <td>0.371827</td>\n",
       "      <td>0.866471</td>\n",
       "      <td>0.647493</td>\n",
       "      <td>0.920977</td>\n",
       "      <td>0.351490</td>\n",
       "      <td>0.822326</td>\n",
       "      <td>0.260945</td>\n",
       "      <td>0.798929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.392987</td>\n",
       "      <td>0.412771</td>\n",
       "      <td>0.693215</td>\n",
       "      <td>0.424816</td>\n",
       "      <td>0.497013</td>\n",
       "      <td>0.642703</td>\n",
       "      <td>0.505492</td>\n",
       "      <td>0.881304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.770979</td>\n",
       "      <td>0.609884</td>\n",
       "      <td>0.472201</td>\n",
       "      <td>0.767288</td>\n",
       "      <td>0.725006</td>\n",
       "      <td>0.486115</td>\n",
       "      <td>0.306139</td>\n",
       "      <td>0.245058</td>\n",
       "      <td>0.470353</td>\n",
       "      <td>0.513469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582232</td>\n",
       "      <td>0.462035</td>\n",
       "      <td>0.680360</td>\n",
       "      <td>0.147260</td>\n",
       "      <td>0.762420</td>\n",
       "      <td>0.523870</td>\n",
       "      <td>0.457805</td>\n",
       "      <td>0.529132</td>\n",
       "      <td>0.585805</td>\n",
       "      <td>0.768942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.378205</td>\n",
       "      <td>0.684244</td>\n",
       "      <td>0.803043</td>\n",
       "      <td>0.365597</td>\n",
       "      <td>0.560090</td>\n",
       "      <td>0.344933</td>\n",
       "      <td>0.611347</td>\n",
       "      <td>0.505534</td>\n",
       "      <td>0.207065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396864</td>\n",
       "      <td>0.931642</td>\n",
       "      <td>0.356744</td>\n",
       "      <td>0.476971</td>\n",
       "      <td>0.827648</td>\n",
       "      <td>0.841364</td>\n",
       "      <td>0.328554</td>\n",
       "      <td>0.372364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.772036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.349676</td>\n",
       "      <td>0.514312</td>\n",
       "      <td>0.644549</td>\n",
       "      <td>0.865052</td>\n",
       "      <td>0.349870</td>\n",
       "      <td>0.228449</td>\n",
       "      <td>0.531764</td>\n",
       "      <td>0.529385</td>\n",
       "      <td>0.571270</td>\n",
       "      <td>0.364624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481909</td>\n",
       "      <td>0.590028</td>\n",
       "      <td>0.583230</td>\n",
       "      <td>0.523065</td>\n",
       "      <td>0.785553</td>\n",
       "      <td>0.669926</td>\n",
       "      <td>0.492800</td>\n",
       "      <td>0.545254</td>\n",
       "      <td>0.610809</td>\n",
       "      <td>0.775099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.349676</td>\n",
       "      <td>0.181405</td>\n",
       "      <td>0.344739</td>\n",
       "      <td>0.854427</td>\n",
       "      <td>0.599471</td>\n",
       "      <td>0.410276</td>\n",
       "      <td>0.081779</td>\n",
       "      <td>0.305339</td>\n",
       "      <td>0.340574</td>\n",
       "      <td>0.654984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515290</td>\n",
       "      <td>0.309892</td>\n",
       "      <td>0.103880</td>\n",
       "      <td>0.453398</td>\n",
       "      <td>0.568838</td>\n",
       "      <td>0.543928</td>\n",
       "      <td>0.015536</td>\n",
       "      <td>0.140975</td>\n",
       "      <td>0.324568</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.724704</td>\n",
       "      <td>0.453317</td>\n",
       "      <td>0.456628</td>\n",
       "      <td>0.778087</td>\n",
       "      <td>0.454955</td>\n",
       "      <td>0.706354</td>\n",
       "      <td>0.253111</td>\n",
       "      <td>0.517497</td>\n",
       "      <td>0.571741</td>\n",
       "      <td>0.470365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617022</td>\n",
       "      <td>0.677978</td>\n",
       "      <td>0.374036</td>\n",
       "      <td>0.353646</td>\n",
       "      <td>0.760671</td>\n",
       "      <td>0.437217</td>\n",
       "      <td>0.193484</td>\n",
       "      <td>0.632245</td>\n",
       "      <td>0.796405</td>\n",
       "      <td>0.786450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.798074</td>\n",
       "      <td>0.646375</td>\n",
       "      <td>0.769616</td>\n",
       "      <td>0.825954</td>\n",
       "      <td>0.479626</td>\n",
       "      <td>0.906207</td>\n",
       "      <td>0.672443</td>\n",
       "      <td>0.764471</td>\n",
       "      <td>0.675428</td>\n",
       "      <td>0.416393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364008</td>\n",
       "      <td>0.575474</td>\n",
       "      <td>0.422153</td>\n",
       "      <td>0.778750</td>\n",
       "      <td>0.785541</td>\n",
       "      <td>0.583994</td>\n",
       "      <td>0.216992</td>\n",
       "      <td>0.575170</td>\n",
       "      <td>0.388617</td>\n",
       "      <td>0.591975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.811202</td>\n",
       "      <td>0.663958</td>\n",
       "      <td>0.646273</td>\n",
       "      <td>0.518614</td>\n",
       "      <td>0.843135</td>\n",
       "      <td>0.277733</td>\n",
       "      <td>0.581899</td>\n",
       "      <td>0.562410</td>\n",
       "      <td>0.409530</td>\n",
       "      <td>0.368601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442845</td>\n",
       "      <td>0.433530</td>\n",
       "      <td>0.317404</td>\n",
       "      <td>0.624272</td>\n",
       "      <td>0.977133</td>\n",
       "      <td>0.552050</td>\n",
       "      <td>0.098945</td>\n",
       "      <td>0.463878</td>\n",
       "      <td>0.683258</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 3187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1CF    A4GALT      AAAS     AADAT      AAMP     ABCA1     ABCA2  \\\n",
       "0    0.845335  0.649994  0.561587  0.845382  0.653407  0.725634  0.563173   \n",
       "1    0.528869  0.611402  0.433718  0.759865  0.564839  0.444908  0.586608   \n",
       "2    0.384299  0.316698  0.371827  0.866471  0.647493  0.920977  0.351490   \n",
       "3    0.770979  0.609884  0.472201  0.767288  0.725006  0.486115  0.306139   \n",
       "4    0.154762  0.378205  0.684244  0.803043  0.365597  0.560090  0.344933   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "176  0.349676  0.514312  0.644549  0.865052  0.349870  0.228449  0.531764   \n",
       "177  0.349676  0.181405  0.344739  0.854427  0.599471  0.410276  0.081779   \n",
       "178  0.724704  0.453317  0.456628  0.778087  0.454955  0.706354  0.253111   \n",
       "179  0.798074  0.646375  0.769616  0.825954  0.479626  0.906207  0.672443   \n",
       "180  0.811202  0.663958  0.646273  0.518614  0.843135  0.277733  0.581899   \n",
       "\n",
       "        ABCA7     ABCB1     ABCB6  ...  ZMPSTE24    ZNF106     ZNF24  \\\n",
       "0    0.874550  0.913055  0.671207  ...  0.527435  0.426953  0.270376   \n",
       "1    0.596735  0.611207  0.444017  ...  0.520119  0.412624  0.363550   \n",
       "2    0.822326  0.260945  0.798929  ...  0.651230  1.000000  0.392987   \n",
       "3    0.245058  0.470353  0.513469  ...  0.582232  0.462035  0.680360   \n",
       "4    0.611347  0.505534  0.207065  ...  0.396864  0.931642  0.356744   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "176  0.529385  0.571270  0.364624  ...  0.481909  0.590028  0.583230   \n",
       "177  0.305339  0.340574  0.654984  ...  0.515290  0.309892  0.103880   \n",
       "178  0.517497  0.571741  0.470365  ...  0.617022  0.677978  0.374036   \n",
       "179  0.764471  0.675428  0.416393  ...  0.364008  0.575474  0.422153   \n",
       "180  0.562410  0.409530  0.368601  ...  0.442845  0.433530  0.317404   \n",
       "\n",
       "       ZNF274    ZNF473    ZNF513    ZRANB1     ZRSR2      ZW10     ZWINT  \n",
       "0    0.234004  0.606001  0.607362  0.137365  0.530862  0.572566  0.707656  \n",
       "1    0.379268  0.668237  0.356705  0.304874  0.680442  0.579149  0.698606  \n",
       "2    0.412771  0.693215  0.424816  0.497013  0.642703  0.505492  0.881304  \n",
       "3    0.147260  0.762420  0.523870  0.457805  0.529132  0.585805  0.768942  \n",
       "4    0.476971  0.827648  0.841364  0.328554  0.372364  1.000000  0.772036  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "176  0.523065  0.785553  0.669926  0.492800  0.545254  0.610809  0.775099  \n",
       "177  0.453398  0.568838  0.543928  0.015536  0.140975  0.324568  0.840000  \n",
       "178  0.353646  0.760671  0.437217  0.193484  0.632245  0.796405  0.786450  \n",
       "179  0.778750  0.785541  0.583994  0.216992  0.575170  0.388617  0.591975  \n",
       "180  0.624272  0.977133  0.552050  0.098945  0.463878  0.683258  1.000000  \n",
       "\n",
       "[181 rows x 3187 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = genes.columns\n",
    "d = scaler.fit_transform(genes)\n",
    "genes = pd.DataFrame(d, columns=names)\n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genes, test_genes, Y_train, Y_test = train_test_split(genes, Y, test_size=0.1, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='C:/Users/sandr/Documents/ART_project/GNN model/Data/PPT-Ohmnet/mRCC_big_pool/Second big pool/network_edges_mrcc_total_RNA_nodes.tsv'\n",
    "data = pd.read_csv(path, delimiter='\\t')\n",
    "edge_index1=data[data.columns[1]].to_numpy()\n",
    "edge_index2=data[data.columns[2]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = np.concatenate((edge_index1, edge_index2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NFKB1', 'NFKB1', 'NFKB1', ..., 'ZFY', 'DMBT1', 'TNIP2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3187"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(edge_index)\n",
    "len(list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index1 = le.transform(edge_index1)\n",
    "edge_index2 = le.transform(edge_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = [edge_index1]+[edge_index2]\n",
    "edge_index = np.array(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1741, 1741, 1741, ..., 1337, 2448, 1741],\n",
       "       [ 541,  714, 1512, ..., 3169,  711, 2858]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1741, 1741, 1741,  ..., 1337, 2448, 1741],\n",
       "        [ 541,  714, 1512,  ..., 3169,  711, 2858]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edge_index, dtype=torch.int64)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "train_data=[]\n",
    "for g in range(len(train_genes)):\n",
    "  b=[]\n",
    "  for i in train_genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    # a.append(Y[g])\n",
    "    # a.append(i*100)\n",
    "    a.append(i)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.float).reshape([-1,1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y_train.iloc[g]], dtype=torch.long).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  train_data.append(data)\n",
    "\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "test_data=[]\n",
    "for g in range(len(test_genes)):\n",
    "  b=[]\n",
    "  for i in test_genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    # a.append(Y[g])\n",
    "    # a.append(i*100)\n",
    "    a.append(i)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.float).reshape([-1,1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y_test.iloc[g]], dtype=torch.long).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  test_data.append(data)\n",
    "\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_graph(data,description=True):\n",
    "    edges_raw = data.edge_index.numpy()\n",
    "    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
    "    labels = data.x.numpy()\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(list(range(np.max(edges_raw))))\n",
    "    G.add_edges_from(edges)\n",
    "    plt.subplot(111)\n",
    "    options = {\n",
    "       'node_size': 100,\n",
    "       'width': 1,\n",
    "    }\n",
    "    nx.draw(G, with_labels=description, node_color=labels.tolist(), cmap=plt.cm.tab10, font_weight='bold', **options)\n",
    "    plt.show()\n",
    "\n",
    "plot_graph(data,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Patient sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 3187\n",
      "Number of charcateristics per node: 1\n",
      "Number of edges: 47649\n",
      "Average node degree: 14.95\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n",
      "Number of node features: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of charcateristics per node: {data.num_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "print(f'Number of node features: {data.num_node_features}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import SAGPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "# from torch_geometric.nn import GCNConv, GINConv\n",
    "# from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "# embed_dim = 32\n",
    "\n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, dim_h):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = GCNConv(1, dim_h)\n",
    "#         self.conv2 = GCNConv(dim_h, dim_h)\n",
    "#         self.conv3 = GCNConv(dim_h, dim_h)\n",
    "#         self.lin = Linear(dim_h, 1)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         # Node embeddings \n",
    "#         h = self.conv1(x, edge_index)\n",
    "#         h = h.relu()\n",
    "#         h = self.conv2(h, edge_index)\n",
    "#         h = h.relu()\n",
    "#         h = self.conv3(h, edge_index)\n",
    "\n",
    "#         # Graph-level readout\n",
    "#         hG = global_mean_pool(h, batch)\n",
    "\n",
    "#         # Classifier\n",
    "#         h = F.dropout(hG, p=0.5, training=self.training)\n",
    "#         h = self.lin(h)\n",
    "        \n",
    "#         return F.sigmoid(h).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv, GINConv\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "embed_dim = 32\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, dim_h):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(1, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        self.lin1 = Linear(dim_h*3, dim_h*3)\n",
    "        self.lin2 = Linear(dim_h*3, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Node embeddings \n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "\n",
    "        # Graph-level readout\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    acc = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(output, data.y.squeeze(1))  \n",
    "        total_loss += loss / len(train_loader)\n",
    "        acc += accuracy(output.argmax(dim=1), data.y.squeeze(1)) / len(train_loader)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # loss_all += loss.item() * data.num_graphs\n",
    "    return total_loss, acc\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def validation(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    loss = 0\n",
    "    for data in val_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss += criterion(output, data.y.squeeze(1))/ len(val_loader)\n",
    "        acc += accuracy(output.argmax(dim=1), data.y.squeeze(1)) / len(val_loader)\n",
    "    return loss, acc \n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def test(model, test_data):\n",
    "    acc = 0\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "    for data in test_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        acc += accuracy(output.argmax(dim=1), data.y.squeeze(1)) / len(test_loader)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "kf=StratifiedKFold(n_splits=10, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  [ 15  17  18  19  21  22  23  24  25  26  27  28  29  30  31  32  33  34\n",
      "  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52\n",
      "  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70\n",
      "  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88\n",
      "  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
      " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
      " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
      " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n",
      " 161] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 20]\n",
      "145\n",
      "17\n",
      "Epoch: 000, Loss: 86.8472, Train Acc: 0.4934, Loss: 76.2830, Test Acc: 0.5294\n",
      "Epoch: 001, Loss: 60.0441, Train Acc: 0.4699, Loss: 46.4057, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 53.9226, Train Acc: 0.4761, Loss: 36.0506, Test Acc: 0.4706\n",
      "Epoch: 003, Loss: 32.4937, Train Acc: 0.5364, Loss: 43.9457, Test Acc: 0.4706\n",
      "Epoch: 004, Loss: 38.6865, Train Acc: 0.4989, Loss: 31.9145, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 19.8328, Train Acc: 0.4886, Loss: 1.1919, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 10.1879, Train Acc: 0.5129, Loss: 12.8718, Test Acc: 0.4706\n",
      "Epoch: 007, Loss: 9.7725, Train Acc: 0.5114, Loss: 2.5515, Test Acc: 0.6471\n",
      "Epoch: 008, Loss: 3.2586, Train Acc: 0.5301, Loss: 1.1065, Test Acc: 0.4706\n",
      "Epoch: 009, Loss: 2.1325, Train Acc: 0.4893, Loss: 5.0819, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 4.5748, Train Acc: 0.5364, Loss: 11.0804, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 7.3897, Train Acc: 0.5011, Loss: 12.1681, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 12.6697, Train Acc: 0.5011, Loss: 21.0392, Test Acc: 0.4706\n",
      "Epoch: 013, Loss: 15.1082, Train Acc: 0.5114, Loss: 9.7394, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 20.2728, Train Acc: 0.5114, Loss: 17.4506, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 13.2257, Train Acc: 0.5011, Loss: 6.4688, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 7.1941, Train Acc: 0.4886, Loss: 0.7928, Test Acc: 0.3529\n",
      "Epoch: 017, Loss: 2.1708, Train Acc: 0.5489, Loss: 4.7263, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 4.7461, Train Acc: 0.5114, Loss: 1.5821, Test Acc: 0.4118\n",
      "Epoch: 019, Loss: 3.7656, Train Acc: 0.4574, Loss: 4.3865, Test Acc: 0.4706\n",
      "Epoch: 020, Loss: 2.9654, Train Acc: 0.4768, Loss: 4.0125, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 3.5730, Train Acc: 0.5364, Loss: 4.8889, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 2.0445, Train Acc: 0.4761, Loss: 1.5314, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 1.7863, Train Acc: 0.4864, Loss: 0.6491, Test Acc: 0.4706\n",
      "Epoch: 024, Loss: 1.7279, Train Acc: 0.4761, Loss: 2.4086, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 2.3643, Train Acc: 0.5364, Loss: 6.1493, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 5.9579, Train Acc: 0.5011, Loss: 5.9765, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 7.3889, Train Acc: 0.5011, Loss: 12.9065, Test Acc: 0.4706\n",
      "Epoch: 028, Loss: 9.0102, Train Acc: 0.5114, Loss: 3.2645, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 7.3647, Train Acc: 0.5114, Loss: 14.1279, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 9.5186, Train Acc: 0.5129, Loss: 6.3652, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 4.6419, Train Acc: 0.5316, Loss: 7.9379, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 7.2933, Train Acc: 0.4886, Loss: 3.7693, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 3.9184, Train Acc: 0.5364, Loss: 8.0362, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 7.8354, Train Acc: 0.5011, Loss: 3.9645, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 7.4214, Train Acc: 0.4886, Loss: 4.2307, Test Acc: 0.4706\n",
      "Epoch: 036, Loss: 8.4437, Train Acc: 0.5364, Loss: 5.2416, Test Acc: 0.4706\n",
      "Epoch: 037, Loss: 8.0302, Train Acc: 0.5114, Loss: 5.4296, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 6.7779, Train Acc: 0.4989, Loss: 4.5123, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 3.0709, Train Acc: 0.5129, Loss: 3.0011, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 2.2180, Train Acc: 0.5364, Loss: 1.5673, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 3.9790, Train Acc: 0.5114, Loss: 2.5836, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 4.5927, Train Acc: 0.4989, Loss: 5.1041, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 2.8323, Train Acc: 0.5129, Loss: 2.8244, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 2.2504, Train Acc: 0.5364, Loss: 2.2750, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 1.3546, Train Acc: 0.4864, Loss: 1.1050, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 1.1504, Train Acc: 0.5357, Loss: 1.7587, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 1.0129, Train Acc: 0.4754, Loss: 1.1528, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 1.0218, Train Acc: 0.5342, Loss: 1.0389, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 1.0407, Train Acc: 0.4449, Loss: 1.3131, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 0.9542, Train Acc: 0.4989, Loss: 0.6532, Test Acc: 0.5882\n",
      "Epoch: 051, Loss: 1.0720, Train Acc: 0.4636, Loss: 1.7365, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 1.1537, Train Acc: 0.5051, Loss: 0.6501, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 1.1726, Train Acc: 0.4449, Loss: 1.6103, Test Acc: 0.5294\n",
      "Epoch: 054, Loss: 1.4138, Train Acc: 0.5426, Loss: 2.0204, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 1.9795, Train Acc: 0.5114, Loss: 1.1081, Test Acc: 0.4706\n",
      "Epoch: 056, Loss: 1.9324, Train Acc: 0.4699, Loss: 1.4121, Test Acc: 0.4706\n",
      "Epoch: 057, Loss: 1.8002, Train Acc: 0.5011, Loss: 1.8650, Test Acc: 0.5294\n",
      "Epoch: 058, Loss: 1.7428, Train Acc: 0.5364, Loss: 2.9225, Test Acc: 0.5294\n",
      "Epoch: 059, Loss: 1.8872, Train Acc: 0.5239, Loss: 3.3379, Test Acc: 0.5294\n",
      "Epoch: 060, Loss: 2.2367, Train Acc: 0.5364, Loss: 3.2641, Test Acc: 0.5294\n",
      "Epoch: 061, Loss: 2.3839, Train Acc: 0.5599, Loss: 3.5680, Test Acc: 0.5294\n",
      "Epoch: 062, Loss: 3.0645, Train Acc: 0.4824, Loss: 0.9359, Test Acc: 0.5294\n",
      "Epoch: 063, Loss: 2.2887, Train Acc: 0.4886, Loss: 0.9193, Test Acc: 0.5294\n",
      "Epoch: 064, Loss: 2.0653, Train Acc: 0.4886, Loss: 1.4209, Test Acc: 0.5294\n",
      "Epoch: 065, Loss: 1.9567, Train Acc: 0.4824, Loss: 2.1847, Test Acc: 0.5294\n",
      "Epoch: 066, Loss: 2.8192, Train Acc: 0.4886, Loss: 0.7563, Test Acc: 0.5294\n",
      "Epoch: 067, Loss: 2.9344, Train Acc: 0.5011, Loss: 3.1931, Test Acc: 0.4706\n",
      "Epoch: 068, Loss: 3.0881, Train Acc: 0.4989, Loss: 1.6528, Test Acc: 0.4706\n",
      "Epoch: 069, Loss: 1.9718, Train Acc: 0.5176, Loss: 0.8299, Test Acc: 0.4706\n",
      "Epoch: 070, Loss: 1.1844, Train Acc: 0.4684, Loss: 0.6746, Test Acc: 0.6471\n",
      "Epoch: 071, Loss: 0.8406, Train Acc: 0.5364, Loss: 1.2698, Test Acc: 0.5294\n",
      "Epoch: 072, Loss: 0.9417, Train Acc: 0.4518, Loss: 0.8809, Test Acc: 0.5294\n",
      "Epoch: 073, Loss: 0.7895, Train Acc: 0.5419, Loss: 0.7180, Test Acc: 0.5294\n",
      "Epoch: 074, Loss: 0.8832, Train Acc: 0.4754, Loss: 1.0378, Test Acc: 0.5294\n",
      "Epoch: 075, Loss: 0.8268, Train Acc: 0.5037, Loss: 0.7681, Test Acc: 0.5294\n",
      "Epoch: 076, Loss: 0.8557, Train Acc: 0.4879, Loss: 1.0582, Test Acc: 0.5294\n",
      "Epoch: 077, Loss: 0.8514, Train Acc: 0.4809, Loss: 0.8937, Test Acc: 0.5294\n",
      "Epoch: 078, Loss: 0.8399, Train Acc: 0.5114, Loss: 0.9319, Test Acc: 0.5294\n",
      "Epoch: 079, Loss: 0.8585, Train Acc: 0.4871, Loss: 0.9478, Test Acc: 0.5294\n",
      "Epoch: 080, Loss: 0.8400, Train Acc: 0.5107, Loss: 0.8823, Test Acc: 0.5294\n",
      "Epoch: 081, Loss: 0.8533, Train Acc: 0.4871, Loss: 0.9484, Test Acc: 0.5294\n",
      "Epoch: 082, Loss: 0.8424, Train Acc: 0.4926, Loss: 0.8873, Test Acc: 0.5294\n",
      "Epoch: 083, Loss: 0.8481, Train Acc: 0.4871, Loss: 0.9359, Test Acc: 0.5294\n",
      "Epoch: 084, Loss: 0.8457, Train Acc: 0.4809, Loss: 0.8955, Test Acc: 0.5294\n",
      "Epoch: 085, Loss: 0.8454, Train Acc: 0.4989, Loss: 0.9200, Test Acc: 0.5294\n",
      "Epoch: 086, Loss: 0.8473, Train Acc: 0.4989, Loss: 0.9065, Test Acc: 0.5294\n",
      "Epoch: 087, Loss: 0.8474, Train Acc: 0.4989, Loss: 0.9147, Test Acc: 0.5294\n",
      "Epoch: 088, Loss: 0.8510, Train Acc: 0.4871, Loss: 0.9230, Test Acc: 0.5294\n",
      "Epoch: 089, Loss: 0.8499, Train Acc: 0.4989, Loss: 0.9187, Test Acc: 0.5294\n",
      "Epoch: 090, Loss: 0.8524, Train Acc: 0.4989, Loss: 0.9263, Test Acc: 0.5294\n",
      "Epoch: 091, Loss: 0.8511, Train Acc: 0.4989, Loss: 0.9165, Test Acc: 0.5294\n",
      "Epoch: 092, Loss: 0.8529, Train Acc: 0.5051, Loss: 0.9235, Test Acc: 0.5294\n",
      "Epoch: 093, Loss: 0.8533, Train Acc: 0.5169, Loss: 0.9222, Test Acc: 0.5294\n",
      "Epoch: 094, Loss: 0.8534, Train Acc: 0.5169, Loss: 0.9170, Test Acc: 0.5294\n",
      "Epoch: 095, Loss: 0.8539, Train Acc: 0.5107, Loss: 0.9189, Test Acc: 0.5294\n",
      "Epoch: 096, Loss: 0.8553, Train Acc: 0.5107, Loss: 0.9198, Test Acc: 0.5294\n",
      "Epoch: 097, Loss: 0.8558, Train Acc: 0.5107, Loss: 0.9194, Test Acc: 0.5294\n",
      "Epoch: 098, Loss: 0.8576, Train Acc: 0.5107, Loss: 0.9228, Test Acc: 0.5294\n",
      "Epoch: 099, Loss: 0.8589, Train Acc: 0.5107, Loss: 0.9252, Test Acc: 0.5294\n",
      "Epoch: 100, Loss: 0.8605, Train Acc: 0.5232, Loss: 0.9341, Test Acc: 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN accuracy: 0.5263158082962036\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  20  32\n",
      "  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52\n",
      "  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70\n",
      "  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88\n",
      "  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
      " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
      " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
      " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n",
      " 161] TEST: [15 17 18 19 21 22 23 24 25 26 27 28 29 30 31 33 34]\n",
      "145\n",
      "17\n",
      "Epoch: 000, Loss: 76.6876, Train Acc: 0.4989, Loss: 146.3796, Test Acc: 0.4706\n",
      "Epoch: 001, Loss: 97.8553, Train Acc: 0.4886, Loss: 13.0397, Test Acc: 0.4706\n",
      "Epoch: 002, Loss: 77.1005, Train Acc: 0.5426, Loss: 12.3024, Test Acc: 0.4706\n",
      "Epoch: 003, Loss: 23.5449, Train Acc: 0.5114, Loss: 12.9283, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 25.9851, Train Acc: 0.4989, Loss: 38.4853, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 35.5408, Train Acc: 0.4824, Loss: 47.5757, Test Acc: 0.4706\n",
      "Epoch: 006, Loss: 35.2696, Train Acc: 0.4989, Loss: 27.7586, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 14.1096, Train Acc: 0.5114, Loss: 25.1564, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 11.4772, Train Acc: 0.5074, Loss: 15.9800, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 13.1049, Train Acc: 0.4886, Loss: 3.4945, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 2.4015, Train Acc: 0.5482, Loss: 1.3308, Test Acc: 0.4706\n",
      "Epoch: 011, Loss: 5.5986, Train Acc: 0.5011, Loss: 10.2539, Test Acc: 0.4706\n",
      "Epoch: 012, Loss: 4.7074, Train Acc: 0.5301, Loss: 6.5181, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 5.7212, Train Acc: 0.5426, Loss: 4.6136, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 3.8898, Train Acc: 0.4879, Loss: 1.4876, Test Acc: 0.5882\n",
      "Epoch: 015, Loss: 2.4725, Train Acc: 0.4574, Loss: 11.7292, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 13.4499, Train Acc: 0.4886, Loss: 15.0863, Test Acc: 0.4706\n",
      "Epoch: 017, Loss: 7.6070, Train Acc: 0.5011, Loss: 9.7351, Test Acc: 0.4118\n",
      "Epoch: 018, Loss: 6.0823, Train Acc: 0.5357, Loss: 5.7169, Test Acc: 0.4118\n",
      "Epoch: 019, Loss: 5.2558, Train Acc: 0.4761, Loss: 2.8319, Test Acc: 0.4118\n",
      "Epoch: 020, Loss: 5.9215, Train Acc: 0.5136, Loss: 4.1654, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 4.2485, Train Acc: 0.5364, Loss: 11.5403, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 11.4332, Train Acc: 0.5011, Loss: 4.1304, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 6.0926, Train Acc: 0.4886, Loss: 2.7253, Test Acc: 0.4706\n",
      "Epoch: 024, Loss: 5.1241, Train Acc: 0.5426, Loss: 1.9965, Test Acc: 0.4118\n",
      "Epoch: 025, Loss: 5.7824, Train Acc: 0.4761, Loss: 6.8844, Test Acc: 0.4706\n",
      "Epoch: 026, Loss: 6.8250, Train Acc: 0.5246, Loss: 12.4848, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 8.5274, Train Acc: 0.5364, Loss: 7.0138, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 7.8910, Train Acc: 0.5114, Loss: 1.5142, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 11.7267, Train Acc: 0.4989, Loss: 14.7614, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 6.5920, Train Acc: 0.5482, Loss: 4.2951, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 3.0987, Train Acc: 0.5114, Loss: 6.8908, Test Acc: 0.4706\n",
      "Epoch: 032, Loss: 4.7228, Train Acc: 0.4643, Loss: 4.5898, Test Acc: 0.2941\n",
      "Epoch: 033, Loss: 2.2641, Train Acc: 0.5559, Loss: 5.7149, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 4.0472, Train Acc: 0.5301, Loss: 3.8024, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 2.7851, Train Acc: 0.5239, Loss: 5.0918, Test Acc: 0.4706\n",
      "Epoch: 036, Loss: 3.7751, Train Acc: 0.4699, Loss: 3.3702, Test Acc: 0.4118\n",
      "Epoch: 037, Loss: 3.7134, Train Acc: 0.5191, Loss: 5.6365, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 4.0361, Train Acc: 0.5426, Loss: 6.3595, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 4.6668, Train Acc: 0.5176, Loss: 2.8104, Test Acc: 0.4706\n",
      "Epoch: 040, Loss: 4.4714, Train Acc: 0.4761, Loss: 1.5930, Test Acc: 0.4118\n",
      "Epoch: 041, Loss: 4.8981, Train Acc: 0.5136, Loss: 3.1657, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 3.1363, Train Acc: 0.5489, Loss: 7.5395, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 4.4765, Train Acc: 0.4864, Loss: 8.5455, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 6.6691, Train Acc: 0.5011, Loss: 8.1053, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 13.2106, Train Acc: 0.4699, Loss: 15.7352, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 18.1683, Train Acc: 0.4886, Loss: 6.2426, Test Acc: 0.4706\n",
      "Epoch: 047, Loss: 6.8573, Train Acc: 0.5426, Loss: 14.8168, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 7.0826, Train Acc: 0.4824, Loss: 11.8541, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 11.0680, Train Acc: 0.5364, Loss: 7.4765, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 6.1134, Train Acc: 0.4864, Loss: 0.8485, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 4.6121, Train Acc: 0.5801, Loss: 12.1253, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 9.1926, Train Acc: 0.5114, Loss: 0.7341, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 8.7630, Train Acc: 0.5551, Loss: 9.0486, Test Acc: 0.5294\n",
      "Epoch: 054, Loss: 10.7685, Train Acc: 0.4989, Loss: 17.6437, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 9.4525, Train Acc: 0.5011, Loss: 6.4625, Test Acc: 0.5294\n",
      "Epoch: 056, Loss: 8.3913, Train Acc: 0.4886, Loss: 6.7511, Test Acc: 0.4706\n",
      "Epoch: 057, Loss: 6.4789, Train Acc: 0.5074, Loss: 5.4536, Test Acc: 0.4118\n",
      "Epoch: 058, Loss: 3.6217, Train Acc: 0.4816, Loss: 3.0700, Test Acc: 0.4118\n",
      "Epoch: 059, Loss: 1.6917, Train Acc: 0.5426, Loss: 3.3130, Test Acc: 0.5294\n",
      "Epoch: 060, Loss: 1.6998, Train Acc: 0.5294, Loss: 2.0645, Test Acc: 0.3529\n",
      "Epoch: 061, Loss: 0.9995, Train Acc: 0.6044, Loss: 1.5940, Test Acc: 0.5294\n",
      "Epoch: 062, Loss: 1.5644, Train Acc: 0.4761, Loss: 1.4843, Test Acc: 0.5294\n",
      "Epoch: 063, Loss: 1.0402, Train Acc: 0.5066, Loss: 1.0879, Test Acc: 0.2941\n",
      "Epoch: 064, Loss: 0.8588, Train Acc: 0.5676, Loss: 1.8382, Test Acc: 0.5294\n",
      "Epoch: 065, Loss: 1.2204, Train Acc: 0.5114, Loss: 1.1438, Test Acc: 0.5882\n",
      "Epoch: 066, Loss: 1.5177, Train Acc: 0.4949, Loss: 2.4874, Test Acc: 0.5294\n",
      "Epoch: 067, Loss: 1.5747, Train Acc: 0.5301, Loss: 1.2045, Test Acc: 0.5294\n",
      "Epoch: 068, Loss: 2.9512, Train Acc: 0.5114, Loss: 1.6137, Test Acc: 0.5294\n",
      "Epoch: 069, Loss: 4.7204, Train Acc: 0.4989, Loss: 9.5900, Test Acc: 0.5294\n",
      "Epoch: 070, Loss: 7.8739, Train Acc: 0.4886, Loss: 9.5643, Test Acc: 0.4706\n",
      "Epoch: 071, Loss: 8.0290, Train Acc: 0.5051, Loss: 7.4776, Test Acc: 0.5294\n",
      "Epoch: 072, Loss: 4.7129, Train Acc: 0.5059, Loss: 4.6981, Test Acc: 0.5294\n",
      "Epoch: 073, Loss: 3.6578, Train Acc: 0.5371, Loss: 2.9863, Test Acc: 0.5882\n",
      "Epoch: 074, Loss: 2.7858, Train Acc: 0.5239, Loss: 1.2310, Test Acc: 0.6471\n",
      "Epoch: 075, Loss: 2.8050, Train Acc: 0.4926, Loss: 1.3320, Test Acc: 0.4118\n",
      "Epoch: 076, Loss: 1.8547, Train Acc: 0.4761, Loss: 0.5693, Test Acc: 0.7059\n",
      "Epoch: 077, Loss: 1.1648, Train Acc: 0.5239, Loss: 1.1522, Test Acc: 0.5882\n",
      "Epoch: 078, Loss: 1.9692, Train Acc: 0.4989, Loss: 0.9403, Test Acc: 0.4706\n",
      "Epoch: 079, Loss: 1.7784, Train Acc: 0.4699, Loss: 0.9724, Test Acc: 0.4706\n",
      "Epoch: 080, Loss: 1.8042, Train Acc: 0.4949, Loss: 1.0157, Test Acc: 0.5882\n",
      "Epoch: 081, Loss: 1.1198, Train Acc: 0.5051, Loss: 1.0477, Test Acc: 0.5294\n",
      "Epoch: 082, Loss: 1.2363, Train Acc: 0.4816, Loss: 1.2142, Test Acc: 0.4706\n",
      "Epoch: 083, Loss: 1.0993, Train Acc: 0.4934, Loss: 1.3641, Test Acc: 0.5294\n",
      "Epoch: 084, Loss: 0.9559, Train Acc: 0.5066, Loss: 0.9795, Test Acc: 0.5294\n",
      "Epoch: 085, Loss: 0.8302, Train Acc: 0.5434, Loss: 0.7247, Test Acc: 0.5882\n",
      "Epoch: 086, Loss: 0.8818, Train Acc: 0.5246, Loss: 1.0010, Test Acc: 0.5294\n",
      "Epoch: 087, Loss: 0.8425, Train Acc: 0.5136, Loss: 0.7792, Test Acc: 0.5882\n",
      "Epoch: 088, Loss: 0.7772, Train Acc: 0.5504, Loss: 0.7755, Test Acc: 0.5882\n",
      "Epoch: 089, Loss: 0.8536, Train Acc: 0.5121, Loss: 1.0308, Test Acc: 0.5294\n",
      "Epoch: 090, Loss: 0.8577, Train Acc: 0.5184, Loss: 0.8740, Test Acc: 0.5294\n",
      "Epoch: 091, Loss: 0.8010, Train Acc: 0.5559, Loss: 0.7502, Test Acc: 0.5294\n",
      "Epoch: 092, Loss: 0.8575, Train Acc: 0.4996, Loss: 1.0193, Test Acc: 0.5294\n",
      "Epoch: 093, Loss: 0.8636, Train Acc: 0.5371, Loss: 0.9659, Test Acc: 0.5294\n",
      "Epoch: 094, Loss: 0.8354, Train Acc: 0.5559, Loss: 0.8500, Test Acc: 0.5294\n",
      "Epoch: 095, Loss: 0.8539, Train Acc: 0.5121, Loss: 0.9668, Test Acc: 0.5294\n",
      "Epoch: 096, Loss: 0.8552, Train Acc: 0.5309, Loss: 0.9687, Test Acc: 0.5294\n",
      "Epoch: 097, Loss: 0.8333, Train Acc: 0.5559, Loss: 0.9025, Test Acc: 0.5294\n",
      "Epoch: 098, Loss: 0.8409, Train Acc: 0.5184, Loss: 0.9503, Test Acc: 0.5294\n",
      "Epoch: 099, Loss: 0.8477, Train Acc: 0.5309, Loss: 0.9762, Test Acc: 0.5294\n",
      "Epoch: 100, Loss: 0.8378, Train Acc: 0.5379, Loss: 0.9408, Test Acc: 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN accuracy: 0.5789473652839661\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  33  34  50  51\n",
      "  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69\n",
      "  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87\n",
      "  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105\n",
      " 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123\n",
      " 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      " 160 161] TEST: [32 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49]\n",
      "146\n",
      "16\n",
      "Epoch: 000, Loss: 61.7235, Train Acc: 0.5069, Loss: 193.2167, Test Acc: 0.5000\n",
      "Epoch: 001, Loss: 110.4562, Train Acc: 0.5361, Loss: 16.5311, Test Acc: 0.5625\n",
      "Epoch: 002, Loss: 16.1089, Train Acc: 0.5111, Loss: 8.4612, Test Acc: 0.5625\n",
      "Epoch: 003, Loss: 6.3738, Train Acc: 0.5660, Loss: 7.7436, Test Acc: 0.5625\n",
      "Epoch: 004, Loss: 5.4293, Train Acc: 0.5125, Loss: 14.1735, Test Acc: 0.5000\n",
      "Epoch: 005, Loss: 11.3035, Train Acc: 0.5299, Loss: 38.6685, Test Acc: 0.5000\n",
      "Epoch: 006, Loss: 16.3768, Train Acc: 0.5472, Loss: 2.5242, Test Acc: 0.5000\n",
      "Epoch: 007, Loss: 11.0752, Train Acc: 0.5361, Loss: 9.9150, Test Acc: 0.5000\n",
      "Epoch: 008, Loss: 15.1483, Train Acc: 0.5111, Loss: 34.3960, Test Acc: 0.5000\n",
      "Epoch: 009, Loss: 22.6411, Train Acc: 0.4889, Loss: 12.0024, Test Acc: 0.5000\n",
      "Epoch: 010, Loss: 9.8964, Train Acc: 0.4778, Loss: 7.5607, Test Acc: 0.5000\n",
      "Epoch: 011, Loss: 4.4021, Train Acc: 0.5097, Loss: 2.1160, Test Acc: 0.5000\n",
      "Epoch: 012, Loss: 3.9519, Train Acc: 0.4764, Loss: 12.6225, Test Acc: 0.5000\n",
      "Epoch: 013, Loss: 10.2476, Train Acc: 0.4889, Loss: 2.4554, Test Acc: 0.5000\n",
      "Epoch: 014, Loss: 4.1977, Train Acc: 0.5361, Loss: 9.9070, Test Acc: 0.5000\n",
      "Epoch: 015, Loss: 8.1698, Train Acc: 0.5139, Loss: 10.0867, Test Acc: 0.5000\n",
      "Epoch: 016, Loss: 11.3063, Train Acc: 0.5139, Loss: 17.5436, Test Acc: 0.5000\n",
      "Epoch: 017, Loss: 13.9678, Train Acc: 0.5049, Loss: 23.0469, Test Acc: 0.5000\n",
      "Epoch: 018, Loss: 15.3942, Train Acc: 0.4889, Loss: 15.9241, Test Acc: 0.5000\n",
      "Epoch: 019, Loss: 12.4546, Train Acc: 0.4986, Loss: 9.6968, Test Acc: 0.5000\n",
      "Epoch: 020, Loss: 7.1446, Train Acc: 0.4639, Loss: 5.7403, Test Acc: 0.5000\n",
      "Epoch: 021, Loss: 4.3281, Train Acc: 0.4639, Loss: 2.7704, Test Acc: 0.5000\n",
      "Epoch: 022, Loss: 2.5153, Train Acc: 0.5049, Loss: 1.4963, Test Acc: 0.4375\n",
      "Epoch: 023, Loss: 1.5120, Train Acc: 0.5125, Loss: 3.7498, Test Acc: 0.5000\n",
      "Epoch: 024, Loss: 1.7891, Train Acc: 0.5500, Loss: 1.5654, Test Acc: 0.5000\n",
      "Epoch: 025, Loss: 2.4029, Train Acc: 0.4861, Loss: 1.1359, Test Acc: 0.3750\n",
      "Epoch: 026, Loss: 4.3725, Train Acc: 0.5361, Loss: 3.1496, Test Acc: 0.5000\n",
      "Epoch: 027, Loss: 6.6944, Train Acc: 0.5111, Loss: 11.9766, Test Acc: 0.5000\n",
      "Epoch: 028, Loss: 10.0911, Train Acc: 0.4889, Loss: 9.0944, Test Acc: 0.5000\n",
      "Epoch: 029, Loss: 5.8571, Train Acc: 0.4889, Loss: 4.1668, Test Acc: 0.5000\n",
      "Epoch: 030, Loss: 3.1154, Train Acc: 0.4639, Loss: 5.6275, Test Acc: 0.5000\n",
      "Epoch: 031, Loss: 6.5376, Train Acc: 0.4889, Loss: 4.8162, Test Acc: 0.5000\n",
      "Epoch: 032, Loss: 4.8720, Train Acc: 0.5111, Loss: 3.2775, Test Acc: 0.5000\n",
      "Epoch: 033, Loss: 2.9339, Train Acc: 0.4812, Loss: 2.4356, Test Acc: 0.5000\n",
      "Epoch: 034, Loss: 1.6050, Train Acc: 0.5000, Loss: 1.7258, Test Acc: 0.5000\n",
      "Epoch: 035, Loss: 0.8849, Train Acc: 0.5375, Loss: 0.7582, Test Acc: 0.4375\n",
      "Epoch: 036, Loss: 0.9026, Train Acc: 0.5361, Loss: 0.7720, Test Acc: 0.3750\n",
      "Epoch: 037, Loss: 1.0655, Train Acc: 0.5097, Loss: 1.2527, Test Acc: 0.5000\n",
      "Epoch: 038, Loss: 0.9955, Train Acc: 0.5174, Loss: 0.7340, Test Acc: 0.5625\n",
      "Epoch: 039, Loss: 1.0250, Train Acc: 0.4924, Loss: 1.0637, Test Acc: 0.5000\n",
      "Epoch: 040, Loss: 1.0112, Train Acc: 0.4986, Loss: 0.7717, Test Acc: 0.3750\n",
      "Epoch: 041, Loss: 1.0251, Train Acc: 0.4736, Loss: 0.9350, Test Acc: 0.5000\n",
      "Epoch: 042, Loss: 1.0114, Train Acc: 0.4924, Loss: 0.7652, Test Acc: 0.5000\n",
      "Epoch: 043, Loss: 0.9817, Train Acc: 0.5174, Loss: 0.9107, Test Acc: 0.5000\n",
      "Epoch: 044, Loss: 0.9796, Train Acc: 0.5188, Loss: 0.9492, Test Acc: 0.5000\n",
      "Epoch: 045, Loss: 0.9853, Train Acc: 0.4924, Loss: 0.8332, Test Acc: 0.5000\n",
      "Epoch: 046, Loss: 0.9992, Train Acc: 0.4938, Loss: 0.8218, Test Acc: 0.4375\n",
      "Epoch: 047, Loss: 1.0081, Train Acc: 0.4799, Loss: 0.8614, Test Acc: 0.3750\n",
      "Epoch: 048, Loss: 1.0084, Train Acc: 0.4799, Loss: 0.7779, Test Acc: 0.3750\n",
      "Epoch: 049, Loss: 0.9872, Train Acc: 0.5111, Loss: 0.9159, Test Acc: 0.5000\n",
      "Epoch: 050, Loss: 0.9994, Train Acc: 0.5174, Loss: 0.7573, Test Acc: 0.5000\n",
      "Epoch: 051, Loss: 0.8808, Train Acc: 0.5611, Loss: 0.7777, Test Acc: 0.4375\n",
      "Epoch: 052, Loss: 1.0458, Train Acc: 0.4875, Loss: 1.4260, Test Acc: 0.5000\n",
      "Epoch: 053, Loss: 0.8456, Train Acc: 0.5549, Loss: 0.8734, Test Acc: 0.5625\n",
      "Epoch: 054, Loss: 1.0957, Train Acc: 0.5111, Loss: 1.2167, Test Acc: 0.5000\n",
      "Epoch: 055, Loss: 2.0128, Train Acc: 0.5361, Loss: 0.9150, Test Acc: 0.5000\n",
      "Epoch: 056, Loss: 1.5411, Train Acc: 0.4639, Loss: 0.7600, Test Acc: 0.5000\n",
      "Epoch: 057, Loss: 2.4932, Train Acc: 0.4764, Loss: 1.2755, Test Acc: 0.5000\n",
      "Epoch: 058, Loss: 1.5895, Train Acc: 0.5076, Loss: 3.9335, Test Acc: 0.5000\n",
      "Epoch: 059, Loss: 4.4275, Train Acc: 0.4889, Loss: 3.2392, Test Acc: 0.5000\n",
      "Epoch: 060, Loss: 4.1554, Train Acc: 0.5111, Loss: 3.5318, Test Acc: 0.5000\n",
      "Epoch: 061, Loss: 2.4861, Train Acc: 0.4750, Loss: 0.9791, Test Acc: 0.5625\n",
      "Epoch: 062, Loss: 2.2526, Train Acc: 0.4639, Loss: 3.7308, Test Acc: 0.5000\n",
      "Epoch: 063, Loss: 4.2043, Train Acc: 0.4889, Loss: 2.2404, Test Acc: 0.5000\n",
      "Epoch: 064, Loss: 4.0493, Train Acc: 0.5000, Loss: 4.9583, Test Acc: 0.5000\n",
      "Epoch: 065, Loss: 4.5305, Train Acc: 0.5111, Loss: 4.3877, Test Acc: 0.5000\n",
      "Epoch: 066, Loss: 3.1613, Train Acc: 0.5264, Loss: 4.5098, Test Acc: 0.5000\n",
      "Epoch: 067, Loss: 4.8017, Train Acc: 0.5139, Loss: 6.6038, Test Acc: 0.5000\n",
      "Epoch: 068, Loss: 4.9215, Train Acc: 0.5111, Loss: 5.6464, Test Acc: 0.5000\n",
      "Epoch: 069, Loss: 4.8473, Train Acc: 0.5139, Loss: 1.0961, Test Acc: 0.6250\n",
      "Epoch: 070, Loss: 4.8560, Train Acc: 0.5111, Loss: 1.9416, Test Acc: 0.5625\n",
      "Epoch: 071, Loss: 2.7208, Train Acc: 0.4861, Loss: 2.4117, Test Acc: 0.5000\n",
      "Epoch: 072, Loss: 1.7706, Train Acc: 0.5236, Loss: 2.0233, Test Acc: 0.5000\n",
      "Epoch: 073, Loss: 1.4816, Train Acc: 0.5361, Loss: 1.3651, Test Acc: 0.5000\n",
      "Epoch: 074, Loss: 1.5811, Train Acc: 0.4639, Loss: 1.2957, Test Acc: 0.5000\n",
      "Epoch: 075, Loss: 1.7528, Train Acc: 0.4826, Loss: 0.7492, Test Acc: 0.4375\n",
      "Epoch: 076, Loss: 1.6474, Train Acc: 0.4889, Loss: 1.4034, Test Acc: 0.5000\n",
      "Epoch: 077, Loss: 1.4911, Train Acc: 0.4889, Loss: 2.0313, Test Acc: 0.5000\n",
      "Epoch: 078, Loss: 2.3548, Train Acc: 0.4889, Loss: 0.8781, Test Acc: 0.5000\n",
      "Epoch: 079, Loss: 2.5654, Train Acc: 0.4889, Loss: 2.2562, Test Acc: 0.5000\n",
      "Epoch: 080, Loss: 1.8212, Train Acc: 0.4861, Loss: 1.8472, Test Acc: 0.5000\n",
      "Epoch: 081, Loss: 1.4493, Train Acc: 0.4750, Loss: 1.0757, Test Acc: 0.5000\n",
      "Epoch: 082, Loss: 0.9801, Train Acc: 0.4799, Loss: 0.9881, Test Acc: 0.5000\n",
      "Epoch: 083, Loss: 0.8622, Train Acc: 0.5361, Loss: 1.0536, Test Acc: 0.5000\n",
      "Epoch: 084, Loss: 0.8418, Train Acc: 0.5361, Loss: 0.8035, Test Acc: 0.3750\n",
      "Epoch: 085, Loss: 0.7991, Train Acc: 0.4875, Loss: 1.0265, Test Acc: 0.5000\n",
      "Epoch: 086, Loss: 0.7674, Train Acc: 0.5674, Loss: 0.7790, Test Acc: 0.4375\n",
      "Epoch: 087, Loss: 0.7293, Train Acc: 0.5312, Loss: 0.7800, Test Acc: 0.6250\n",
      "Epoch: 088, Loss: 0.7351, Train Acc: 0.5736, Loss: 0.8774, Test Acc: 0.5000\n",
      "Epoch: 089, Loss: 0.7048, Train Acc: 0.5611, Loss: 0.7558, Test Acc: 0.4375\n",
      "Epoch: 090, Loss: 0.7059, Train Acc: 0.5674, Loss: 0.8789, Test Acc: 0.5000\n",
      "Epoch: 091, Loss: 0.6991, Train Acc: 0.5486, Loss: 0.7595, Test Acc: 0.5625\n",
      "Epoch: 092, Loss: 0.7055, Train Acc: 0.5736, Loss: 0.8290, Test Acc: 0.5000\n",
      "Epoch: 093, Loss: 0.6935, Train Acc: 0.5861, Loss: 0.7733, Test Acc: 0.5000\n",
      "Epoch: 094, Loss: 0.6975, Train Acc: 0.5924, Loss: 0.8251, Test Acc: 0.5000\n",
      "Epoch: 095, Loss: 0.6909, Train Acc: 0.5625, Loss: 0.7835, Test Acc: 0.4375\n",
      "Epoch: 096, Loss: 0.6951, Train Acc: 0.5424, Loss: 0.8166, Test Acc: 0.5000\n",
      "Epoch: 097, Loss: 0.6903, Train Acc: 0.5486, Loss: 0.7893, Test Acc: 0.4375\n",
      "Epoch: 098, Loss: 0.6937, Train Acc: 0.5486, Loss: 0.8099, Test Acc: 0.5000\n",
      "Epoch: 099, Loss: 0.6931, Train Acc: 0.5674, Loss: 0.7976, Test Acc: 0.5000\n",
      "Epoch: 100, Loss: 0.6942, Train Acc: 0.5424, Loss: 0.8081, Test Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN accuracy: 0.5263158082962036\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  65  67  68  69\n",
      "  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87\n",
      "  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105\n",
      " 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123\n",
      " 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      " 160 161] TEST: [50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 66]\n",
      "146\n",
      "16\n",
      "Epoch: 000, Loss: 55.5664, Train Acc: 0.5674, Loss: 221.6406, Test Acc: 0.5000\n",
      "Epoch: 001, Loss: 94.2470, Train Acc: 0.5139, Loss: 59.9668, Test Acc: 0.5000\n",
      "Epoch: 002, Loss: 39.7393, Train Acc: 0.4889, Loss: 76.6473, Test Acc: 0.5000\n",
      "Epoch: 003, Loss: 59.1146, Train Acc: 0.4889, Loss: 32.5854, Test Acc: 0.5000\n",
      "Epoch: 004, Loss: 22.1086, Train Acc: 0.4889, Loss: 27.9373, Test Acc: 0.5000\n",
      "Epoch: 005, Loss: 23.9648, Train Acc: 0.4924, Loss: 4.7668, Test Acc: 0.3750\n",
      "Epoch: 006, Loss: 9.7784, Train Acc: 0.5674, Loss: 6.2600, Test Acc: 0.4375\n",
      "Epoch: 007, Loss: 8.4152, Train Acc: 0.5736, Loss: 5.2124, Test Acc: 0.4375\n",
      "Epoch: 008, Loss: 4.5387, Train Acc: 0.5035, Loss: 6.6727, Test Acc: 0.4375\n",
      "Epoch: 009, Loss: 3.0240, Train Acc: 0.5736, Loss: 4.0403, Test Acc: 0.5000\n",
      "Epoch: 010, Loss: 4.8004, Train Acc: 0.4951, Loss: 2.6567, Test Acc: 0.3750\n",
      "Epoch: 011, Loss: 1.4426, Train Acc: 0.5236, Loss: 6.2172, Test Acc: 0.5000\n",
      "Epoch: 012, Loss: 5.0161, Train Acc: 0.4924, Loss: 5.3037, Test Acc: 0.5000\n",
      "Epoch: 013, Loss: 7.5501, Train Acc: 0.5361, Loss: 5.3348, Test Acc: 0.5000\n",
      "Epoch: 014, Loss: 7.4410, Train Acc: 0.4764, Loss: 13.3848, Test Acc: 0.5000\n",
      "Epoch: 015, Loss: 6.1139, Train Acc: 0.4861, Loss: 9.5758, Test Acc: 0.5000\n",
      "Epoch: 016, Loss: 7.0440, Train Acc: 0.5361, Loss: 8.2904, Test Acc: 0.5000\n",
      "Epoch: 017, Loss: 3.2061, Train Acc: 0.4799, Loss: 4.8958, Test Acc: 0.5625\n",
      "Epoch: 018, Loss: 3.8437, Train Acc: 0.4972, Loss: 11.4453, Test Acc: 0.5000\n",
      "Epoch: 019, Loss: 6.3847, Train Acc: 0.5424, Loss: 5.2702, Test Acc: 0.5000\n",
      "Epoch: 020, Loss: 6.1383, Train Acc: 0.5361, Loss: 2.2633, Test Acc: 0.6250\n",
      "Epoch: 021, Loss: 5.0519, Train Acc: 0.4951, Loss: 7.6671, Test Acc: 0.5000\n",
      "Epoch: 022, Loss: 5.5497, Train Acc: 0.4972, Loss: 7.6634, Test Acc: 0.5000\n",
      "Epoch: 023, Loss: 5.8256, Train Acc: 0.5299, Loss: 5.5259, Test Acc: 0.5000\n",
      "Epoch: 024, Loss: 3.1985, Train Acc: 0.5597, Loss: 6.7749, Test Acc: 0.5000\n",
      "Epoch: 025, Loss: 7.5476, Train Acc: 0.4924, Loss: 12.1040, Test Acc: 0.5000\n",
      "Epoch: 026, Loss: 11.4076, Train Acc: 0.5111, Loss: 9.5428, Test Acc: 0.5000\n",
      "Epoch: 027, Loss: 4.2629, Train Acc: 0.5174, Loss: 3.6316, Test Acc: 0.5625\n",
      "Epoch: 028, Loss: 5.0130, Train Acc: 0.4826, Loss: 7.7789, Test Acc: 0.5000\n",
      "Epoch: 029, Loss: 4.7679, Train Acc: 0.4639, Loss: 3.5857, Test Acc: 0.5000\n",
      "Epoch: 030, Loss: 1.8694, Train Acc: 0.5438, Loss: 3.6575, Test Acc: 0.5000\n",
      "Epoch: 031, Loss: 2.4376, Train Acc: 0.5049, Loss: 2.1725, Test Acc: 0.3750\n",
      "Epoch: 032, Loss: 2.1199, Train Acc: 0.5472, Loss: 1.6589, Test Acc: 0.4375\n",
      "Epoch: 033, Loss: 1.4396, Train Acc: 0.5472, Loss: 2.1275, Test Acc: 0.5000\n",
      "Epoch: 034, Loss: 3.4213, Train Acc: 0.5361, Loss: 10.3187, Test Acc: 0.5000\n",
      "Epoch: 035, Loss: 9.8177, Train Acc: 0.4861, Loss: 7.0555, Test Acc: 0.5000\n",
      "Epoch: 036, Loss: 5.4176, Train Acc: 0.5035, Loss: 2.6831, Test Acc: 0.4375\n",
      "Epoch: 037, Loss: 5.3290, Train Acc: 0.5201, Loss: 5.6749, Test Acc: 0.5000\n",
      "Epoch: 038, Loss: 8.8067, Train Acc: 0.4701, Loss: 11.7637, Test Acc: 0.5000\n",
      "Epoch: 039, Loss: 11.3996, Train Acc: 0.4924, Loss: 2.2645, Test Acc: 0.3750\n",
      "Epoch: 040, Loss: 5.5553, Train Acc: 0.5361, Loss: 4.6721, Test Acc: 0.4375\n",
      "Epoch: 041, Loss: 4.0695, Train Acc: 0.5486, Loss: 4.4944, Test Acc: 0.5000\n",
      "Epoch: 042, Loss: 4.1516, Train Acc: 0.4701, Loss: 3.2141, Test Acc: 0.4375\n",
      "Epoch: 043, Loss: 3.0511, Train Acc: 0.5736, Loss: 5.0912, Test Acc: 0.5000\n",
      "Epoch: 044, Loss: 4.0968, Train Acc: 0.5236, Loss: 2.5050, Test Acc: 0.3750\n",
      "Epoch: 045, Loss: 2.1663, Train Acc: 0.5111, Loss: 3.2626, Test Acc: 0.5000\n",
      "Epoch: 046, Loss: 2.1397, Train Acc: 0.5174, Loss: 1.1103, Test Acc: 0.4375\n",
      "Epoch: 047, Loss: 2.2767, Train Acc: 0.5264, Loss: 1.8898, Test Acc: 0.5000\n",
      "Epoch: 048, Loss: 1.6808, Train Acc: 0.5049, Loss: 2.4595, Test Acc: 0.5000\n",
      "Epoch: 049, Loss: 3.5766, Train Acc: 0.5361, Loss: 2.9235, Test Acc: 0.5000\n",
      "Epoch: 050, Loss: 3.8545, Train Acc: 0.4889, Loss: 5.8785, Test Acc: 0.5000\n",
      "Epoch: 051, Loss: 3.6115, Train Acc: 0.4639, Loss: 7.1889, Test Acc: 0.5000\n",
      "Epoch: 052, Loss: 8.0932, Train Acc: 0.4889, Loss: 6.9430, Test Acc: 0.5000\n",
      "Epoch: 053, Loss: 6.2959, Train Acc: 0.4889, Loss: 3.8138, Test Acc: 0.5625\n",
      "Epoch: 054, Loss: 2.8543, Train Acc: 0.5208, Loss: 5.1239, Test Acc: 0.5000\n",
      "Epoch: 055, Loss: 3.4682, Train Acc: 0.5361, Loss: 3.7091, Test Acc: 0.5000\n",
      "Epoch: 056, Loss: 1.7971, Train Acc: 0.5535, Loss: 2.0151, Test Acc: 0.5625\n",
      "Epoch: 057, Loss: 1.8375, Train Acc: 0.5021, Loss: 3.6883, Test Acc: 0.5000\n",
      "Epoch: 058, Loss: 3.2598, Train Acc: 0.5236, Loss: 7.4279, Test Acc: 0.5000\n",
      "Epoch: 059, Loss: 5.8841, Train Acc: 0.5139, Loss: 7.2666, Test Acc: 0.5000\n",
      "Epoch: 060, Loss: 7.2775, Train Acc: 0.4889, Loss: 8.1814, Test Acc: 0.5000\n",
      "Epoch: 061, Loss: 7.6687, Train Acc: 0.4924, Loss: 2.3686, Test Acc: 0.3750\n",
      "Epoch: 062, Loss: 3.7310, Train Acc: 0.5549, Loss: 2.1990, Test Acc: 0.3750\n",
      "Epoch: 063, Loss: 2.6368, Train Acc: 0.5674, Loss: 4.2927, Test Acc: 0.5000\n",
      "Epoch: 064, Loss: 3.7085, Train Acc: 0.4639, Loss: 2.0850, Test Acc: 0.4375\n",
      "Epoch: 065, Loss: 3.5255, Train Acc: 0.4764, Loss: 4.0656, Test Acc: 0.5000\n",
      "Epoch: 066, Loss: 2.6197, Train Acc: 0.5424, Loss: 4.9052, Test Acc: 0.5000\n",
      "Epoch: 067, Loss: 2.9584, Train Acc: 0.5361, Loss: 3.1989, Test Acc: 0.5000\n",
      "Epoch: 068, Loss: 1.5731, Train Acc: 0.5410, Loss: 1.8330, Test Acc: 0.5625\n",
      "Epoch: 069, Loss: 1.6219, Train Acc: 0.5222, Loss: 2.5835, Test Acc: 0.5000\n",
      "Epoch: 070, Loss: 1.2479, Train Acc: 0.5458, Loss: 1.0293, Test Acc: 0.4375\n",
      "Epoch: 071, Loss: 1.3362, Train Acc: 0.5097, Loss: 1.9183, Test Acc: 0.4375\n",
      "Epoch: 072, Loss: 1.3478, Train Acc: 0.5035, Loss: 2.0134, Test Acc: 0.5000\n",
      "Epoch: 073, Loss: 1.3606, Train Acc: 0.5111, Loss: 1.1109, Test Acc: 0.3750\n",
      "Epoch: 074, Loss: 1.3601, Train Acc: 0.5694, Loss: 1.3685, Test Acc: 0.5000\n",
      "Epoch: 075, Loss: 1.3596, Train Acc: 0.5285, Loss: 1.1762, Test Acc: 0.3750\n",
      "Epoch: 076, Loss: 1.3139, Train Acc: 0.5285, Loss: 1.3862, Test Acc: 0.4375\n",
      "Epoch: 077, Loss: 1.3284, Train Acc: 0.5174, Loss: 1.0653, Test Acc: 0.3125\n",
      "Epoch: 078, Loss: 1.2985, Train Acc: 0.5111, Loss: 1.7175, Test Acc: 0.5000\n",
      "Epoch: 079, Loss: 1.2017, Train Acc: 0.5097, Loss: 1.4783, Test Acc: 0.5000\n",
      "Epoch: 080, Loss: 1.3390, Train Acc: 0.5347, Loss: 1.1531, Test Acc: 0.3750\n",
      "Epoch: 081, Loss: 1.2737, Train Acc: 0.5396, Loss: 1.3909, Test Acc: 0.5000\n",
      "Epoch: 082, Loss: 1.3163, Train Acc: 0.5236, Loss: 1.0833, Test Acc: 0.3750\n",
      "Epoch: 083, Loss: 1.2755, Train Acc: 0.5347, Loss: 1.3571, Test Acc: 0.5000\n",
      "Epoch: 084, Loss: 1.3022, Train Acc: 0.5236, Loss: 1.0401, Test Acc: 0.3125\n",
      "Epoch: 085, Loss: 1.2815, Train Acc: 0.5285, Loss: 1.6970, Test Acc: 0.5000\n",
      "Epoch: 086, Loss: 1.1902, Train Acc: 0.4986, Loss: 1.6385, Test Acc: 0.5000\n",
      "Epoch: 087, Loss: 1.3260, Train Acc: 0.5097, Loss: 1.1044, Test Acc: 0.3750\n",
      "Epoch: 088, Loss: 1.2565, Train Acc: 0.5444, Loss: 1.5655, Test Acc: 0.5000\n",
      "Epoch: 089, Loss: 1.2946, Train Acc: 0.5222, Loss: 1.0004, Test Acc: 0.4375\n",
      "Epoch: 090, Loss: 1.2685, Train Acc: 0.5097, Loss: 1.8558, Test Acc: 0.5000\n",
      "Epoch: 091, Loss: 1.1983, Train Acc: 0.4924, Loss: 1.8444, Test Acc: 0.5000\n",
      "Epoch: 092, Loss: 1.3096, Train Acc: 0.4924, Loss: 1.1096, Test Acc: 0.3750\n",
      "Epoch: 093, Loss: 1.2514, Train Acc: 0.5396, Loss: 1.4480, Test Acc: 0.5000\n",
      "Epoch: 094, Loss: 1.2795, Train Acc: 0.5285, Loss: 1.0334, Test Acc: 0.3750\n",
      "Epoch: 095, Loss: 1.2425, Train Acc: 0.5347, Loss: 1.4275, Test Acc: 0.5000\n",
      "Epoch: 096, Loss: 1.2249, Train Acc: 0.5236, Loss: 0.9784, Test Acc: 0.4375\n",
      "Epoch: 097, Loss: 1.2231, Train Acc: 0.5347, Loss: 1.8610, Test Acc: 0.5000\n",
      "Epoch: 098, Loss: 1.2294, Train Acc: 0.4937, Loss: 2.0196, Test Acc: 0.5000\n",
      "Epoch: 099, Loss: 1.1290, Train Acc: 0.5111, Loss: 0.9582, Test Acc: 0.4375\n",
      "Epoch: 100, Loss: 1.2240, Train Acc: 0.5271, Loss: 1.5204, Test Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN accuracy: 0.3684210479259491\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  66  76  79  82  83  84  85\n",
      "  86  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105\n",
      " 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123\n",
      " 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      " 160 161] TEST: [65 67 68 69 70 71 72 73 74 75 77 78 80 81 87 88]\n",
      "146\n",
      "16\n",
      "Epoch: 000, Loss: 104.4081, Train Acc: 0.4729, Loss: 55.9314, Test Acc: 0.5000\n",
      "Epoch: 001, Loss: 49.7789, Train Acc: 0.5361, Loss: 7.3782, Test Acc: 0.5000\n",
      "Epoch: 002, Loss: 16.4063, Train Acc: 0.4917, Loss: 31.3369, Test Acc: 0.5000\n",
      "Epoch: 003, Loss: 16.5975, Train Acc: 0.5236, Loss: 24.1536, Test Acc: 0.5000\n",
      "Epoch: 004, Loss: 14.2801, Train Acc: 0.5111, Loss: 47.0622, Test Acc: 0.5000\n",
      "Epoch: 005, Loss: 24.2549, Train Acc: 0.5111, Loss: 32.4421, Test Acc: 0.5000\n",
      "Epoch: 006, Loss: 32.0542, Train Acc: 0.5111, Loss: 38.8878, Test Acc: 0.5000\n",
      "Epoch: 007, Loss: 28.8015, Train Acc: 0.4951, Loss: 16.7412, Test Acc: 0.5000\n",
      "Epoch: 008, Loss: 22.6385, Train Acc: 0.4889, Loss: 4.7529, Test Acc: 0.5000\n",
      "Epoch: 009, Loss: 20.8111, Train Acc: 0.5014, Loss: 12.3571, Test Acc: 0.5000\n",
      "Epoch: 010, Loss: 7.2666, Train Acc: 0.4924, Loss: 2.3502, Test Acc: 0.5000\n",
      "Epoch: 011, Loss: 4.0529, Train Acc: 0.4889, Loss: 8.8965, Test Acc: 0.5000\n",
      "Epoch: 012, Loss: 10.4331, Train Acc: 0.4764, Loss: 18.2427, Test Acc: 0.5000\n",
      "Epoch: 013, Loss: 17.2481, Train Acc: 0.4826, Loss: 4.8396, Test Acc: 0.5000\n",
      "Epoch: 014, Loss: 25.7373, Train Acc: 0.5111, Loss: 4.0160, Test Acc: 0.5000\n",
      "Epoch: 015, Loss: 10.4911, Train Acc: 0.5111, Loss: 11.8723, Test Acc: 0.5000\n",
      "Epoch: 016, Loss: 6.7686, Train Acc: 0.5299, Loss: 2.5123, Test Acc: 0.6250\n",
      "Epoch: 017, Loss: 5.8405, Train Acc: 0.5236, Loss: 1.8874, Test Acc: 0.5000\n",
      "Epoch: 018, Loss: 4.8758, Train Acc: 0.5361, Loss: 1.7303, Test Acc: 0.5625\n",
      "Epoch: 019, Loss: 4.2938, Train Acc: 0.5361, Loss: 1.5620, Test Acc: 0.5625\n",
      "Epoch: 020, Loss: 2.7356, Train Acc: 0.5597, Loss: 4.2193, Test Acc: 0.5000\n",
      "Epoch: 021, Loss: 3.4669, Train Acc: 0.4826, Loss: 1.0378, Test Acc: 0.5000\n",
      "Epoch: 022, Loss: 2.7072, Train Acc: 0.5611, Loss: 2.1618, Test Acc: 0.5000\n",
      "Epoch: 023, Loss: 4.2811, Train Acc: 0.5236, Loss: 1.2752, Test Acc: 0.5000\n",
      "Epoch: 024, Loss: 1.6912, Train Acc: 0.5208, Loss: 4.5168, Test Acc: 0.5000\n",
      "Epoch: 025, Loss: 2.8030, Train Acc: 0.4451, Loss: 7.3798, Test Acc: 0.5000\n",
      "Epoch: 026, Loss: 4.9225, Train Acc: 0.5111, Loss: 8.2621, Test Acc: 0.5000\n",
      "Epoch: 027, Loss: 9.5017, Train Acc: 0.4889, Loss: 5.0601, Test Acc: 0.5000\n",
      "Epoch: 028, Loss: 5.9366, Train Acc: 0.4889, Loss: 4.2395, Test Acc: 0.5000\n",
      "Epoch: 029, Loss: 3.9837, Train Acc: 0.5111, Loss: 8.4585, Test Acc: 0.5000\n",
      "Epoch: 030, Loss: 8.3383, Train Acc: 0.4889, Loss: 4.7506, Test Acc: 0.5000\n",
      "Epoch: 031, Loss: 5.4554, Train Acc: 0.4889, Loss: 3.0513, Test Acc: 0.5000\n",
      "Epoch: 032, Loss: 3.4766, Train Acc: 0.5111, Loss: 6.3767, Test Acc: 0.5000\n",
      "Epoch: 033, Loss: 6.9367, Train Acc: 0.4889, Loss: 5.3015, Test Acc: 0.5000\n",
      "Epoch: 034, Loss: 5.9398, Train Acc: 0.5139, Loss: 3.7169, Test Acc: 0.5000\n",
      "Epoch: 035, Loss: 4.3982, Train Acc: 0.4701, Loss: 1.8705, Test Acc: 0.5000\n",
      "Epoch: 036, Loss: 5.5884, Train Acc: 0.4889, Loss: 7.4636, Test Acc: 0.5000\n",
      "Epoch: 037, Loss: 6.4896, Train Acc: 0.5111, Loss: 0.9544, Test Acc: 0.5000\n",
      "Epoch: 038, Loss: 3.3203, Train Acc: 0.5361, Loss: 1.2765, Test Acc: 0.5625\n",
      "Epoch: 039, Loss: 4.0708, Train Acc: 0.5361, Loss: 10.1611, Test Acc: 0.5000\n",
      "Epoch: 040, Loss: 7.7758, Train Acc: 0.4701, Loss: 6.1921, Test Acc: 0.5000\n",
      "Epoch: 041, Loss: 4.6102, Train Acc: 0.4889, Loss: 2.8154, Test Acc: 0.5000\n",
      "Epoch: 042, Loss: 2.6560, Train Acc: 0.4639, Loss: 4.6322, Test Acc: 0.5000\n",
      "Epoch: 043, Loss: 3.2774, Train Acc: 0.4639, Loss: 6.2935, Test Acc: 0.5000\n",
      "Epoch: 044, Loss: 5.7776, Train Acc: 0.4889, Loss: 1.3835, Test Acc: 0.5000\n",
      "Epoch: 045, Loss: 5.0930, Train Acc: 0.4889, Loss: 3.2500, Test Acc: 0.5000\n",
      "Epoch: 046, Loss: 1.9756, Train Acc: 0.4389, Loss: 3.1566, Test Acc: 0.5000\n",
      "Epoch: 047, Loss: 2.8309, Train Acc: 0.5111, Loss: 2.3561, Test Acc: 0.5000\n",
      "Epoch: 048, Loss: 1.4682, Train Acc: 0.5361, Loss: 2.4893, Test Acc: 0.5000\n",
      "Epoch: 049, Loss: 2.2315, Train Acc: 0.5076, Loss: 2.0883, Test Acc: 0.5000\n",
      "Epoch: 050, Loss: 2.7063, Train Acc: 0.5076, Loss: 0.8584, Test Acc: 0.5625\n",
      "Epoch: 051, Loss: 2.3575, Train Acc: 0.4889, Loss: 2.8888, Test Acc: 0.5000\n",
      "Epoch: 052, Loss: 2.0885, Train Acc: 0.5236, Loss: 1.1882, Test Acc: 0.5000\n",
      "Epoch: 053, Loss: 1.1764, Train Acc: 0.5174, Loss: 1.5951, Test Acc: 0.5000\n",
      "Epoch: 054, Loss: 1.3572, Train Acc: 0.4486, Loss: 0.9022, Test Acc: 0.4375\n",
      "Epoch: 055, Loss: 1.0691, Train Acc: 0.4889, Loss: 0.8040, Test Acc: 0.3750\n",
      "Epoch: 056, Loss: 0.8609, Train Acc: 0.5646, Loss: 1.2336, Test Acc: 0.5000\n",
      "Epoch: 057, Loss: 1.2657, Train Acc: 0.4799, Loss: 0.9210, Test Acc: 0.4375\n",
      "Epoch: 058, Loss: 2.2773, Train Acc: 0.5299, Loss: 0.7849, Test Acc: 0.4375\n",
      "Epoch: 059, Loss: 2.2884, Train Acc: 0.5236, Loss: 0.8023, Test Acc: 0.5000\n",
      "Epoch: 060, Loss: 2.0480, Train Acc: 0.5299, Loss: 0.8718, Test Acc: 0.5625\n",
      "Epoch: 061, Loss: 1.3217, Train Acc: 0.5486, Loss: 2.9243, Test Acc: 0.5000\n",
      "Epoch: 062, Loss: 2.1584, Train Acc: 0.4951, Loss: 1.4384, Test Acc: 0.5000\n",
      "Epoch: 063, Loss: 2.2137, Train Acc: 0.4764, Loss: 2.9508, Test Acc: 0.5000\n",
      "Epoch: 064, Loss: 3.5004, Train Acc: 0.4889, Loss: 0.8363, Test Acc: 0.3750\n",
      "Epoch: 065, Loss: 2.2651, Train Acc: 0.4764, Loss: 2.6442, Test Acc: 0.5000\n",
      "Epoch: 066, Loss: 2.0617, Train Acc: 0.5111, Loss: 2.2778, Test Acc: 0.5000\n",
      "Epoch: 067, Loss: 1.6804, Train Acc: 0.5361, Loss: 2.0071, Test Acc: 0.5000\n",
      "Epoch: 068, Loss: 1.6923, Train Acc: 0.4826, Loss: 1.0181, Test Acc: 0.6250\n",
      "Epoch: 069, Loss: 2.3476, Train Acc: 0.4951, Loss: 2.2288, Test Acc: 0.5000\n",
      "Epoch: 070, Loss: 1.9082, Train Acc: 0.4764, Loss: 2.7372, Test Acc: 0.5000\n",
      "Epoch: 071, Loss: 2.3874, Train Acc: 0.5111, Loss: 2.3466, Test Acc: 0.5000\n",
      "Epoch: 072, Loss: 1.3112, Train Acc: 0.5361, Loss: 1.6918, Test Acc: 0.5000\n",
      "Epoch: 073, Loss: 1.1504, Train Acc: 0.4986, Loss: 0.7878, Test Acc: 0.4375\n",
      "Epoch: 074, Loss: 0.7757, Train Acc: 0.5535, Loss: 0.8052, Test Acc: 0.5000\n",
      "Epoch: 075, Loss: 0.7619, Train Acc: 0.5660, Loss: 0.7663, Test Acc: 0.5000\n",
      "Epoch: 076, Loss: 0.8378, Train Acc: 0.5535, Loss: 1.1611, Test Acc: 0.5000\n",
      "Epoch: 077, Loss: 1.1637, Train Acc: 0.5049, Loss: 0.7973, Test Acc: 0.3750\n",
      "Epoch: 078, Loss: 1.0350, Train Acc: 0.4938, Loss: 0.8005, Test Acc: 0.4375\n",
      "Epoch: 079, Loss: 0.9575, Train Acc: 0.5486, Loss: 1.4576, Test Acc: 0.5000\n",
      "Epoch: 080, Loss: 0.9739, Train Acc: 0.5361, Loss: 0.9606, Test Acc: 0.5000\n",
      "Epoch: 081, Loss: 0.7452, Train Acc: 0.5799, Loss: 0.9802, Test Acc: 0.5000\n",
      "Epoch: 082, Loss: 0.9052, Train Acc: 0.5236, Loss: 1.2854, Test Acc: 0.5000\n",
      "Epoch: 083, Loss: 0.9870, Train Acc: 0.5299, Loss: 1.0500, Test Acc: 0.5000\n",
      "Epoch: 084, Loss: 0.7752, Train Acc: 0.5535, Loss: 0.9161, Test Acc: 0.5000\n",
      "Epoch: 085, Loss: 0.9048, Train Acc: 0.5299, Loss: 1.3464, Test Acc: 0.5000\n",
      "Epoch: 086, Loss: 1.0494, Train Acc: 0.5236, Loss: 0.9336, Test Acc: 0.5000\n",
      "Epoch: 087, Loss: 0.8364, Train Acc: 0.5486, Loss: 0.9440, Test Acc: 0.5000\n",
      "Epoch: 088, Loss: 0.7565, Train Acc: 0.5486, Loss: 0.8163, Test Acc: 0.6250\n",
      "Epoch: 089, Loss: 0.8760, Train Acc: 0.5299, Loss: 1.3569, Test Acc: 0.5000\n",
      "Epoch: 090, Loss: 1.1812, Train Acc: 0.4986, Loss: 0.8233, Test Acc: 0.3750\n",
      "Epoch: 091, Loss: 1.0299, Train Acc: 0.5076, Loss: 1.0627, Test Acc: 0.5000\n",
      "Epoch: 092, Loss: 1.1149, Train Acc: 0.5611, Loss: 1.2525, Test Acc: 0.5000\n",
      "Epoch: 093, Loss: 0.8336, Train Acc: 0.5424, Loss: 1.1333, Test Acc: 0.5000\n",
      "Epoch: 094, Loss: 0.9006, Train Acc: 0.5111, Loss: 1.1317, Test Acc: 0.5000\n",
      "Epoch: 095, Loss: 1.0775, Train Acc: 0.5299, Loss: 0.9420, Test Acc: 0.5000\n",
      "Epoch: 096, Loss: 0.7974, Train Acc: 0.5236, Loss: 1.1016, Test Acc: 0.5000\n",
      "Epoch: 097, Loss: 0.8711, Train Acc: 0.4924, Loss: 1.0283, Test Acc: 0.5000\n",
      "Epoch: 098, Loss: 0.9953, Train Acc: 0.5424, Loss: 1.3189, Test Acc: 0.5000\n",
      "Epoch: 099, Loss: 0.9100, Train Acc: 0.5174, Loss: 1.1929, Test Acc: 0.5000\n",
      "Epoch: 100, Loss: 1.0938, Train Acc: 0.5236, Loss: 0.9393, Test Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN accuracy: 0.5789473652839661\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  77  78  80  81  87  88  94  95  96  98 101 103 104 105\n",
      " 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123\n",
      " 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      " 160 161] TEST: [ 76  79  82  83  84  85  86  89  90  91  92  93  97  99 100 102]\n",
      "146\n",
      "16\n",
      "Epoch: 000, Loss: 60.3533, Train Acc: 0.5687, Loss: 334.6794, Test Acc: 0.5000\n",
      "Epoch: 001, Loss: 119.7666, Train Acc: 0.5111, Loss: 49.3354, Test Acc: 0.5000\n",
      "Epoch: 002, Loss: 37.3890, Train Acc: 0.5361, Loss: 23.1589, Test Acc: 0.5000\n",
      "Epoch: 003, Loss: 40.9065, Train Acc: 0.5000, Loss: 28.2188, Test Acc: 0.5000\n",
      "Epoch: 004, Loss: 27.3931, Train Acc: 0.4889, Loss: 26.4490, Test Acc: 0.5000\n",
      "Epoch: 005, Loss: 28.9719, Train Acc: 0.5139, Loss: 46.7519, Test Acc: 0.5000\n",
      "Epoch: 006, Loss: 32.9249, Train Acc: 0.5111, Loss: 43.5124, Test Acc: 0.5000\n",
      "Epoch: 007, Loss: 21.4493, Train Acc: 0.4889, Loss: 7.1261, Test Acc: 0.5000\n",
      "Epoch: 008, Loss: 12.3842, Train Acc: 0.4889, Loss: 2.5760, Test Acc: 0.4375\n",
      "Epoch: 009, Loss: 12.6870, Train Acc: 0.4889, Loss: 18.3262, Test Acc: 0.5000\n",
      "Epoch: 010, Loss: 17.3053, Train Acc: 0.5111, Loss: 3.8415, Test Acc: 0.5000\n",
      "Epoch: 011, Loss: 21.8961, Train Acc: 0.4889, Loss: 11.5930, Test Acc: 0.5000\n",
      "Epoch: 012, Loss: 15.1570, Train Acc: 0.5139, Loss: 13.0103, Test Acc: 0.5000\n",
      "Epoch: 013, Loss: 8.3120, Train Acc: 0.5111, Loss: 7.7315, Test Acc: 0.5000\n",
      "Epoch: 014, Loss: 9.3051, Train Acc: 0.5111, Loss: 15.3520, Test Acc: 0.5000\n",
      "Epoch: 015, Loss: 14.6718, Train Acc: 0.4889, Loss: 6.6525, Test Acc: 0.5000\n",
      "Epoch: 016, Loss: 13.3181, Train Acc: 0.5111, Loss: 1.4459, Test Acc: 0.5000\n",
      "Epoch: 017, Loss: 4.8170, Train Acc: 0.5111, Loss: 3.3892, Test Acc: 0.5000\n",
      "Epoch: 018, Loss: 6.7708, Train Acc: 0.5111, Loss: 15.7712, Test Acc: 0.5000\n",
      "Epoch: 019, Loss: 11.5443, Train Acc: 0.4764, Loss: 7.9044, Test Acc: 0.5000\n",
      "Epoch: 020, Loss: 6.6416, Train Acc: 0.5000, Loss: 4.4812, Test Acc: 0.5000\n",
      "Epoch: 021, Loss: 2.5662, Train Acc: 0.5097, Loss: 1.3471, Test Acc: 0.3750\n",
      "Epoch: 022, Loss: 1.5525, Train Acc: 0.5326, Loss: 1.6083, Test Acc: 0.5000\n",
      "Epoch: 023, Loss: 1.0130, Train Acc: 0.4889, Loss: 3.3937, Test Acc: 0.5000\n",
      "Epoch: 024, Loss: 3.7220, Train Acc: 0.5049, Loss: 7.0401, Test Acc: 0.5000\n",
      "Epoch: 025, Loss: 7.2939, Train Acc: 0.4889, Loss: 5.9282, Test Acc: 0.5000\n",
      "Epoch: 026, Loss: 5.4828, Train Acc: 0.4889, Loss: 2.7175, Test Acc: 0.5000\n",
      "Epoch: 027, Loss: 3.3168, Train Acc: 0.5111, Loss: 4.6307, Test Acc: 0.5000\n",
      "Epoch: 028, Loss: 3.5442, Train Acc: 0.5236, Loss: 7.4408, Test Acc: 0.5000\n",
      "Epoch: 029, Loss: 5.6111, Train Acc: 0.4889, Loss: 6.1503, Test Acc: 0.5000\n",
      "Epoch: 030, Loss: 5.9615, Train Acc: 0.5389, Loss: 3.6349, Test Acc: 0.5000\n",
      "Epoch: 031, Loss: 3.6753, Train Acc: 0.4639, Loss: 1.9802, Test Acc: 0.5000\n",
      "Epoch: 032, Loss: 4.1958, Train Acc: 0.4889, Loss: 7.0521, Test Acc: 0.5000\n",
      "Epoch: 033, Loss: 4.7560, Train Acc: 0.5174, Loss: 6.2018, Test Acc: 0.5000\n",
      "Epoch: 034, Loss: 5.5354, Train Acc: 0.5174, Loss: 5.3008, Test Acc: 0.5000\n",
      "Epoch: 035, Loss: 4.0323, Train Acc: 0.5160, Loss: 4.5879, Test Acc: 0.5000\n",
      "Epoch: 036, Loss: 3.3467, Train Acc: 0.5111, Loss: 3.9962, Test Acc: 0.5000\n",
      "Epoch: 037, Loss: 3.0295, Train Acc: 0.5111, Loss: 2.4219, Test Acc: 0.5000\n",
      "Epoch: 038, Loss: 1.6668, Train Acc: 0.5174, Loss: 1.8902, Test Acc: 0.5000\n",
      "Epoch: 039, Loss: 1.3240, Train Acc: 0.4799, Loss: 0.8456, Test Acc: 0.5000\n",
      "Epoch: 040, Loss: 0.9549, Train Acc: 0.5125, Loss: 0.8525, Test Acc: 0.4375\n",
      "Epoch: 041, Loss: 1.0388, Train Acc: 0.5236, Loss: 1.4747, Test Acc: 0.5000\n",
      "Epoch: 042, Loss: 1.3462, Train Acc: 0.5049, Loss: 0.7236, Test Acc: 0.3750\n",
      "Epoch: 043, Loss: 1.2346, Train Acc: 0.5076, Loss: 0.7544, Test Acc: 0.5000\n",
      "Epoch: 044, Loss: 0.9244, Train Acc: 0.5125, Loss: 0.9870, Test Acc: 0.4375\n",
      "Epoch: 045, Loss: 1.5366, Train Acc: 0.4861, Loss: 1.9413, Test Acc: 0.5000\n",
      "Epoch: 046, Loss: 2.3726, Train Acc: 0.5361, Loss: 1.6155, Test Acc: 0.5000\n",
      "Epoch: 047, Loss: 1.2729, Train Acc: 0.4986, Loss: 0.8750, Test Acc: 0.4375\n",
      "Epoch: 048, Loss: 0.9857, Train Acc: 0.5125, Loss: 0.9086, Test Acc: 0.4375\n",
      "Epoch: 049, Loss: 1.0789, Train Acc: 0.5236, Loss: 1.6004, Test Acc: 0.5000\n",
      "Epoch: 050, Loss: 1.0177, Train Acc: 0.5062, Loss: 0.8155, Test Acc: 0.4375\n",
      "Epoch: 051, Loss: 1.0588, Train Acc: 0.5236, Loss: 1.7198, Test Acc: 0.5000\n",
      "Epoch: 052, Loss: 1.1457, Train Acc: 0.5424, Loss: 1.4738, Test Acc: 0.5000\n",
      "Epoch: 053, Loss: 1.0234, Train Acc: 0.5062, Loss: 0.7975, Test Acc: 0.5000\n",
      "Epoch: 054, Loss: 1.1284, Train Acc: 0.5236, Loss: 0.7619, Test Acc: 0.5000\n",
      "Epoch: 055, Loss: 1.8460, Train Acc: 0.5299, Loss: 2.7114, Test Acc: 0.5000\n",
      "Epoch: 056, Loss: 2.4256, Train Acc: 0.4889, Loss: 1.0473, Test Acc: 0.4375\n",
      "Epoch: 057, Loss: 2.8157, Train Acc: 0.4639, Loss: 2.5531, Test Acc: 0.5000\n",
      "Epoch: 058, Loss: 2.2274, Train Acc: 0.5111, Loss: 2.0678, Test Acc: 0.5000\n",
      "Epoch: 059, Loss: 2.2809, Train Acc: 0.5361, Loss: 1.2043, Test Acc: 0.4375\n",
      "Epoch: 060, Loss: 1.1135, Train Acc: 0.5188, Loss: 0.7948, Test Acc: 0.5625\n",
      "Epoch: 061, Loss: 0.8057, Train Acc: 0.5361, Loss: 1.2941, Test Acc: 0.5000\n",
      "Epoch: 062, Loss: 1.0207, Train Acc: 0.4889, Loss: 0.7501, Test Acc: 0.5000\n",
      "Epoch: 063, Loss: 1.0309, Train Acc: 0.5049, Loss: 1.6651, Test Acc: 0.5000\n",
      "Epoch: 064, Loss: 1.2734, Train Acc: 0.5049, Loss: 0.7480, Test Acc: 0.5000\n",
      "Epoch: 065, Loss: 1.1178, Train Acc: 0.5111, Loss: 0.7384, Test Acc: 0.5000\n",
      "Epoch: 066, Loss: 1.0973, Train Acc: 0.4986, Loss: 0.8308, Test Acc: 0.5000\n",
      "Epoch: 067, Loss: 2.4917, Train Acc: 0.5361, Loss: 0.9922, Test Acc: 0.5000\n",
      "Epoch: 068, Loss: 4.2617, Train Acc: 0.5111, Loss: 10.1188, Test Acc: 0.5000\n",
      "Epoch: 069, Loss: 9.8548, Train Acc: 0.4889, Loss: 9.6700, Test Acc: 0.5000\n",
      "Epoch: 070, Loss: 4.2773, Train Acc: 0.4889, Loss: 3.3931, Test Acc: 0.5000\n",
      "Epoch: 071, Loss: 4.1924, Train Acc: 0.4639, Loss: 3.0036, Test Acc: 0.5000\n",
      "Epoch: 072, Loss: 2.7967, Train Acc: 0.5174, Loss: 2.7541, Test Acc: 0.5000\n",
      "Epoch: 073, Loss: 2.6519, Train Acc: 0.5361, Loss: 1.8735, Test Acc: 0.4375\n",
      "Epoch: 074, Loss: 1.4475, Train Acc: 0.5285, Loss: 1.4155, Test Acc: 0.5000\n",
      "Epoch: 075, Loss: 1.1545, Train Acc: 0.5375, Loss: 0.7400, Test Acc: 0.6875\n",
      "Epoch: 076, Loss: 1.5336, Train Acc: 0.5674, Loss: 1.0221, Test Acc: 0.5000\n",
      "Epoch: 077, Loss: 2.5994, Train Acc: 0.5361, Loss: 1.4166, Test Acc: 0.5000\n",
      "Epoch: 078, Loss: 1.3164, Train Acc: 0.4799, Loss: 0.7253, Test Acc: 0.5000\n",
      "Epoch: 079, Loss: 1.0586, Train Acc: 0.5174, Loss: 1.1279, Test Acc: 0.5000\n",
      "Epoch: 080, Loss: 1.0309, Train Acc: 0.5000, Loss: 0.7379, Test Acc: 0.5625\n",
      "Epoch: 081, Loss: 1.0724, Train Acc: 0.5174, Loss: 1.8134, Test Acc: 0.5000\n",
      "Epoch: 082, Loss: 1.1151, Train Acc: 0.5236, Loss: 1.2559, Test Acc: 0.5000\n",
      "Epoch: 083, Loss: 1.0129, Train Acc: 0.4889, Loss: 0.8412, Test Acc: 0.5000\n",
      "Epoch: 084, Loss: 1.0629, Train Acc: 0.5111, Loss: 0.7037, Test Acc: 0.6250\n",
      "Epoch: 085, Loss: 0.9665, Train Acc: 0.5549, Loss: 0.7219, Test Acc: 0.5000\n",
      "Epoch: 086, Loss: 0.9557, Train Acc: 0.5111, Loss: 1.1313, Test Acc: 0.5000\n",
      "Epoch: 087, Loss: 1.4990, Train Acc: 0.4986, Loss: 1.6035, Test Acc: 0.5000\n",
      "Epoch: 088, Loss: 2.3296, Train Acc: 0.5361, Loss: 1.0837, Test Acc: 0.5000\n",
      "Epoch: 089, Loss: 1.0482, Train Acc: 0.5160, Loss: 0.7824, Test Acc: 0.5000\n",
      "Epoch: 090, Loss: 0.9346, Train Acc: 0.5111, Loss: 0.8467, Test Acc: 0.4375\n",
      "Epoch: 091, Loss: 0.8755, Train Acc: 0.4951, Loss: 0.7410, Test Acc: 0.5000\n",
      "Epoch: 092, Loss: 0.8405, Train Acc: 0.5014, Loss: 0.9077, Test Acc: 0.5000\n",
      "Epoch: 093, Loss: 0.7802, Train Acc: 0.5312, Loss: 1.1598, Test Acc: 0.5000\n",
      "Epoch: 094, Loss: 0.8525, Train Acc: 0.4889, Loss: 1.2578, Test Acc: 0.5000\n",
      "Epoch: 095, Loss: 0.9480, Train Acc: 0.5389, Loss: 1.1543, Test Acc: 0.5000\n",
      "Epoch: 096, Loss: 0.9362, Train Acc: 0.5201, Loss: 0.7037, Test Acc: 0.4375\n",
      "Epoch: 097, Loss: 0.9242, Train Acc: 0.5361, Loss: 1.2292, Test Acc: 0.5000\n",
      "Epoch: 098, Loss: 1.4362, Train Acc: 0.5049, Loss: 1.4607, Test Acc: 0.5000\n",
      "Epoch: 099, Loss: 2.2784, Train Acc: 0.5361, Loss: 0.9724, Test Acc: 0.5000\n",
      "Epoch: 100, Loss: 1.0521, Train Acc: 0.4826, Loss: 1.5851, Test Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN accuracy: 0.5263158082962036\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  97  99 100 102 113 114 116 117 118 119 120 121 122 123\n",
      " 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      " 160 161] TEST: [ 94  95  96  98 101 103 104 105 106 107 108 109 110 111 112 115]\n",
      "146\n",
      "16\n",
      "Epoch: 000, Loss: 76.6160, Train Acc: 0.4403, Loss: 187.8840, Test Acc: 0.5625\n",
      "Epoch: 001, Loss: 66.0682, Train Acc: 0.5174, Loss: 76.1357, Test Acc: 0.4375\n",
      "Epoch: 002, Loss: 51.4736, Train Acc: 0.5049, Loss: 93.7247, Test Acc: 0.4375\n",
      "Epoch: 003, Loss: 55.3457, Train Acc: 0.5049, Loss: 13.6146, Test Acc: 0.5625\n",
      "Epoch: 004, Loss: 25.2918, Train Acc: 0.5174, Loss: 25.2713, Test Acc: 0.5625\n",
      "Epoch: 005, Loss: 15.7828, Train Acc: 0.5222, Loss: 9.9287, Test Acc: 0.5000\n",
      "Epoch: 006, Loss: 7.4280, Train Acc: 0.5597, Loss: 10.7208, Test Acc: 0.4375\n",
      "Epoch: 007, Loss: 5.3231, Train Acc: 0.5049, Loss: 13.8806, Test Acc: 0.4375\n",
      "Epoch: 008, Loss: 10.6918, Train Acc: 0.4764, Loss: 12.2446, Test Acc: 0.4375\n",
      "Epoch: 009, Loss: 16.3610, Train Acc: 0.4826, Loss: 39.7388, Test Acc: 0.4375\n",
      "Epoch: 010, Loss: 15.7557, Train Acc: 0.4938, Loss: 7.3252, Test Acc: 0.5000\n",
      "Epoch: 011, Loss: 7.0567, Train Acc: 0.4924, Loss: 3.5556, Test Acc: 0.5000\n",
      "Epoch: 012, Loss: 4.0881, Train Acc: 0.4875, Loss: 8.3242, Test Acc: 0.5625\n",
      "Epoch: 013, Loss: 6.7846, Train Acc: 0.5174, Loss: 9.8393, Test Acc: 0.5625\n",
      "Epoch: 014, Loss: 8.9057, Train Acc: 0.5299, Loss: 4.3539, Test Acc: 0.5000\n",
      "Epoch: 015, Loss: 3.4611, Train Acc: 0.5111, Loss: 6.8601, Test Acc: 0.4375\n",
      "Epoch: 016, Loss: 3.4131, Train Acc: 0.5174, Loss: 0.8370, Test Acc: 0.4375\n",
      "Epoch: 017, Loss: 5.6942, Train Acc: 0.5139, Loss: 15.8600, Test Acc: 0.4375\n",
      "Epoch: 018, Loss: 10.3399, Train Acc: 0.5174, Loss: 18.4600, Test Acc: 0.4375\n",
      "Epoch: 019, Loss: 12.9518, Train Acc: 0.5049, Loss: 3.0614, Test Acc: 0.5625\n",
      "Epoch: 020, Loss: 8.9111, Train Acc: 0.5174, Loss: 17.8670, Test Acc: 0.5625\n",
      "Epoch: 021, Loss: 18.8979, Train Acc: 0.4951, Loss: 10.2286, Test Acc: 0.4375\n",
      "Epoch: 022, Loss: 16.5344, Train Acc: 0.4826, Loss: 17.8528, Test Acc: 0.4375\n",
      "Epoch: 023, Loss: 9.2176, Train Acc: 0.4764, Loss: 8.0191, Test Acc: 0.5000\n",
      "Epoch: 024, Loss: 8.1236, Train Acc: 0.4875, Loss: 7.8582, Test Acc: 0.5625\n",
      "Epoch: 025, Loss: 7.4661, Train Acc: 0.5236, Loss: 9.9812, Test Acc: 0.5625\n",
      "Epoch: 026, Loss: 8.5626, Train Acc: 0.5236, Loss: 11.9924, Test Acc: 0.5625\n",
      "Epoch: 027, Loss: 11.7239, Train Acc: 0.4889, Loss: 11.6106, Test Acc: 0.5625\n",
      "Epoch: 028, Loss: 13.9347, Train Acc: 0.4889, Loss: 21.9730, Test Acc: 0.4375\n",
      "Epoch: 029, Loss: 10.2218, Train Acc: 0.5049, Loss: 3.0269, Test Acc: 0.4375\n",
      "Epoch: 030, Loss: 2.8989, Train Acc: 0.5535, Loss: 5.1357, Test Acc: 0.4375\n",
      "Epoch: 031, Loss: 3.9224, Train Acc: 0.4764, Loss: 2.9499, Test Acc: 0.5000\n",
      "Epoch: 032, Loss: 4.4607, Train Acc: 0.4764, Loss: 1.1195, Test Acc: 0.4375\n",
      "Epoch: 033, Loss: 3.2258, Train Acc: 0.5174, Loss: 7.6066, Test Acc: 0.5625\n",
      "Epoch: 034, Loss: 7.0566, Train Acc: 0.5014, Loss: 14.3583, Test Acc: 0.5625\n",
      "Epoch: 035, Loss: 16.6071, Train Acc: 0.5076, Loss: 18.7941, Test Acc: 0.4375\n",
      "Epoch: 036, Loss: 13.2223, Train Acc: 0.5049, Loss: 16.9256, Test Acc: 0.5625\n",
      "Epoch: 037, Loss: 11.1578, Train Acc: 0.5139, Loss: 7.2856, Test Acc: 0.5000\n",
      "Epoch: 038, Loss: 5.4987, Train Acc: 0.5236, Loss: 5.7388, Test Acc: 0.4375\n",
      "Epoch: 039, Loss: 4.6802, Train Acc: 0.4826, Loss: 4.0850, Test Acc: 0.4375\n",
      "Epoch: 040, Loss: 5.5758, Train Acc: 0.4701, Loss: 2.2855, Test Acc: 0.5625\n",
      "Epoch: 041, Loss: 3.4928, Train Acc: 0.5111, Loss: 5.5388, Test Acc: 0.5625\n",
      "Epoch: 042, Loss: 4.4548, Train Acc: 0.5236, Loss: 5.0388, Test Acc: 0.5625\n",
      "Epoch: 043, Loss: 4.0613, Train Acc: 0.5111, Loss: 3.7280, Test Acc: 0.5625\n",
      "Epoch: 044, Loss: 3.2861, Train Acc: 0.5361, Loss: 5.7291, Test Acc: 0.5625\n",
      "Epoch: 045, Loss: 6.7700, Train Acc: 0.4951, Loss: 2.5148, Test Acc: 0.5625\n",
      "Epoch: 046, Loss: 4.5187, Train Acc: 0.4951, Loss: 1.3946, Test Acc: 0.5625\n",
      "Epoch: 047, Loss: 3.9930, Train Acc: 0.5076, Loss: 4.7152, Test Acc: 0.4375\n",
      "Epoch: 048, Loss: 2.2735, Train Acc: 0.5000, Loss: 1.9368, Test Acc: 0.5625\n",
      "Epoch: 049, Loss: 2.2220, Train Acc: 0.5361, Loss: 3.8189, Test Acc: 0.5625\n",
      "Epoch: 050, Loss: 2.5349, Train Acc: 0.5799, Loss: 2.3066, Test Acc: 0.5625\n",
      "Epoch: 051, Loss: 2.4121, Train Acc: 0.5299, Loss: 2.8634, Test Acc: 0.4375\n",
      "Epoch: 052, Loss: 2.5921, Train Acc: 0.4889, Loss: 3.5295, Test Acc: 0.4375\n",
      "Epoch: 053, Loss: 2.5938, Train Acc: 0.4812, Loss: 3.0509, Test Acc: 0.5625\n",
      "Epoch: 054, Loss: 2.7339, Train Acc: 0.5111, Loss: 2.6114, Test Acc: 0.5625\n",
      "Epoch: 055, Loss: 1.7828, Train Acc: 0.5299, Loss: 2.3695, Test Acc: 0.5000\n",
      "Epoch: 056, Loss: 1.5150, Train Acc: 0.4826, Loss: 1.8193, Test Acc: 0.5625\n",
      "Epoch: 057, Loss: 2.5324, Train Acc: 0.5174, Loss: 5.4876, Test Acc: 0.5625\n",
      "Epoch: 058, Loss: 5.0801, Train Acc: 0.4951, Loss: 4.4271, Test Acc: 0.5625\n",
      "Epoch: 059, Loss: 4.9015, Train Acc: 0.5076, Loss: 1.4917, Test Acc: 0.4375\n",
      "Epoch: 060, Loss: 3.8640, Train Acc: 0.4889, Loss: 7.8281, Test Acc: 0.4375\n",
      "Epoch: 061, Loss: 5.2217, Train Acc: 0.5174, Loss: 1.8341, Test Acc: 0.4375\n",
      "Epoch: 062, Loss: 2.8055, Train Acc: 0.5236, Loss: 1.0795, Test Acc: 0.4375\n",
      "Epoch: 063, Loss: 2.1200, Train Acc: 0.5674, Loss: 1.4970, Test Acc: 0.4375\n",
      "Epoch: 064, Loss: 1.1386, Train Acc: 0.5111, Loss: 0.6929, Test Acc: 0.5625\n",
      "Epoch: 065, Loss: 0.9432, Train Acc: 0.4764, Loss: 0.8764, Test Acc: 0.5625\n",
      "Epoch: 066, Loss: 1.0388, Train Acc: 0.5326, Loss: 0.8324, Test Acc: 0.5625\n",
      "Epoch: 067, Loss: 1.0367, Train Acc: 0.5063, Loss: 1.2527, Test Acc: 0.5000\n",
      "Epoch: 068, Loss: 1.2885, Train Acc: 0.5299, Loss: 0.6939, Test Acc: 0.6250\n",
      "Epoch: 069, Loss: 0.9929, Train Acc: 0.5174, Loss: 0.8141, Test Acc: 0.5000\n",
      "Epoch: 070, Loss: 0.9997, Train Acc: 0.4764, Loss: 0.6371, Test Acc: 0.5625\n",
      "Epoch: 071, Loss: 0.9578, Train Acc: 0.5188, Loss: 0.6956, Test Acc: 0.4375\n",
      "Epoch: 072, Loss: 1.1225, Train Acc: 0.5299, Loss: 1.5635, Test Acc: 0.4375\n",
      "Epoch: 073, Loss: 0.9278, Train Acc: 0.5285, Loss: 0.7392, Test Acc: 0.5625\n",
      "Epoch: 074, Loss: 0.8911, Train Acc: 0.5424, Loss: 1.0677, Test Acc: 0.5000\n",
      "Epoch: 075, Loss: 1.0483, Train Acc: 0.4826, Loss: 0.7772, Test Acc: 0.5625\n",
      "Epoch: 076, Loss: 0.7965, Train Acc: 0.5674, Loss: 0.9383, Test Acc: 0.5000\n",
      "Epoch: 077, Loss: 1.0328, Train Acc: 0.4604, Loss: 0.7181, Test Acc: 0.4375\n",
      "Epoch: 078, Loss: 1.0295, Train Acc: 0.5111, Loss: 1.4879, Test Acc: 0.4375\n",
      "Epoch: 079, Loss: 1.1147, Train Acc: 0.5111, Loss: 1.3985, Test Acc: 0.4375\n",
      "Epoch: 080, Loss: 1.0782, Train Acc: 0.4826, Loss: 1.0544, Test Acc: 0.5625\n",
      "Epoch: 081, Loss: 0.9785, Train Acc: 0.5389, Loss: 1.4078, Test Acc: 0.5625\n",
      "Epoch: 082, Loss: 1.1819, Train Acc: 0.5424, Loss: 1.3913, Test Acc: 0.4375\n",
      "Epoch: 083, Loss: 1.2524, Train Acc: 0.5111, Loss: 0.8082, Test Acc: 0.5000\n",
      "Epoch: 084, Loss: 0.9377, Train Acc: 0.4924, Loss: 0.7671, Test Acc: 0.5000\n",
      "Epoch: 085, Loss: 0.9321, Train Acc: 0.4986, Loss: 0.7876, Test Acc: 0.5000\n",
      "Epoch: 086, Loss: 0.7628, Train Acc: 0.5361, Loss: 1.2477, Test Acc: 0.4375\n",
      "Epoch: 087, Loss: 1.0424, Train Acc: 0.5160, Loss: 0.6864, Test Acc: 0.5625\n",
      "Epoch: 088, Loss: 0.9831, Train Acc: 0.5424, Loss: 0.7168, Test Acc: 0.3750\n",
      "Epoch: 089, Loss: 1.0728, Train Acc: 0.5236, Loss: 0.6081, Test Acc: 0.6250\n",
      "Epoch: 090, Loss: 0.9941, Train Acc: 0.5111, Loss: 0.8437, Test Acc: 0.4375\n",
      "Epoch: 091, Loss: 1.0405, Train Acc: 0.5222, Loss: 0.6677, Test Acc: 0.5625\n",
      "Epoch: 092, Loss: 0.9678, Train Acc: 0.5264, Loss: 0.6088, Test Acc: 0.6250\n",
      "Epoch: 093, Loss: 1.0347, Train Acc: 0.5063, Loss: 0.9463, Test Acc: 0.5000\n",
      "Epoch: 094, Loss: 1.2248, Train Acc: 0.5236, Loss: 0.9635, Test Acc: 0.5000\n",
      "Epoch: 095, Loss: 1.0333, Train Acc: 0.4826, Loss: 0.8693, Test Acc: 0.5625\n",
      "Epoch: 096, Loss: 1.0879, Train Acc: 0.5174, Loss: 0.6983, Test Acc: 0.5625\n",
      "Epoch: 097, Loss: 1.1127, Train Acc: 0.5174, Loss: 0.7337, Test Acc: 0.4375\n",
      "Epoch: 098, Loss: 0.9548, Train Acc: 0.4764, Loss: 1.2167, Test Acc: 0.5625\n",
      "Epoch: 099, Loss: 1.1780, Train Acc: 0.5299, Loss: 2.5539, Test Acc: 0.4375\n",
      "Epoch: 100, Loss: 1.5038, Train Acc: 0.4924, Loss: 0.7088, Test Acc: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN accuracy: 0.5263158082962036\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 115 127 128 130 133 134 135 136 137 138 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      " 160 161] TEST: [113 114 116 117 118 119 120 121 122 123 124 125 126 129 131 132]\n",
      "146\n",
      "16\n",
      "Epoch: 000, Loss: 45.0616, Train Acc: 0.5521, Loss: 219.3397, Test Acc: 0.4375\n",
      "Epoch: 001, Loss: 124.8127, Train Acc: 0.5049, Loss: 23.8329, Test Acc: 0.5625\n",
      "Epoch: 002, Loss: 67.5867, Train Acc: 0.5236, Loss: 52.9318, Test Acc: 0.5625\n",
      "Epoch: 003, Loss: 32.7604, Train Acc: 0.4951, Loss: 28.8205, Test Acc: 0.5625\n",
      "Epoch: 004, Loss: 24.0597, Train Acc: 0.5264, Loss: 3.9560, Test Acc: 0.5000\n",
      "Epoch: 005, Loss: 19.9110, Train Acc: 0.4764, Loss: 26.5435, Test Acc: 0.4375\n",
      "Epoch: 006, Loss: 21.9700, Train Acc: 0.5049, Loss: 8.3629, Test Acc: 0.4375\n",
      "Epoch: 007, Loss: 11.9851, Train Acc: 0.4736, Loss: 13.4068, Test Acc: 0.5625\n",
      "Epoch: 008, Loss: 11.0957, Train Acc: 0.5076, Loss: 9.8048, Test Acc: 0.5625\n",
      "Epoch: 009, Loss: 12.6362, Train Acc: 0.5076, Loss: 21.9993, Test Acc: 0.4375\n",
      "Epoch: 010, Loss: 15.3016, Train Acc: 0.4799, Loss: 19.5989, Test Acc: 0.5625\n",
      "Epoch: 011, Loss: 12.3061, Train Acc: 0.5076, Loss: 9.2097, Test Acc: 0.4375\n",
      "Epoch: 012, Loss: 6.4529, Train Acc: 0.5049, Loss: 6.1846, Test Acc: 0.4375\n",
      "Epoch: 013, Loss: 3.5307, Train Acc: 0.4826, Loss: 3.3205, Test Acc: 0.4375\n",
      "Epoch: 014, Loss: 2.6011, Train Acc: 0.4951, Loss: 2.9671, Test Acc: 0.5625\n",
      "Epoch: 015, Loss: 2.2133, Train Acc: 0.5236, Loss: 0.9288, Test Acc: 0.4375\n",
      "Epoch: 016, Loss: 0.9356, Train Acc: 0.4826, Loss: 1.6867, Test Acc: 0.5625\n",
      "Epoch: 017, Loss: 2.4746, Train Acc: 0.5174, Loss: 4.6282, Test Acc: 0.5625\n",
      "Epoch: 018, Loss: 5.1511, Train Acc: 0.4951, Loss: 3.6778, Test Acc: 0.5625\n",
      "Epoch: 019, Loss: 4.1863, Train Acc: 0.5201, Loss: 1.2548, Test Acc: 0.4375\n",
      "Epoch: 020, Loss: 4.0960, Train Acc: 0.4826, Loss: 8.1171, Test Acc: 0.4375\n",
      "Epoch: 021, Loss: 4.9288, Train Acc: 0.5049, Loss: 2.7243, Test Acc: 0.4375\n",
      "Epoch: 022, Loss: 2.5349, Train Acc: 0.4986, Loss: 3.0409, Test Acc: 0.4375\n",
      "Epoch: 023, Loss: 2.0284, Train Acc: 0.4764, Loss: 1.1390, Test Acc: 0.4375\n",
      "Epoch: 024, Loss: 1.4688, Train Acc: 0.5062, Loss: 2.2522, Test Acc: 0.5625\n",
      "Epoch: 025, Loss: 2.2829, Train Acc: 0.5174, Loss: 1.7995, Test Acc: 0.5625\n",
      "Epoch: 026, Loss: 2.1477, Train Acc: 0.5049, Loss: 2.1060, Test Acc: 0.4375\n",
      "Epoch: 027, Loss: 1.8957, Train Acc: 0.4951, Loss: 2.6936, Test Acc: 0.4375\n",
      "Epoch: 028, Loss: 1.9718, Train Acc: 0.4889, Loss: 1.1978, Test Acc: 0.5625\n",
      "Epoch: 029, Loss: 1.8856, Train Acc: 0.5174, Loss: 1.8293, Test Acc: 0.5625\n",
      "Epoch: 030, Loss: 1.7433, Train Acc: 0.5174, Loss: 1.6443, Test Acc: 0.5000\n",
      "Epoch: 031, Loss: 1.2749, Train Acc: 0.4826, Loss: 0.6796, Test Acc: 0.5625\n",
      "Epoch: 032, Loss: 1.1387, Train Acc: 0.4986, Loss: 1.1590, Test Acc: 0.4375\n",
      "Epoch: 033, Loss: 1.0688, Train Acc: 0.4639, Loss: 0.7752, Test Acc: 0.5625\n",
      "Epoch: 034, Loss: 1.1389, Train Acc: 0.4986, Loss: 1.8225, Test Acc: 0.4375\n",
      "Epoch: 035, Loss: 0.9803, Train Acc: 0.5062, Loss: 0.7842, Test Acc: 0.5625\n",
      "Epoch: 036, Loss: 1.0651, Train Acc: 0.5139, Loss: 0.6791, Test Acc: 0.5625\n",
      "Epoch: 037, Loss: 1.0639, Train Acc: 0.4701, Loss: 0.6623, Test Acc: 0.5625\n",
      "Epoch: 038, Loss: 1.0197, Train Acc: 0.4701, Loss: 1.1382, Test Acc: 0.4375\n",
      "Epoch: 039, Loss: 1.0834, Train Acc: 0.5111, Loss: 0.6757, Test Acc: 0.6250\n",
      "Epoch: 040, Loss: 0.8426, Train Acc: 0.4951, Loss: 1.1553, Test Acc: 0.5625\n",
      "Epoch: 041, Loss: 1.2234, Train Acc: 0.5174, Loss: 1.3810, Test Acc: 0.4375\n",
      "Epoch: 042, Loss: 1.0962, Train Acc: 0.4639, Loss: 0.8944, Test Acc: 0.5625\n",
      "Epoch: 043, Loss: 1.1706, Train Acc: 0.5222, Loss: 2.1031, Test Acc: 0.4375\n",
      "Epoch: 044, Loss: 1.0567, Train Acc: 0.5111, Loss: 0.6513, Test Acc: 0.5625\n",
      "Epoch: 045, Loss: 0.8928, Train Acc: 0.4889, Loss: 1.0005, Test Acc: 0.5625\n",
      "Epoch: 046, Loss: 1.1603, Train Acc: 0.5285, Loss: 1.1193, Test Acc: 0.4375\n",
      "Epoch: 047, Loss: 1.0366, Train Acc: 0.4576, Loss: 1.0449, Test Acc: 0.5625\n",
      "Epoch: 048, Loss: 1.2016, Train Acc: 0.5049, Loss: 2.8183, Test Acc: 0.4375\n",
      "Epoch: 049, Loss: 2.2396, Train Acc: 0.4764, Loss: 2.2014, Test Acc: 0.4375\n",
      "Epoch: 050, Loss: 1.9254, Train Acc: 0.4951, Loss: 1.8109, Test Acc: 0.5625\n",
      "Epoch: 051, Loss: 2.0362, Train Acc: 0.5111, Loss: 1.8545, Test Acc: 0.5625\n",
      "Epoch: 052, Loss: 1.8330, Train Acc: 0.5174, Loss: 2.3521, Test Acc: 0.4375\n",
      "Epoch: 053, Loss: 1.9225, Train Acc: 0.4951, Loss: 1.8532, Test Acc: 0.5000\n",
      "Epoch: 054, Loss: 1.8025, Train Acc: 0.4826, Loss: 1.4465, Test Acc: 0.5625\n",
      "Epoch: 055, Loss: 1.8052, Train Acc: 0.5111, Loss: 1.2686, Test Acc: 0.5625\n",
      "Epoch: 056, Loss: 2.0413, Train Acc: 0.5049, Loss: 1.3004, Test Acc: 0.4375\n",
      "Epoch: 057, Loss: 1.3707, Train Acc: 0.4951, Loss: 1.0376, Test Acc: 0.5000\n",
      "Epoch: 058, Loss: 1.8120, Train Acc: 0.4951, Loss: 1.4214, Test Acc: 0.5625\n",
      "Epoch: 059, Loss: 1.5839, Train Acc: 0.5111, Loss: 0.9566, Test Acc: 0.5625\n",
      "Epoch: 060, Loss: 2.0543, Train Acc: 0.5049, Loss: 0.7912, Test Acc: 0.4375\n",
      "Epoch: 061, Loss: 1.2537, Train Acc: 0.5049, Loss: 2.5947, Test Acc: 0.4375\n",
      "Epoch: 062, Loss: 1.8894, Train Acc: 0.4951, Loss: 1.7009, Test Acc: 0.4375\n",
      "Epoch: 063, Loss: 1.5175, Train Acc: 0.4951, Loss: 1.7774, Test Acc: 0.5625\n",
      "Epoch: 064, Loss: 1.6108, Train Acc: 0.5174, Loss: 0.8419, Test Acc: 0.5625\n",
      "Epoch: 065, Loss: 1.8278, Train Acc: 0.5049, Loss: 0.8933, Test Acc: 0.4375\n",
      "Epoch: 066, Loss: 1.1486, Train Acc: 0.4951, Loss: 0.7150, Test Acc: 0.5000\n",
      "Epoch: 067, Loss: 1.0024, Train Acc: 0.4986, Loss: 0.9031, Test Acc: 0.5625\n",
      "Epoch: 068, Loss: 1.0343, Train Acc: 0.5222, Loss: 1.8077, Test Acc: 0.4375\n",
      "Epoch: 069, Loss: 1.0018, Train Acc: 0.5062, Loss: 1.1212, Test Acc: 0.5625\n",
      "Epoch: 070, Loss: 1.0646, Train Acc: 0.5111, Loss: 1.1217, Test Acc: 0.4375\n",
      "Epoch: 071, Loss: 0.8378, Train Acc: 0.4563, Loss: 0.7679, Test Acc: 0.5625\n",
      "Epoch: 072, Loss: 0.9208, Train Acc: 0.5160, Loss: 1.0070, Test Acc: 0.4375\n",
      "Epoch: 073, Loss: 0.8663, Train Acc: 0.5111, Loss: 0.6637, Test Acc: 0.5625\n",
      "Epoch: 074, Loss: 0.8744, Train Acc: 0.4951, Loss: 0.6538, Test Acc: 0.4375\n",
      "Epoch: 075, Loss: 0.8980, Train Acc: 0.5035, Loss: 0.7097, Test Acc: 0.5000\n",
      "Epoch: 076, Loss: 0.8691, Train Acc: 0.4576, Loss: 0.6552, Test Acc: 0.5625\n",
      "Epoch: 077, Loss: 0.8899, Train Acc: 0.4750, Loss: 0.7261, Test Acc: 0.5000\n",
      "Epoch: 078, Loss: 0.8686, Train Acc: 0.4625, Loss: 0.6621, Test Acc: 0.6250\n",
      "Epoch: 079, Loss: 0.8872, Train Acc: 0.4875, Loss: 0.7659, Test Acc: 0.5000\n",
      "Epoch: 080, Loss: 0.8562, Train Acc: 0.4750, Loss: 0.6712, Test Acc: 0.5625\n",
      "Epoch: 081, Loss: 0.8774, Train Acc: 0.4764, Loss: 0.7141, Test Acc: 0.5625\n",
      "Epoch: 082, Loss: 0.8714, Train Acc: 0.4812, Loss: 0.6635, Test Acc: 0.6250\n",
      "Epoch: 083, Loss: 0.8801, Train Acc: 0.4875, Loss: 0.7168, Test Acc: 0.5625\n",
      "Epoch: 084, Loss: 0.8748, Train Acc: 0.4750, Loss: 0.6634, Test Acc: 0.6250\n",
      "Epoch: 085, Loss: 0.8860, Train Acc: 0.4938, Loss: 0.6901, Test Acc: 0.5625\n",
      "Epoch: 086, Loss: 0.8802, Train Acc: 0.4625, Loss: 0.6718, Test Acc: 0.5625\n",
      "Epoch: 087, Loss: 0.8879, Train Acc: 0.4889, Loss: 0.7268, Test Acc: 0.5000\n",
      "Epoch: 088, Loss: 0.8773, Train Acc: 0.4750, Loss: 0.6717, Test Acc: 0.5625\n",
      "Epoch: 089, Loss: 0.8863, Train Acc: 0.4701, Loss: 0.7155, Test Acc: 0.5625\n",
      "Epoch: 090, Loss: 0.8839, Train Acc: 0.4750, Loss: 0.6680, Test Acc: 0.6250\n",
      "Epoch: 091, Loss: 0.8887, Train Acc: 0.4826, Loss: 0.7207, Test Acc: 0.5625\n",
      "Epoch: 092, Loss: 0.8832, Train Acc: 0.4750, Loss: 0.6698, Test Acc: 0.5625\n",
      "Epoch: 093, Loss: 0.8896, Train Acc: 0.4826, Loss: 0.7098, Test Acc: 0.5625\n",
      "Epoch: 094, Loss: 0.8843, Train Acc: 0.4750, Loss: 0.6539, Test Acc: 0.5625\n",
      "Epoch: 095, Loss: 0.8933, Train Acc: 0.4826, Loss: 0.7271, Test Acc: 0.5000\n",
      "Epoch: 096, Loss: 0.8826, Train Acc: 0.4750, Loss: 0.6749, Test Acc: 0.5625\n",
      "Epoch: 097, Loss: 0.8892, Train Acc: 0.4889, Loss: 0.7028, Test Acc: 0.5000\n",
      "Epoch: 098, Loss: 0.8936, Train Acc: 0.4875, Loss: 0.6669, Test Acc: 0.5625\n",
      "Epoch: 099, Loss: 0.8945, Train Acc: 0.4812, Loss: 0.6718, Test Acc: 0.6250\n",
      "Epoch: 100, Loss: 0.8992, Train Acc: 0.4688, Loss: 0.6704, Test Acc: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN accuracy: 0.5263158082962036\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 129 131 132 145 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      " 160 161] TEST: [127 128 130 133 134 135 136 137 138 139 140 141 142 143 144 146]\n",
      "146\n",
      "16\n",
      "Epoch: 000, Loss: 80.1476, Train Acc: 0.5549, Loss: 85.8511, Test Acc: 0.5625\n",
      "Epoch: 001, Loss: 33.7405, Train Acc: 0.5250, Loss: 27.9326, Test Acc: 0.4375\n",
      "Epoch: 002, Loss: 14.8229, Train Acc: 0.4875, Loss: 13.1761, Test Acc: 0.4375\n",
      "Epoch: 003, Loss: 27.7871, Train Acc: 0.5062, Loss: 48.6380, Test Acc: 0.4375\n",
      "Epoch: 004, Loss: 34.9194, Train Acc: 0.5000, Loss: 4.0276, Test Acc: 0.5000\n",
      "Epoch: 005, Loss: 23.5725, Train Acc: 0.5236, Loss: 27.3855, Test Acc: 0.5625\n",
      "Epoch: 006, Loss: 19.8297, Train Acc: 0.5361, Loss: 11.1494, Test Acc: 0.5625\n",
      "Epoch: 007, Loss: 10.1592, Train Acc: 0.5174, Loss: 5.0309, Test Acc: 0.5000\n",
      "Epoch: 008, Loss: 6.7488, Train Acc: 0.5312, Loss: 4.2318, Test Acc: 0.4375\n",
      "Epoch: 009, Loss: 5.2808, Train Acc: 0.5312, Loss: 7.3503, Test Acc: 0.4375\n",
      "Epoch: 010, Loss: 4.0191, Train Acc: 0.5250, Loss: 3.7338, Test Acc: 0.4375\n",
      "Epoch: 011, Loss: 5.2241, Train Acc: 0.5125, Loss: 1.6082, Test Acc: 0.5625\n",
      "Epoch: 012, Loss: 4.2632, Train Acc: 0.5000, Loss: 8.0726, Test Acc: 0.5625\n",
      "Epoch: 013, Loss: 8.1973, Train Acc: 0.4875, Loss: 13.3066, Test Acc: 0.5625\n",
      "Epoch: 014, Loss: 14.2081, Train Acc: 0.5188, Loss: 28.6580, Test Acc: 0.4375\n",
      "Epoch: 015, Loss: 15.6847, Train Acc: 0.4812, Loss: 9.8034, Test Acc: 0.5625\n",
      "Epoch: 016, Loss: 7.4630, Train Acc: 0.5188, Loss: 7.2104, Test Acc: 0.5000\n",
      "Epoch: 017, Loss: 4.8436, Train Acc: 0.5125, Loss: 6.7854, Test Acc: 0.4375\n",
      "Epoch: 018, Loss: 11.4953, Train Acc: 0.5000, Loss: 26.1275, Test Acc: 0.4375\n",
      "Epoch: 019, Loss: 11.8660, Train Acc: 0.4875, Loss: 2.3384, Test Acc: 0.5000\n",
      "Epoch: 020, Loss: 2.3241, Train Acc: 0.5188, Loss: 2.0954, Test Acc: 0.5625\n",
      "Epoch: 021, Loss: 3.3572, Train Acc: 0.5125, Loss: 2.2487, Test Acc: 0.5625\n",
      "Epoch: 022, Loss: 3.2345, Train Acc: 0.5000, Loss: 2.7640, Test Acc: 0.5625\n",
      "Epoch: 023, Loss: 2.5866, Train Acc: 0.4938, Loss: 4.1665, Test Acc: 0.4375\n",
      "Epoch: 024, Loss: 2.8665, Train Acc: 0.5125, Loss: 3.2135, Test Acc: 0.4375\n",
      "Epoch: 025, Loss: 2.6334, Train Acc: 0.5000, Loss: 2.1020, Test Acc: 0.5625\n",
      "Epoch: 026, Loss: 2.6642, Train Acc: 0.5062, Loss: 2.5425, Test Acc: 0.5625\n",
      "Epoch: 027, Loss: 2.3717, Train Acc: 0.5062, Loss: 2.7575, Test Acc: 0.4375\n",
      "Epoch: 028, Loss: 1.3684, Train Acc: 0.5438, Loss: 1.2736, Test Acc: 0.5625\n",
      "Epoch: 029, Loss: 1.3603, Train Acc: 0.5236, Loss: 2.3916, Test Acc: 0.4375\n",
      "Epoch: 030, Loss: 1.1902, Train Acc: 0.5097, Loss: 0.7979, Test Acc: 0.5625\n",
      "Epoch: 031, Loss: 1.2525, Train Acc: 0.4951, Loss: 1.8407, Test Acc: 0.4375\n",
      "Epoch: 032, Loss: 1.5455, Train Acc: 0.4938, Loss: 0.8591, Test Acc: 0.5625\n",
      "Epoch: 033, Loss: 1.1995, Train Acc: 0.5236, Loss: 3.7356, Test Acc: 0.4375\n",
      "Epoch: 034, Loss: 2.7365, Train Acc: 0.5187, Loss: 3.9419, Test Acc: 0.4375\n",
      "Epoch: 035, Loss: 2.2591, Train Acc: 0.4938, Loss: 1.9939, Test Acc: 0.5625\n",
      "Epoch: 036, Loss: 1.9681, Train Acc: 0.5188, Loss: 1.4203, Test Acc: 0.5000\n",
      "Epoch: 037, Loss: 1.3082, Train Acc: 0.5563, Loss: 1.1973, Test Acc: 0.4375\n",
      "Epoch: 038, Loss: 2.2542, Train Acc: 0.5062, Loss: 2.9841, Test Acc: 0.5625\n",
      "Epoch: 039, Loss: 2.5232, Train Acc: 0.5312, Loss: 4.0839, Test Acc: 0.5625\n",
      "Epoch: 040, Loss: 4.7441, Train Acc: 0.5250, Loss: 4.6897, Test Acc: 0.4375\n",
      "Epoch: 041, Loss: 4.4608, Train Acc: 0.5222, Loss: 6.9473, Test Acc: 0.4375\n",
      "Epoch: 042, Loss: 4.1479, Train Acc: 0.5000, Loss: 1.8514, Test Acc: 0.4375\n",
      "Epoch: 043, Loss: 2.6538, Train Acc: 0.5250, Loss: 1.4508, Test Acc: 0.4375\n",
      "Epoch: 044, Loss: 1.9572, Train Acc: 0.5375, Loss: 3.2028, Test Acc: 0.4375\n",
      "Epoch: 045, Loss: 2.1182, Train Acc: 0.5000, Loss: 1.8034, Test Acc: 0.4375\n",
      "Epoch: 046, Loss: 2.0920, Train Acc: 0.4938, Loss: 1.4224, Test Acc: 0.5625\n",
      "Epoch: 047, Loss: 1.9515, Train Acc: 0.4937, Loss: 1.8713, Test Acc: 0.5625\n",
      "Epoch: 048, Loss: 2.0460, Train Acc: 0.5125, Loss: 2.5702, Test Acc: 0.4375\n",
      "Epoch: 049, Loss: 1.6015, Train Acc: 0.5000, Loss: 1.9319, Test Acc: 0.4375\n",
      "Epoch: 050, Loss: 2.0250, Train Acc: 0.5125, Loss: 1.1152, Test Acc: 0.5625\n",
      "Epoch: 051, Loss: 1.7193, Train Acc: 0.5125, Loss: 1.4569, Test Acc: 0.5625\n",
      "Epoch: 052, Loss: 2.2107, Train Acc: 0.5000, Loss: 1.8916, Test Acc: 0.4375\n",
      "Epoch: 053, Loss: 1.4127, Train Acc: 0.5000, Loss: 1.6164, Test Acc: 0.4375\n",
      "Epoch: 054, Loss: 2.2075, Train Acc: 0.5187, Loss: 1.5731, Test Acc: 0.4375\n",
      "Epoch: 055, Loss: 2.7615, Train Acc: 0.4875, Loss: 6.3018, Test Acc: 0.4375\n",
      "Epoch: 056, Loss: 4.3939, Train Acc: 0.5125, Loss: 2.0943, Test Acc: 0.4375\n",
      "Epoch: 057, Loss: 2.9270, Train Acc: 0.4938, Loss: 1.4119, Test Acc: 0.5625\n",
      "Epoch: 058, Loss: 3.3785, Train Acc: 0.4938, Loss: 5.1787, Test Acc: 0.5625\n",
      "Epoch: 059, Loss: 3.0744, Train Acc: 0.5125, Loss: 2.2017, Test Acc: 0.5625\n",
      "Epoch: 060, Loss: 1.7794, Train Acc: 0.5250, Loss: 1.2943, Test Acc: 0.5000\n",
      "Epoch: 061, Loss: 1.1492, Train Acc: 0.5187, Loss: 1.5946, Test Acc: 0.4375\n",
      "Epoch: 062, Loss: 0.8953, Train Acc: 0.5000, Loss: 0.6691, Test Acc: 0.6875\n",
      "Epoch: 063, Loss: 0.7899, Train Acc: 0.4688, Loss: 0.7588, Test Acc: 0.4375\n",
      "Epoch: 064, Loss: 0.8825, Train Acc: 0.5062, Loss: 1.3756, Test Acc: 0.4375\n",
      "Epoch: 065, Loss: 1.0036, Train Acc: 0.5000, Loss: 0.8915, Test Acc: 0.3125\n",
      "Epoch: 066, Loss: 0.7627, Train Acc: 0.5375, Loss: 0.7443, Test Acc: 0.5625\n",
      "Epoch: 067, Loss: 0.8980, Train Acc: 0.5222, Loss: 1.2096, Test Acc: 0.4375\n",
      "Epoch: 068, Loss: 1.0696, Train Acc: 0.5000, Loss: 0.7367, Test Acc: 0.5000\n",
      "Epoch: 069, Loss: 0.6939, Train Acc: 0.5437, Loss: 0.7907, Test Acc: 0.5625\n",
      "Epoch: 070, Loss: 0.9059, Train Acc: 0.5299, Loss: 1.6289, Test Acc: 0.4375\n",
      "Epoch: 071, Loss: 1.1085, Train Acc: 0.4938, Loss: 0.7504, Test Acc: 0.4375\n",
      "Epoch: 072, Loss: 0.7004, Train Acc: 0.5563, Loss: 0.8267, Test Acc: 0.5625\n",
      "Epoch: 073, Loss: 0.9149, Train Acc: 0.5410, Loss: 1.6429, Test Acc: 0.4375\n",
      "Epoch: 074, Loss: 1.0914, Train Acc: 0.5000, Loss: 0.7170, Test Acc: 0.5000\n",
      "Epoch: 075, Loss: 0.7158, Train Acc: 0.5625, Loss: 0.8038, Test Acc: 0.5625\n",
      "Epoch: 076, Loss: 0.9249, Train Acc: 0.5174, Loss: 1.6413, Test Acc: 0.4375\n",
      "Epoch: 077, Loss: 1.0866, Train Acc: 0.5000, Loss: 0.7308, Test Acc: 0.4375\n",
      "Epoch: 078, Loss: 0.7464, Train Acc: 0.5437, Loss: 0.7489, Test Acc: 0.5625\n",
      "Epoch: 079, Loss: 0.9176, Train Acc: 0.5000, Loss: 1.3880, Test Acc: 0.4375\n",
      "Epoch: 080, Loss: 1.0907, Train Acc: 0.5062, Loss: 0.7084, Test Acc: 0.5000\n",
      "Epoch: 081, Loss: 0.7207, Train Acc: 0.5500, Loss: 0.7602, Test Acc: 0.5625\n",
      "Epoch: 082, Loss: 0.9272, Train Acc: 0.5049, Loss: 1.5198, Test Acc: 0.4375\n",
      "Epoch: 083, Loss: 1.1024, Train Acc: 0.5187, Loss: 0.7157, Test Acc: 0.5000\n",
      "Epoch: 084, Loss: 0.7302, Train Acc: 0.5500, Loss: 0.7345, Test Acc: 0.5625\n",
      "Epoch: 085, Loss: 0.9237, Train Acc: 0.5160, Loss: 1.5180, Test Acc: 0.4375\n",
      "Epoch: 086, Loss: 1.0983, Train Acc: 0.5062, Loss: 0.7116, Test Acc: 0.5000\n",
      "Epoch: 087, Loss: 0.7124, Train Acc: 0.5687, Loss: 0.7759, Test Acc: 0.5625\n",
      "Epoch: 088, Loss: 0.9245, Train Acc: 0.4889, Loss: 1.6513, Test Acc: 0.4375\n",
      "Epoch: 089, Loss: 1.1272, Train Acc: 0.5125, Loss: 0.7278, Test Acc: 0.5000\n",
      "Epoch: 090, Loss: 0.7172, Train Acc: 0.5750, Loss: 0.7893, Test Acc: 0.5625\n",
      "Epoch: 091, Loss: 0.9294, Train Acc: 0.5222, Loss: 1.5555, Test Acc: 0.4375\n",
      "Epoch: 092, Loss: 1.1340, Train Acc: 0.5000, Loss: 0.7228, Test Acc: 0.5000\n",
      "Epoch: 093, Loss: 0.6773, Train Acc: 0.5861, Loss: 0.7935, Test Acc: 0.5625\n",
      "Epoch: 094, Loss: 0.8976, Train Acc: 0.5535, Loss: 1.5762, Test Acc: 0.4375\n",
      "Epoch: 095, Loss: 1.1771, Train Acc: 0.5000, Loss: 0.7778, Test Acc: 0.5625\n",
      "Epoch: 096, Loss: 0.8077, Train Acc: 0.5312, Loss: 0.7634, Test Acc: 0.5625\n",
      "Epoch: 097, Loss: 1.0097, Train Acc: 0.5000, Loss: 1.7774, Test Acc: 0.4375\n",
      "Epoch: 098, Loss: 0.9388, Train Acc: 0.5188, Loss: 0.9027, Test Acc: 0.4375\n",
      "Epoch: 099, Loss: 0.8801, Train Acc: 0.4861, Loss: 0.7061, Test Acc: 0.5000\n",
      "Epoch: 100, Loss: 0.9130, Train Acc: 0.5174, Loss: 1.1904, Test Acc: 0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN accuracy: 0.4736842215061188\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 146] TEST: [145 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161]\n",
      "146\n",
      "16\n",
      "Epoch: 000, Loss: 95.8048, Train Acc: 0.5049, Loss: 13.2716, Test Acc: 0.4375\n",
      "Epoch: 001, Loss: 109.9181, Train Acc: 0.5174, Loss: 221.9301, Test Acc: 0.4375\n",
      "Epoch: 002, Loss: 140.9634, Train Acc: 0.4701, Loss: 160.2074, Test Acc: 0.5625\n",
      "Epoch: 003, Loss: 90.0237, Train Acc: 0.5299, Loss: 95.4039, Test Acc: 0.4375\n",
      "Epoch: 004, Loss: 56.2354, Train Acc: 0.4701, Loss: 54.0225, Test Acc: 0.5625\n",
      "Epoch: 005, Loss: 40.7566, Train Acc: 0.5236, Loss: 63.4509, Test Acc: 0.4375\n",
      "Epoch: 006, Loss: 32.2064, Train Acc: 0.4951, Loss: 19.2692, Test Acc: 0.4375\n",
      "Epoch: 007, Loss: 24.4475, Train Acc: 0.4701, Loss: 23.3355, Test Acc: 0.5625\n",
      "Epoch: 008, Loss: 15.0950, Train Acc: 0.4889, Loss: 16.7137, Test Acc: 0.5625\n",
      "Epoch: 009, Loss: 12.2389, Train Acc: 0.4764, Loss: 12.3973, Test Acc: 0.5625\n",
      "Epoch: 010, Loss: 15.1651, Train Acc: 0.5049, Loss: 10.6915, Test Acc: 0.5625\n",
      "Epoch: 011, Loss: 12.4119, Train Acc: 0.5299, Loss: 19.9820, Test Acc: 0.4375\n",
      "Epoch: 012, Loss: 16.8767, Train Acc: 0.4951, Loss: 4.9184, Test Acc: 0.5625\n",
      "Epoch: 013, Loss: 18.1759, Train Acc: 0.5049, Loss: 10.2623, Test Acc: 0.5625\n",
      "Epoch: 014, Loss: 13.5910, Train Acc: 0.5424, Loss: 14.0641, Test Acc: 0.4375\n",
      "Epoch: 015, Loss: 7.5141, Train Acc: 0.4951, Loss: 13.4908, Test Acc: 0.4375\n",
      "Epoch: 016, Loss: 11.3939, Train Acc: 0.4701, Loss: 5.4602, Test Acc: 0.5625\n",
      "Epoch: 017, Loss: 7.3087, Train Acc: 0.4826, Loss: 13.4425, Test Acc: 0.5625\n",
      "Epoch: 018, Loss: 6.9964, Train Acc: 0.4826, Loss: 0.9638, Test Acc: 0.5625\n",
      "Epoch: 019, Loss: 2.6010, Train Acc: 0.4826, Loss: 1.0942, Test Acc: 0.3750\n",
      "Epoch: 020, Loss: 1.7581, Train Acc: 0.4889, Loss: 0.8311, Test Acc: 0.4375\n",
      "Epoch: 021, Loss: 2.1438, Train Acc: 0.5486, Loss: 2.7299, Test Acc: 0.4375\n",
      "Epoch: 022, Loss: 6.3812, Train Acc: 0.5174, Loss: 14.7011, Test Acc: 0.4375\n",
      "Epoch: 023, Loss: 8.1167, Train Acc: 0.4951, Loss: 7.9069, Test Acc: 0.4375\n",
      "Epoch: 024, Loss: 9.7820, Train Acc: 0.4701, Loss: 10.9024, Test Acc: 0.5625\n",
      "Epoch: 025, Loss: 14.0682, Train Acc: 0.5049, Loss: 9.6384, Test Acc: 0.4375\n",
      "Epoch: 026, Loss: 11.7875, Train Acc: 0.4951, Loss: 5.1269, Test Acc: 0.4375\n",
      "Epoch: 027, Loss: 8.2362, Train Acc: 0.4701, Loss: 4.6909, Test Acc: 0.5625\n",
      "Epoch: 028, Loss: 8.0018, Train Acc: 0.4826, Loss: 9.4027, Test Acc: 0.5625\n",
      "Epoch: 029, Loss: 5.5025, Train Acc: 0.4826, Loss: 0.8693, Test Acc: 0.4375\n",
      "Epoch: 030, Loss: 1.1574, Train Acc: 0.5833, Loss: 1.6808, Test Acc: 0.5625\n",
      "Epoch: 031, Loss: 1.3940, Train Acc: 0.5424, Loss: 1.9161, Test Acc: 0.5625\n",
      "Epoch: 032, Loss: 2.4533, Train Acc: 0.5014, Loss: 5.8155, Test Acc: 0.4375\n",
      "Epoch: 033, Loss: 3.2795, Train Acc: 0.5174, Loss: 1.0184, Test Acc: 0.3750\n",
      "Epoch: 034, Loss: 3.6001, Train Acc: 0.4986, Loss: 1.7096, Test Acc: 0.5625\n",
      "Epoch: 035, Loss: 3.6949, Train Acc: 0.4826, Loss: 6.0982, Test Acc: 0.5625\n",
      "Epoch: 036, Loss: 7.2369, Train Acc: 0.5049, Loss: 4.7216, Test Acc: 0.5625\n",
      "Epoch: 037, Loss: 5.0725, Train Acc: 0.5299, Loss: 1.8223, Test Acc: 0.5625\n",
      "Epoch: 038, Loss: 3.6592, Train Acc: 0.4826, Loss: 5.9055, Test Acc: 0.5625\n",
      "Epoch: 039, Loss: 7.0684, Train Acc: 0.5049, Loss: 4.2789, Test Acc: 0.5625\n",
      "Epoch: 040, Loss: 4.9003, Train Acc: 0.5299, Loss: 1.3131, Test Acc: 0.5625\n",
      "Epoch: 041, Loss: 3.3514, Train Acc: 0.4826, Loss: 6.5062, Test Acc: 0.5625\n",
      "Epoch: 042, Loss: 7.3451, Train Acc: 0.5049, Loss: 2.9350, Test Acc: 0.5625\n",
      "Epoch: 043, Loss: 4.7671, Train Acc: 0.5299, Loss: 1.8707, Test Acc: 0.4375\n",
      "Epoch: 044, Loss: 6.1767, Train Acc: 0.4889, Loss: 12.8398, Test Acc: 0.4375\n",
      "Epoch: 045, Loss: 11.9955, Train Acc: 0.4576, Loss: 12.5501, Test Acc: 0.5625\n",
      "Epoch: 046, Loss: 8.4461, Train Acc: 0.5299, Loss: 3.7489, Test Acc: 0.4375\n",
      "Epoch: 047, Loss: 4.4969, Train Acc: 0.5174, Loss: 8.7264, Test Acc: 0.4375\n",
      "Epoch: 048, Loss: 6.3662, Train Acc: 0.4951, Loss: 2.3638, Test Acc: 0.3750\n",
      "Epoch: 049, Loss: 4.8474, Train Acc: 0.4764, Loss: 4.0802, Test Acc: 0.5625\n",
      "Epoch: 050, Loss: 5.5121, Train Acc: 0.4826, Loss: 7.8033, Test Acc: 0.5625\n",
      "Epoch: 051, Loss: 4.5883, Train Acc: 0.4826, Loss: 2.5953, Test Acc: 0.5625\n",
      "Epoch: 052, Loss: 2.2493, Train Acc: 0.4951, Loss: 5.9323, Test Acc: 0.4375\n",
      "Epoch: 053, Loss: 3.4432, Train Acc: 0.5014, Loss: 7.2857, Test Acc: 0.4375\n",
      "Epoch: 054, Loss: 7.9940, Train Acc: 0.4701, Loss: 9.4786, Test Acc: 0.5625\n",
      "Epoch: 055, Loss: 10.1671, Train Acc: 0.5049, Loss: 6.0341, Test Acc: 0.4375\n",
      "Epoch: 056, Loss: 8.6929, Train Acc: 0.4951, Loss: 3.8742, Test Acc: 0.4375\n",
      "Epoch: 057, Loss: 4.8759, Train Acc: 0.4701, Loss: 1.1141, Test Acc: 0.5625\n",
      "Epoch: 058, Loss: 4.7712, Train Acc: 0.4826, Loss: 9.0103, Test Acc: 0.5625\n",
      "Epoch: 059, Loss: 6.2167, Train Acc: 0.5299, Loss: 3.0190, Test Acc: 0.4375\n",
      "Epoch: 060, Loss: 5.1116, Train Acc: 0.4951, Loss: 9.5785, Test Acc: 0.4375\n",
      "Epoch: 061, Loss: 9.0954, Train Acc: 0.4576, Loss: 11.9059, Test Acc: 0.5625\n",
      "Epoch: 062, Loss: 8.3773, Train Acc: 0.5424, Loss: 12.1136, Test Acc: 0.4375\n",
      "Epoch: 063, Loss: 9.4095, Train Acc: 0.4701, Loss: 12.8151, Test Acc: 0.5625\n",
      "Epoch: 064, Loss: 9.6582, Train Acc: 0.5299, Loss: 16.3389, Test Acc: 0.4375\n",
      "Epoch: 065, Loss: 11.3303, Train Acc: 0.4764, Loss: 16.2426, Test Acc: 0.5625\n",
      "Epoch: 066, Loss: 9.6139, Train Acc: 0.5299, Loss: 8.4731, Test Acc: 0.4375\n",
      "Epoch: 067, Loss: 5.6488, Train Acc: 0.4951, Loss: 1.0305, Test Acc: 0.4375\n",
      "Epoch: 068, Loss: 3.4526, Train Acc: 0.5014, Loss: 5.0240, Test Acc: 0.5625\n",
      "Epoch: 069, Loss: 4.7511, Train Acc: 0.5049, Loss: 2.4206, Test Acc: 0.5625\n",
      "Epoch: 070, Loss: 3.1893, Train Acc: 0.5299, Loss: 1.0993, Test Acc: 0.5000\n",
      "Epoch: 071, Loss: 3.1884, Train Acc: 0.5299, Loss: 7.0275, Test Acc: 0.4375\n",
      "Epoch: 072, Loss: 3.9508, Train Acc: 0.4951, Loss: 3.5131, Test Acc: 0.4375\n",
      "Epoch: 073, Loss: 4.3567, Train Acc: 0.4701, Loss: 4.2481, Test Acc: 0.5625\n",
      "Epoch: 074, Loss: 3.2015, Train Acc: 0.4826, Loss: 2.8395, Test Acc: 0.5625\n",
      "Epoch: 075, Loss: 2.5413, Train Acc: 0.5014, Loss: 2.8025, Test Acc: 0.5625\n",
      "Epoch: 076, Loss: 2.4343, Train Acc: 0.4701, Loss: 3.1770, Test Acc: 0.5625\n",
      "Epoch: 077, Loss: 2.6913, Train Acc: 0.5014, Loss: 2.7000, Test Acc: 0.5625\n",
      "Epoch: 078, Loss: 2.4517, Train Acc: 0.4951, Loss: 3.4427, Test Acc: 0.5625\n",
      "Epoch: 079, Loss: 3.0920, Train Acc: 0.5049, Loss: 4.3985, Test Acc: 0.5625\n",
      "Epoch: 080, Loss: 3.7876, Train Acc: 0.5299, Loss: 1.1701, Test Acc: 0.3750\n",
      "Epoch: 081, Loss: 2.6221, Train Acc: 0.5236, Loss: 5.8207, Test Acc: 0.4375\n",
      "Epoch: 082, Loss: 4.6705, Train Acc: 0.4951, Loss: 0.8273, Test Acc: 0.5625\n",
      "Epoch: 083, Loss: 2.6809, Train Acc: 0.4951, Loss: 1.5573, Test Acc: 0.5625\n",
      "Epoch: 084, Loss: 2.3130, Train Acc: 0.5139, Loss: 0.8810, Test Acc: 0.5625\n",
      "Epoch: 085, Loss: 2.2648, Train Acc: 0.4951, Loss: 0.9110, Test Acc: 0.5625\n",
      "Epoch: 086, Loss: 1.8514, Train Acc: 0.4889, Loss: 1.8653, Test Acc: 0.4375\n",
      "Epoch: 087, Loss: 1.3604, Train Acc: 0.5458, Loss: 1.7074, Test Acc: 0.4375\n",
      "Epoch: 088, Loss: 0.9818, Train Acc: 0.5014, Loss: 0.9598, Test Acc: 0.5000\n",
      "Epoch: 089, Loss: 0.8708, Train Acc: 0.5208, Loss: 1.2387, Test Acc: 0.3750\n",
      "Epoch: 090, Loss: 0.9431, Train Acc: 0.4764, Loss: 1.3559, Test Acc: 0.4375\n",
      "Epoch: 091, Loss: 0.8310, Train Acc: 0.5347, Loss: 0.9241, Test Acc: 0.3750\n",
      "Epoch: 092, Loss: 0.8892, Train Acc: 0.5083, Loss: 1.0893, Test Acc: 0.4375\n",
      "Epoch: 093, Loss: 0.9414, Train Acc: 0.4764, Loss: 1.4459, Test Acc: 0.4375\n",
      "Epoch: 094, Loss: 0.8560, Train Acc: 0.5299, Loss: 1.0092, Test Acc: 0.3750\n",
      "Epoch: 095, Loss: 0.8835, Train Acc: 0.5319, Loss: 0.8869, Test Acc: 0.3750\n",
      "Epoch: 096, Loss: 0.8981, Train Acc: 0.5035, Loss: 1.0646, Test Acc: 0.4375\n",
      "Epoch: 097, Loss: 0.9186, Train Acc: 0.4826, Loss: 1.4332, Test Acc: 0.4375\n",
      "Epoch: 098, Loss: 0.8901, Train Acc: 0.4826, Loss: 1.2271, Test Acc: 0.4375\n",
      "Epoch: 099, Loss: 0.8862, Train Acc: 0.4986, Loss: 1.1193, Test Acc: 0.4375\n",
      "Epoch: 100, Loss: 0.8904, Train Acc: 0.4986, Loss: 1.1291, Test Acc: 0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN accuracy: 0.42105263471603394\n",
      "Val accuracy: 0.5058823585510254\n",
      "Test accuracy: 0.5052631676197052\n",
      "Val stv: 0.0412420727957663\n",
      "Test stv: 0.06315789719422732\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_avg = []\n",
    "test_avg = []\n",
    "for train_index, val_index in kf.split(train_data, Y_train):\n",
    "    train_dataset=[]\n",
    "    val_dataset=[]\n",
    "    print(\"TRAIN: \", train_index, \"TEST:\", val_index)\n",
    "    for i in train_index:\n",
    "        train_dataset.append(train_data[i])\n",
    "    for i in val_index:\n",
    "        val_dataset.append(train_data[i])\n",
    "\n",
    "    print(len(train_dataset))\n",
    "    print(len(val_dataset))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = GIN(dim_h=32)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.6)\n",
    "    # optimizer = torch.optim.Adadelta(model.parameters(), lr=0.7)\n",
    "    train_epoch=[]\n",
    "    val_epoch=[]\n",
    "    train_loss_=[]\n",
    "    val_loss_=[]\n",
    "    epochs = 100\n",
    "    train_acc=0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs+1):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc = validation(model, val_loader, criterion)\n",
    "\n",
    "        train_loss = train_loss.detach().numpy()\n",
    "        train_loss_.append(train_loss)\n",
    "        val_loss_.append(val_loss.detach().numpy())\n",
    "        train_epoch.append(train_acc)\n",
    "        val_epoch.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, '\n",
    "            f'Train Acc: {train_acc:.4f}, Loss: {val_loss:.4f}, Test Acc: {val_acc:.4f}')\n",
    "\n",
    "    test_acc = test(model, test_data)\n",
    "    print(\"GIN accuracy: \" + str(test_acc))\n",
    "\n",
    "    plt.plot(train_epoch, color=\"red\")\n",
    "    plt.plot(val_epoch, color=\"blue\")\n",
    "    plt.plot(train_loss_, color=\"orange\")\n",
    "    plt.plot(val_loss_, color=\"purple\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    val_avg.append(val_acc)\n",
    "    test_avg.append(test_acc)\n",
    "\n",
    "print('Val accuracy: '+ str(np.array(val_avg).mean()))\n",
    "print('Test accuracy: '+ str(np.array(test_avg).mean()))\n",
    "\n",
    "print('Val stv: '+ str(np.array(val_avg).std()))\n",
    "print('Test stv: '+ str(np.array(test_avg).std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb15f1e0f376981e7b6e1fc44ae8b8146823f10f258bcd6e448b0230b889fc06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
