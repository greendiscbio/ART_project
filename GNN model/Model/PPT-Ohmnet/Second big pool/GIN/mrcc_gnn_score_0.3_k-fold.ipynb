{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cpu.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gene matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APAF1</th>\n",
       "      <th>ARID1A</th>\n",
       "      <th>ATM</th>\n",
       "      <th>BAP1</th>\n",
       "      <th>EPAS1</th>\n",
       "      <th>ERBB2</th>\n",
       "      <th>FLT1</th>\n",
       "      <th>FLT4</th>\n",
       "      <th>GSTP1</th>\n",
       "      <th>HSPB1</th>\n",
       "      <th>...</th>\n",
       "      <th>RNF139</th>\n",
       "      <th>SETD2</th>\n",
       "      <th>SLC2A1</th>\n",
       "      <th>SOD2</th>\n",
       "      <th>TGM2</th>\n",
       "      <th>TP53</th>\n",
       "      <th>TSC1</th>\n",
       "      <th>TSC2</th>\n",
       "      <th>VEGFA</th>\n",
       "      <th>VHL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.668769</td>\n",
       "      <td>33.848026</td>\n",
       "      <td>35.942429</td>\n",
       "      <td>33.677294</td>\n",
       "      <td>37.95811</td>\n",
       "      <td>35.32243</td>\n",
       "      <td>33.69326</td>\n",
       "      <td>30.79376</td>\n",
       "      <td>36.48088</td>\n",
       "      <td>38.25591</td>\n",
       "      <td>...</td>\n",
       "      <td>32.46554</td>\n",
       "      <td>32.58565</td>\n",
       "      <td>33.38586</td>\n",
       "      <td>38.67433</td>\n",
       "      <td>38.50142</td>\n",
       "      <td>33.83518</td>\n",
       "      <td>32.93402</td>\n",
       "      <td>34.93520</td>\n",
       "      <td>37.79678</td>\n",
       "      <td>32.30615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.337493</td>\n",
       "      <td>33.843513</td>\n",
       "      <td>35.988225</td>\n",
       "      <td>32.643149</td>\n",
       "      <td>38.83281</td>\n",
       "      <td>33.71706</td>\n",
       "      <td>35.56873</td>\n",
       "      <td>33.38444</td>\n",
       "      <td>36.21403</td>\n",
       "      <td>37.41814</td>\n",
       "      <td>...</td>\n",
       "      <td>32.27190</td>\n",
       "      <td>33.19915</td>\n",
       "      <td>33.69538</td>\n",
       "      <td>38.64559</td>\n",
       "      <td>34.33752</td>\n",
       "      <td>34.44810</td>\n",
       "      <td>33.16630</td>\n",
       "      <td>35.08304</td>\n",
       "      <td>40.09193</td>\n",
       "      <td>32.19988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.818198</td>\n",
       "      <td>33.516005</td>\n",
       "      <td>36.193587</td>\n",
       "      <td>32.368866</td>\n",
       "      <td>37.19345</td>\n",
       "      <td>33.38917</td>\n",
       "      <td>34.21918</td>\n",
       "      <td>33.34670</td>\n",
       "      <td>35.34069</td>\n",
       "      <td>37.94992</td>\n",
       "      <td>...</td>\n",
       "      <td>32.55514</td>\n",
       "      <td>32.84628</td>\n",
       "      <td>36.23588</td>\n",
       "      <td>40.50559</td>\n",
       "      <td>35.50178</td>\n",
       "      <td>35.41980</td>\n",
       "      <td>33.63282</td>\n",
       "      <td>34.79244</td>\n",
       "      <td>38.22308</td>\n",
       "      <td>31.49147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.601293</td>\n",
       "      <td>34.197698</td>\n",
       "      <td>36.578348</td>\n",
       "      <td>31.895400</td>\n",
       "      <td>39.46713</td>\n",
       "      <td>33.22340</td>\n",
       "      <td>36.25593</td>\n",
       "      <td>34.21029</td>\n",
       "      <td>35.36208</td>\n",
       "      <td>37.86790</td>\n",
       "      <td>...</td>\n",
       "      <td>33.19823</td>\n",
       "      <td>33.68316</td>\n",
       "      <td>34.41938</td>\n",
       "      <td>38.99231</td>\n",
       "      <td>35.77236</td>\n",
       "      <td>34.18862</td>\n",
       "      <td>32.88250</td>\n",
       "      <td>35.02014</td>\n",
       "      <td>39.94908</td>\n",
       "      <td>32.11538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.593121</td>\n",
       "      <td>33.351460</td>\n",
       "      <td>36.807497</td>\n",
       "      <td>33.968348</td>\n",
       "      <td>38.49884</td>\n",
       "      <td>33.40876</td>\n",
       "      <td>35.39769</td>\n",
       "      <td>34.92401</td>\n",
       "      <td>34.26885</td>\n",
       "      <td>35.26187</td>\n",
       "      <td>...</td>\n",
       "      <td>30.89813</td>\n",
       "      <td>34.63036</td>\n",
       "      <td>34.59911</td>\n",
       "      <td>38.41437</td>\n",
       "      <td>33.47112</td>\n",
       "      <td>34.91241</td>\n",
       "      <td>33.44515</td>\n",
       "      <td>35.01310</td>\n",
       "      <td>39.31564</td>\n",
       "      <td>33.33646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>32.603769</td>\n",
       "      <td>34.133940</td>\n",
       "      <td>35.318612</td>\n",
       "      <td>33.843872</td>\n",
       "      <td>39.13826</td>\n",
       "      <td>33.62978</td>\n",
       "      <td>35.17642</td>\n",
       "      <td>33.60519</td>\n",
       "      <td>35.75912</td>\n",
       "      <td>37.34151</td>\n",
       "      <td>...</td>\n",
       "      <td>32.12573</td>\n",
       "      <td>33.34867</td>\n",
       "      <td>36.50807</td>\n",
       "      <td>35.15898</td>\n",
       "      <td>34.57504</td>\n",
       "      <td>35.39631</td>\n",
       "      <td>32.93248</td>\n",
       "      <td>35.12781</td>\n",
       "      <td>40.48054</td>\n",
       "      <td>31.79913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>33.619701</td>\n",
       "      <td>32.373330</td>\n",
       "      <td>35.771711</td>\n",
       "      <td>32.519967</td>\n",
       "      <td>35.86338</td>\n",
       "      <td>31.25871</td>\n",
       "      <td>32.24347</td>\n",
       "      <td>31.63139</td>\n",
       "      <td>37.02994</td>\n",
       "      <td>38.71080</td>\n",
       "      <td>...</td>\n",
       "      <td>34.27276</td>\n",
       "      <td>32.16275</td>\n",
       "      <td>33.97705</td>\n",
       "      <td>38.85295</td>\n",
       "      <td>32.38354</td>\n",
       "      <td>32.04003</td>\n",
       "      <td>32.62658</td>\n",
       "      <td>33.78873</td>\n",
       "      <td>37.41392</td>\n",
       "      <td>31.66344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>33.316811</td>\n",
       "      <td>34.118843</td>\n",
       "      <td>36.008091</td>\n",
       "      <td>33.115209</td>\n",
       "      <td>37.91340</td>\n",
       "      <td>32.66502</td>\n",
       "      <td>35.55199</td>\n",
       "      <td>33.43254</td>\n",
       "      <td>35.47039</td>\n",
       "      <td>38.35448</td>\n",
       "      <td>...</td>\n",
       "      <td>32.92305</td>\n",
       "      <td>34.01015</td>\n",
       "      <td>34.85694</td>\n",
       "      <td>37.96021</td>\n",
       "      <td>36.65499</td>\n",
       "      <td>33.34126</td>\n",
       "      <td>32.81059</td>\n",
       "      <td>35.24316</td>\n",
       "      <td>38.72091</td>\n",
       "      <td>32.39461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>33.046782</td>\n",
       "      <td>33.833796</td>\n",
       "      <td>37.008936</td>\n",
       "      <td>32.895151</td>\n",
       "      <td>37.96870</td>\n",
       "      <td>33.57688</td>\n",
       "      <td>35.18870</td>\n",
       "      <td>33.74302</td>\n",
       "      <td>33.76634</td>\n",
       "      <td>36.74006</td>\n",
       "      <td>...</td>\n",
       "      <td>31.87160</td>\n",
       "      <td>33.23246</td>\n",
       "      <td>34.24055</td>\n",
       "      <td>37.24924</td>\n",
       "      <td>36.84744</td>\n",
       "      <td>34.98283</td>\n",
       "      <td>34.04810</td>\n",
       "      <td>35.60526</td>\n",
       "      <td>40.53108</td>\n",
       "      <td>32.34561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>32.170042</td>\n",
       "      <td>33.739764</td>\n",
       "      <td>35.937812</td>\n",
       "      <td>33.404526</td>\n",
       "      <td>38.75226</td>\n",
       "      <td>32.10887</td>\n",
       "      <td>32.99715</td>\n",
       "      <td>28.89508</td>\n",
       "      <td>33.23928</td>\n",
       "      <td>37.84644</td>\n",
       "      <td>...</td>\n",
       "      <td>32.47268</td>\n",
       "      <td>32.81781</td>\n",
       "      <td>35.99620</td>\n",
       "      <td>38.54211</td>\n",
       "      <td>37.23935</td>\n",
       "      <td>33.82151</td>\n",
       "      <td>33.82576</td>\n",
       "      <td>35.13995</td>\n",
       "      <td>40.81516</td>\n",
       "      <td>30.34566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         APAF1     ARID1A        ATM       BAP1     EPAS1     ERBB2      FLT1  \\\n",
       "0    32.668769  33.848026  35.942429  33.677294  37.95811  35.32243  33.69326   \n",
       "1    32.337493  33.843513  35.988225  32.643149  38.83281  33.71706  35.56873   \n",
       "2    31.818198  33.516005  36.193587  32.368866  37.19345  33.38917  34.21918   \n",
       "3    32.601293  34.197698  36.578348  31.895400  39.46713  33.22340  36.25593   \n",
       "4    33.593121  33.351460  36.807497  33.968348  38.49884  33.40876  35.39769   \n",
       "..         ...        ...        ...        ...       ...       ...       ...   \n",
       "176  32.603769  34.133940  35.318612  33.843872  39.13826  33.62978  35.17642   \n",
       "177  33.619701  32.373330  35.771711  32.519967  35.86338  31.25871  32.24347   \n",
       "178  33.316811  34.118843  36.008091  33.115209  37.91340  32.66502  35.55199   \n",
       "179  33.046782  33.833796  37.008936  32.895151  37.96870  33.57688  35.18870   \n",
       "180  32.170042  33.739764  35.937812  33.404526  38.75226  32.10887  32.99715   \n",
       "\n",
       "         FLT4     GSTP1     HSPB1  ...    RNF139     SETD2    SLC2A1  \\\n",
       "0    30.79376  36.48088  38.25591  ...  32.46554  32.58565  33.38586   \n",
       "1    33.38444  36.21403  37.41814  ...  32.27190  33.19915  33.69538   \n",
       "2    33.34670  35.34069  37.94992  ...  32.55514  32.84628  36.23588   \n",
       "3    34.21029  35.36208  37.86790  ...  33.19823  33.68316  34.41938   \n",
       "4    34.92401  34.26885  35.26187  ...  30.89813  34.63036  34.59911   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "176  33.60519  35.75912  37.34151  ...  32.12573  33.34867  36.50807   \n",
       "177  31.63139  37.02994  38.71080  ...  34.27276  32.16275  33.97705   \n",
       "178  33.43254  35.47039  38.35448  ...  32.92305  34.01015  34.85694   \n",
       "179  33.74302  33.76634  36.74006  ...  31.87160  33.23246  34.24055   \n",
       "180  28.89508  33.23928  37.84644  ...  32.47268  32.81781  35.99620   \n",
       "\n",
       "         SOD2      TGM2      TP53      TSC1      TSC2     VEGFA       VHL  \n",
       "0    38.67433  38.50142  33.83518  32.93402  34.93520  37.79678  32.30615  \n",
       "1    38.64559  34.33752  34.44810  33.16630  35.08304  40.09193  32.19988  \n",
       "2    40.50559  35.50178  35.41980  33.63282  34.79244  38.22308  31.49147  \n",
       "3    38.99231  35.77236  34.18862  32.88250  35.02014  39.94908  32.11538  \n",
       "4    38.41437  33.47112  34.91241  33.44515  35.01310  39.31564  33.33646  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "176  35.15898  34.57504  35.39631  32.93248  35.12781  40.48054  31.79913  \n",
       "177  38.85295  32.38354  32.04003  32.62658  33.78873  37.41392  31.66344  \n",
       "178  37.96021  36.65499  33.34126  32.81059  35.24316  38.72091  32.39461  \n",
       "179  37.24924  36.84744  34.98283  34.04810  35.60526  40.53108  32.34561  \n",
       "180  38.54211  37.23935  33.82151  33.82576  35.13995  40.81516  30.34566  \n",
       "\n",
       "[181 rows x 32 columns]"
      ]
     },
     "execution_count": 1193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes = pd.read_csv('..\\..\\..\\Data\\PPT-Ohmnet\\mRCC_big_pool\\Second big pool/mrcc_protein_matrix_84_genes_32_nodes.csv')\n",
    "Y = genes.Y\n",
    "\n",
    "genes = genes.iloc[:,1:33] \n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APAF1</th>\n",
       "      <th>ARID1A</th>\n",
       "      <th>ATM</th>\n",
       "      <th>BAP1</th>\n",
       "      <th>EPAS1</th>\n",
       "      <th>ERBB2</th>\n",
       "      <th>FLT1</th>\n",
       "      <th>FLT4</th>\n",
       "      <th>GSTP1</th>\n",
       "      <th>HSPB1</th>\n",
       "      <th>...</th>\n",
       "      <th>RNF139</th>\n",
       "      <th>SETD2</th>\n",
       "      <th>SLC2A1</th>\n",
       "      <th>SOD2</th>\n",
       "      <th>TGM2</th>\n",
       "      <th>TP53</th>\n",
       "      <th>TSC1</th>\n",
       "      <th>TSC2</th>\n",
       "      <th>VEGFA</th>\n",
       "      <th>VHL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.610274</td>\n",
       "      <td>0.474298</td>\n",
       "      <td>0.551095</td>\n",
       "      <td>0.703386</td>\n",
       "      <td>0.614968</td>\n",
       "      <td>0.879366</td>\n",
       "      <td>0.485379</td>\n",
       "      <td>0.345731</td>\n",
       "      <td>0.697909</td>\n",
       "      <td>0.655601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547741</td>\n",
       "      <td>0.361620</td>\n",
       "      <td>0.420160</td>\n",
       "      <td>0.542412</td>\n",
       "      <td>0.945549</td>\n",
       "      <td>0.403803</td>\n",
       "      <td>0.411780</td>\n",
       "      <td>0.408244</td>\n",
       "      <td>0.439826</td>\n",
       "      <td>0.681580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.536117</td>\n",
       "      <td>0.472846</td>\n",
       "      <td>0.561963</td>\n",
       "      <td>0.465055</td>\n",
       "      <td>0.796869</td>\n",
       "      <td>0.573713</td>\n",
       "      <td>0.753386</td>\n",
       "      <td>0.679993</td>\n",
       "      <td>0.653713</td>\n",
       "      <td>0.472156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504091</td>\n",
       "      <td>0.518369</td>\n",
       "      <td>0.458930</td>\n",
       "      <td>0.538144</td>\n",
       "      <td>0.301997</td>\n",
       "      <td>0.538341</td>\n",
       "      <td>0.474109</td>\n",
       "      <td>0.451980</td>\n",
       "      <td>0.760074</td>\n",
       "      <td>0.664154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.419872</td>\n",
       "      <td>0.367512</td>\n",
       "      <td>0.610698</td>\n",
       "      <td>0.401843</td>\n",
       "      <td>0.455951</td>\n",
       "      <td>0.511285</td>\n",
       "      <td>0.560534</td>\n",
       "      <td>0.675123</td>\n",
       "      <td>0.509070</td>\n",
       "      <td>0.588599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567938</td>\n",
       "      <td>0.428211</td>\n",
       "      <td>0.777154</td>\n",
       "      <td>0.814317</td>\n",
       "      <td>0.481939</td>\n",
       "      <td>0.751632</td>\n",
       "      <td>0.599295</td>\n",
       "      <td>0.366011</td>\n",
       "      <td>0.499309</td>\n",
       "      <td>0.547991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.595169</td>\n",
       "      <td>0.586761</td>\n",
       "      <td>0.702007</td>\n",
       "      <td>0.292727</td>\n",
       "      <td>0.928781</td>\n",
       "      <td>0.479723</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>0.786548</td>\n",
       "      <td>0.512613</td>\n",
       "      <td>0.570639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712900</td>\n",
       "      <td>0.642034</td>\n",
       "      <td>0.549619</td>\n",
       "      <td>0.589625</td>\n",
       "      <td>0.523759</td>\n",
       "      <td>0.481384</td>\n",
       "      <td>0.397955</td>\n",
       "      <td>0.433372</td>\n",
       "      <td>0.740142</td>\n",
       "      <td>0.650298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.817191</td>\n",
       "      <td>0.314590</td>\n",
       "      <td>0.756387</td>\n",
       "      <td>0.770463</td>\n",
       "      <td>0.727417</td>\n",
       "      <td>0.515014</td>\n",
       "      <td>0.728944</td>\n",
       "      <td>0.878635</td>\n",
       "      <td>0.331552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194423</td>\n",
       "      <td>0.884044</td>\n",
       "      <td>0.572132</td>\n",
       "      <td>0.503813</td>\n",
       "      <td>0.168091</td>\n",
       "      <td>0.640258</td>\n",
       "      <td>0.548936</td>\n",
       "      <td>0.431290</td>\n",
       "      <td>0.651756</td>\n",
       "      <td>0.850528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.595723</td>\n",
       "      <td>0.566255</td>\n",
       "      <td>0.403055</td>\n",
       "      <td>0.741776</td>\n",
       "      <td>0.860390</td>\n",
       "      <td>0.557095</td>\n",
       "      <td>0.697324</td>\n",
       "      <td>0.708475</td>\n",
       "      <td>0.578371</td>\n",
       "      <td>0.455376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471142</td>\n",
       "      <td>0.556572</td>\n",
       "      <td>0.811248</td>\n",
       "      <td>0.020453</td>\n",
       "      <td>0.338707</td>\n",
       "      <td>0.746476</td>\n",
       "      <td>0.411366</td>\n",
       "      <td>0.465225</td>\n",
       "      <td>0.814298</td>\n",
       "      <td>0.598440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.823141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510581</td>\n",
       "      <td>0.436667</td>\n",
       "      <td>0.179353</td>\n",
       "      <td>0.105657</td>\n",
       "      <td>0.278203</td>\n",
       "      <td>0.453806</td>\n",
       "      <td>0.788844</td>\n",
       "      <td>0.755208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955115</td>\n",
       "      <td>0.253569</td>\n",
       "      <td>0.494212</td>\n",
       "      <td>0.568933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.329281</td>\n",
       "      <td>0.069080</td>\n",
       "      <td>0.386405</td>\n",
       "      <td>0.576190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.755339</td>\n",
       "      <td>0.561399</td>\n",
       "      <td>0.566677</td>\n",
       "      <td>0.573847</td>\n",
       "      <td>0.605671</td>\n",
       "      <td>0.373411</td>\n",
       "      <td>0.750994</td>\n",
       "      <td>0.686199</td>\n",
       "      <td>0.530551</td>\n",
       "      <td>0.677185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650870</td>\n",
       "      <td>0.725580</td>\n",
       "      <td>0.604427</td>\n",
       "      <td>0.436379</td>\n",
       "      <td>0.660174</td>\n",
       "      <td>0.295386</td>\n",
       "      <td>0.378658</td>\n",
       "      <td>0.499349</td>\n",
       "      <td>0.568772</td>\n",
       "      <td>0.696085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.694893</td>\n",
       "      <td>0.469721</td>\n",
       "      <td>0.804191</td>\n",
       "      <td>0.523132</td>\n",
       "      <td>0.617171</td>\n",
       "      <td>0.547024</td>\n",
       "      <td>0.699079</td>\n",
       "      <td>0.726258</td>\n",
       "      <td>0.248326</td>\n",
       "      <td>0.323678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413858</td>\n",
       "      <td>0.526880</td>\n",
       "      <td>0.527218</td>\n",
       "      <td>0.330815</td>\n",
       "      <td>0.689918</td>\n",
       "      <td>0.655716</td>\n",
       "      <td>0.710731</td>\n",
       "      <td>0.606470</td>\n",
       "      <td>0.821350</td>\n",
       "      <td>0.688050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.498633</td>\n",
       "      <td>0.439478</td>\n",
       "      <td>0.549999</td>\n",
       "      <td>0.640524</td>\n",
       "      <td>0.780118</td>\n",
       "      <td>0.267523</td>\n",
       "      <td>0.385904</td>\n",
       "      <td>0.100754</td>\n",
       "      <td>0.161034</td>\n",
       "      <td>0.565940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549350</td>\n",
       "      <td>0.420937</td>\n",
       "      <td>0.747131</td>\n",
       "      <td>0.522780</td>\n",
       "      <td>0.750490</td>\n",
       "      <td>0.400802</td>\n",
       "      <td>0.651068</td>\n",
       "      <td>0.468816</td>\n",
       "      <td>0.860988</td>\n",
       "      <td>0.360103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        APAF1    ARID1A       ATM      BAP1     EPAS1     ERBB2      FLT1  \\\n",
       "0    0.610274  0.474298  0.551095  0.703386  0.614968  0.879366  0.485379   \n",
       "1    0.536117  0.472846  0.561963  0.465055  0.796869  0.573713  0.753386   \n",
       "2    0.419872  0.367512  0.610698  0.401843  0.455951  0.511285  0.560534   \n",
       "3    0.595169  0.586761  0.702007  0.292727  0.928781  0.479723  0.851587   \n",
       "4    0.817191  0.314590  0.756387  0.770463  0.727417  0.515014  0.728944   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "176  0.595723  0.566255  0.403055  0.741776  0.860390  0.557095  0.697324   \n",
       "177  0.823141  0.000000  0.510581  0.436667  0.179353  0.105657  0.278203   \n",
       "178  0.755339  0.561399  0.566677  0.573847  0.605671  0.373411  0.750994   \n",
       "179  0.694893  0.469721  0.804191  0.523132  0.617171  0.547024  0.699079   \n",
       "180  0.498633  0.439478  0.549999  0.640524  0.780118  0.267523  0.385904   \n",
       "\n",
       "         FLT4     GSTP1     HSPB1  ...    RNF139     SETD2    SLC2A1  \\\n",
       "0    0.345731  0.697909  0.655601  ...  0.547741  0.361620  0.420160   \n",
       "1    0.679993  0.653713  0.472156  ...  0.504091  0.518369  0.458930   \n",
       "2    0.675123  0.509070  0.588599  ...  0.567938  0.428211  0.777154   \n",
       "3    0.786548  0.512613  0.570639  ...  0.712900  0.642034  0.549619   \n",
       "4    0.878635  0.331552  0.000000  ...  0.194423  0.884044  0.572132   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "176  0.708475  0.578371  0.455376  ...  0.471142  0.556572  0.811248   \n",
       "177  0.453806  0.788844  0.755208  ...  0.955115  0.253569  0.494212   \n",
       "178  0.686199  0.530551  0.677185  ...  0.650870  0.725580  0.604427   \n",
       "179  0.726258  0.248326  0.323678  ...  0.413858  0.526880  0.527218   \n",
       "180  0.100754  0.161034  0.565940  ...  0.549350  0.420937  0.747131   \n",
       "\n",
       "         SOD2      TGM2      TP53      TSC1      TSC2     VEGFA       VHL  \n",
       "0    0.542412  0.945549  0.403803  0.411780  0.408244  0.439826  0.681580  \n",
       "1    0.538144  0.301997  0.538341  0.474109  0.451980  0.760074  0.664154  \n",
       "2    0.814317  0.481939  0.751632  0.599295  0.366011  0.499309  0.547991  \n",
       "3    0.589625  0.523759  0.481384  0.397955  0.433372  0.740142  0.650298  \n",
       "4    0.503813  0.168091  0.640258  0.548936  0.431290  0.651756  0.850528  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "176  0.020453  0.338707  0.746476  0.411366  0.465225  0.814298  0.598440  \n",
       "177  0.568933  0.000000  0.009761  0.329281  0.069080  0.386405  0.576190  \n",
       "178  0.436379  0.660174  0.295386  0.378658  0.499349  0.568772  0.696085  \n",
       "179  0.330815  0.689918  0.655716  0.710731  0.606470  0.821350  0.688050  \n",
       "180  0.522780  0.750490  0.400802  0.651068  0.468816  0.860988  0.360103  \n",
       "\n",
       "[181 rows x 32 columns]"
      ]
     },
     "execution_count": 1194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = genes.columns\n",
    "d = scaler.fit_transform(genes)\n",
    "genes = pd.DataFrame(d, columns=names)\n",
    "genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='../../../Data/PPT-Ohmnet/mRCC_big_pool/Second big pool/network_edges_mrcc_84_genes_32_nodes.tsv'\n",
    "data = pd.read_csv(path, delimiter='\\t')\n",
    "edge_index1=data[data.columns[1]].to_numpy()\n",
    "edge_index2=data[data.columns[2]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = np.concatenate((edge_index1, edge_index2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['VHL', 'VHL', 'VHL', 'VHL', 'VHL', 'VHL', 'VHL', 'GSTP1', 'GSTP1',\n",
       "       'TGM2', 'TGM2', 'SETD2', 'ERBB2', 'ERBB2', 'FLT1', 'FLT1', 'FLT4',\n",
       "       'NDRG1', 'NF2', 'NF2', 'PIK3CA', 'MTOR', 'MTOR', 'APAF1', 'KDR',\n",
       "       'TSC1', 'RELA', 'RELA', 'RELA', 'ATM', 'MAPK8', 'PTEN', 'PTEN',\n",
       "       'ARID1A', 'PTGS2', 'HSPB1', 'HSPD1', 'SLC2A1', 'RNF139', 'ATM',\n",
       "       'TGM2', 'TP53', 'SOD2', 'EPAS1', 'TGM2', 'MAPK8', 'RELA', 'PAK1',\n",
       "       'TP53', 'NF2', 'PAK1', 'VEGFA', 'KDR', 'KDR', 'TP53', 'PAK1',\n",
       "       'TSC1', 'PTEN', 'TP53', 'MAPK8', 'TP53', 'VEGFA', 'TSC2', 'IL6',\n",
       "       'TP53', 'ATM', 'TP53', 'TP53', 'TP53', 'BAP1', 'TP53', 'TP53',\n",
       "       'TP53', 'PTEN'], dtype=object)"
      ]
     },
     "execution_count": 1197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 1198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(edge_index)\n",
    "len(list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index1 = le.transform(edge_index1)\n",
    "edge_index2 = le.transform(edge_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = [edge_index1]+[edge_index2]\n",
    "edge_index = np.array(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31, 31, 31, 31, 31, 31, 31,  8,  8, 26, 26, 23,  5,  5,  6,  6,\n",
       "         7, 15, 16, 16, 18, 14, 14,  0, 12, 28, 21, 21, 21,  2, 13, 19,\n",
       "        19,  1, 20,  9, 10],\n",
       "       [24, 22,  2, 26, 27, 25,  4, 26, 13, 21, 17, 27, 16, 17, 30, 12,\n",
       "        12, 27, 17, 28, 19, 27, 13, 27, 30, 29, 11, 27,  2, 27, 27, 27,\n",
       "         3, 27, 27, 27, 19]])"
      ]
     },
     "execution_count": 1201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31, 31, 31, 31, 31, 31, 31,  8,  8, 26, 26, 23,  5,  5,  6,  6,  7, 15,\n",
       "         16, 16, 18, 14, 14,  0, 12, 28, 21, 21, 21,  2, 13, 19, 19,  1, 20,  9,\n",
       "         10],\n",
       "        [24, 22,  2, 26, 27, 25,  4, 26, 13, 21, 17, 27, 16, 17, 30, 12, 12, 27,\n",
       "         17, 28, 19, 27, 13, 27, 30, 29, 11, 27,  2, 27, 27, 27,  3, 27, 27, 27,\n",
       "         19]])"
      ]
     },
     "execution_count": 1202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edge_index, dtype=torch.int64)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_16244/1239388042.py:11: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  x = torch.tensor([b], dtype=torch.long).reshape([-1,1])\n"
     ]
    }
   ],
   "source": [
    "list_data_0=[]\n",
    "list_data_1=[]\n",
    "total_data=[]\n",
    "for g in range(len(genes)):\n",
    "  b=[]\n",
    "  for i in genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    # a.append(Y[g])\n",
    "    a.append(i*100)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.long).reshape([-1,1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y.iloc[g]], dtype=torch.long).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  total_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algunas estadísticas del grafo:\n",
      "Número de Características: 1\n",
      "Número de Nodos: 32\n",
      "Número de bordes: 37\n",
      "Grado promedio de nodos: 1.16\n",
      "¿Contiene nodos aislados?: False\n",
      "¿Contiene autoloops?: False\n",
      "¿Es no dirigido?: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'contains_isolated_nodes' is deprecated, use 'has_isolated_nodes' instead\n",
      "  warnings.warn(out)\n",
      "C:\\Users\\sandr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'contains_self_loops' is deprecated, use 'has_self_loops' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "def plot_graph(data,description=True):\n",
    "    edges_raw = data.edge_index.numpy()\n",
    "    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
    "    labels = data.x.numpy()\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(list(range(np.max(edges_raw))))\n",
    "    G.add_edges_from(edges)\n",
    "    plt.subplot(111)\n",
    "    options = {\n",
    "       'node_size': 100,\n",
    "       'width': 1,\n",
    "    }\n",
    "    nx.draw(G, with_labels=description, node_color=labels.tolist(), cmap=plt.cm.tab10, font_weight='bold', **options)\n",
    "    plt.show()\n",
    "\n",
    "plot_graph(data,True)\n",
    "print(\"Algunas estadísticas del grafo:\")\n",
    "print(f'Número de Características: {data.num_features}')\n",
    "print(f'Número de Nodos: {data.num_nodes}')\n",
    "print(f'Número de bordes: {data.num_edges}')\n",
    "print(f'Grado promedio de nodos: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'¿Contiene nodos aislados?: {data.contains_isolated_nodes()}')\n",
    "print(f'¿Contiene autoloops?: {data.contains_self_loops()}')\n",
    "print(f'¿Es no dirigido?: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Patient sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 32\n",
      "Number of charcateristics per node: 1\n",
      "Number of edges: 37\n",
      "Average node degree: 1.16\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n",
      "Number of node features: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of charcateristics per node: {data.num_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "print(f'Number of node features: {data.num_node_features}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import SAGPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "# from torch_geometric.nn import GCNConv, GINConv\n",
    "# from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "# embed_dim = 32\n",
    "\n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, dim_h):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = GCNConv(1, dim_h)\n",
    "#         self.conv2 = GCNConv(dim_h, dim_h)\n",
    "#         self.conv3 = GCNConv(dim_h, dim_h)\n",
    "#         self.lin = Linear(dim_h, 1)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         # Node embeddings \n",
    "#         h = self.conv1(x, edge_index)\n",
    "#         h = h.relu()\n",
    "#         h = self.conv2(h, edge_index)\n",
    "#         h = h.relu()\n",
    "#         h = self.conv3(h, edge_index)\n",
    "\n",
    "#         # Graph-level readout\n",
    "#         hG = global_mean_pool(h, batch)\n",
    "\n",
    "#         # Classifier\n",
    "#         h = F.dropout(hG, p=0.5, training=self.training)\n",
    "#         h = self.lin(h)\n",
    "        \n",
    "#         return F.sigmoid(h).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv, GINConv\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "embed_dim = 32\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dim_h):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(1, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        self.lin1 = Linear(dim_h*3, dim_h*3)\n",
    "        self.lin2 = Linear(dim_h*3, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Node embeddings \n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "\n",
    "        # Graph-level readout\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model):\n",
    "    # model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(output, data.y.squeeze(1))  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def test(loader, model):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(output, data.y.squeeze(1))\n",
    "        correct += accuracy(output.argmax(dim=1), data.y.squeeze(1)) / len(loader)\n",
    "    return loss, correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "kf=StratifiedKFold(n_splits=5, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  [ 30  32  33  34  40  41  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 31 35 36 37 38 39 42]\n",
      "144\n",
      "37\n",
      "Epoch: 001, Loss: 1.4575, Train Acc: 0.5000, Loss: 0.1424, Test Acc: 0.5750\n",
      "Epoch: 002, Loss: 0.8205, Train Acc: 0.5000, Loss: 1.3466, Test Acc: 0.4250\n",
      "Epoch: 003, Loss: 0.8117, Train Acc: 0.5000, Loss: 0.4629, Test Acc: 0.5750\n",
      "Epoch: 004, Loss: 0.7210, Train Acc: 0.5000, Loss: 0.6367, Test Acc: 0.5750\n",
      "Epoch: 005, Loss: 0.7054, Train Acc: 0.5000, Loss: 0.9103, Test Acc: 0.4250\n",
      "Epoch: 006, Loss: 0.7174, Train Acc: 0.5000, Loss: 0.6225, Test Acc: 0.5750\n",
      "Epoch: 007, Loss: 0.6954, Train Acc: 0.5000, Loss: 0.6170, Test Acc: 0.5750\n",
      "Epoch: 008, Loss: 0.6923, Train Acc: 0.5000, Loss: 0.7761, Test Acc: 0.4250\n",
      "Epoch: 009, Loss: 0.7035, Train Acc: 0.5375, Loss: 0.6985, Test Acc: 0.4150\n",
      "Epoch: 010, Loss: 0.6946, Train Acc: 0.5000, Loss: 0.6498, Test Acc: 0.5750\n",
      "Epoch: 011, Loss: 0.6920, Train Acc: 0.5125, Loss: 0.7283, Test Acc: 0.4250\n",
      "Epoch: 012, Loss: 0.6972, Train Acc: 0.5500, Loss: 0.7215, Test Acc: 0.4500\n",
      "Epoch: 013, Loss: 0.6950, Train Acc: 0.5063, Loss: 0.6792, Test Acc: 0.5500\n",
      "Epoch: 014, Loss: 0.6927, Train Acc: 0.5625, Loss: 0.7066, Test Acc: 0.3000\n",
      "Epoch: 015, Loss: 0.6945, Train Acc: 0.5437, Loss: 0.7153, Test Acc: 0.4250\n",
      "Epoch: 016, Loss: 0.6945, Train Acc: 0.5125, Loss: 0.7010, Test Acc: 0.4550\n",
      "Epoch: 017, Loss: 0.6933, Train Acc: 0.5312, Loss: 0.7126, Test Acc: 0.2750\n",
      "Epoch: 018, Loss: 0.6941, Train Acc: 0.5312, Loss: 0.7184, Test Acc: 0.3000\n",
      "Epoch: 019, Loss: 0.6941, Train Acc: 0.5062, Loss: 0.7073, Test Acc: 0.3800\n",
      "Epoch: 020, Loss: 0.6933, Train Acc: 0.5250, Loss: 0.7157, Test Acc: 0.2750\n",
      "Epoch: 021, Loss: 0.6938, Train Acc: 0.5312, Loss: 0.7176, Test Acc: 0.2750\n",
      "Epoch: 022, Loss: 0.6937, Train Acc: 0.5125, Loss: 0.7133, Test Acc: 0.2750\n",
      "Epoch: 023, Loss: 0.6933, Train Acc: 0.5250, Loss: 0.7181, Test Acc: 0.2750\n",
      "Epoch: 024, Loss: 0.6935, Train Acc: 0.5375, Loss: 0.7211, Test Acc: 0.2750\n",
      "Epoch: 025, Loss: 0.6935, Train Acc: 0.5188, Loss: 0.7179, Test Acc: 0.2750\n",
      "Epoch: 026, Loss: 0.6931, Train Acc: 0.5375, Loss: 0.7213, Test Acc: 0.2750\n",
      "Epoch: 027, Loss: 0.6933, Train Acc: 0.5375, Loss: 0.7220, Test Acc: 0.2750\n",
      "Epoch: 028, Loss: 0.6932, Train Acc: 0.5312, Loss: 0.7214, Test Acc: 0.2750\n",
      "Epoch: 029, Loss: 0.6930, Train Acc: 0.5250, Loss: 0.7224, Test Acc: 0.2750\n",
      "Epoch: 030, Loss: 0.6929, Train Acc: 0.5375, Loss: 0.7261, Test Acc: 0.2750\n",
      "Epoch: 031, Loss: 0.6930, Train Acc: 0.5375, Loss: 0.7214, Test Acc: 0.2750\n",
      "Epoch: 032, Loss: 0.6927, Train Acc: 0.5188, Loss: 0.7196, Test Acc: 0.3150\n",
      "Epoch: 033, Loss: 0.6925, Train Acc: 0.5312, Loss: 0.7233, Test Acc: 0.2750\n",
      "Epoch: 034, Loss: 0.6928, Train Acc: 0.5312, Loss: 0.7256, Test Acc: 0.2750\n",
      "Epoch: 035, Loss: 0.6928, Train Acc: 0.5437, Loss: 0.7265, Test Acc: 0.2750\n",
      "Epoch: 036, Loss: 0.6927, Train Acc: 0.5312, Loss: 0.7254, Test Acc: 0.3150\n",
      "Epoch: 037, Loss: 0.6923, Train Acc: 0.5312, Loss: 0.7253, Test Acc: 0.3150\n",
      "Epoch: 038, Loss: 0.6925, Train Acc: 0.5250, Loss: 0.7265, Test Acc: 0.2750\n",
      "Epoch: 039, Loss: 0.6923, Train Acc: 0.5437, Loss: 0.7351, Test Acc: 0.3000\n",
      "Epoch: 040, Loss: 0.6924, Train Acc: 0.5375, Loss: 0.7292, Test Acc: 0.2750\n",
      "Epoch: 041, Loss: 0.6921, Train Acc: 0.5375, Loss: 0.7243, Test Acc: 0.3150\n",
      "Epoch: 042, Loss: 0.6918, Train Acc: 0.5437, Loss: 0.7353, Test Acc: 0.3000\n",
      "Epoch: 043, Loss: 0.6921, Train Acc: 0.5375, Loss: 0.7347, Test Acc: 0.3500\n",
      "Epoch: 044, Loss: 0.6920, Train Acc: 0.5375, Loss: 0.7236, Test Acc: 0.3150\n",
      "Epoch: 045, Loss: 0.6912, Train Acc: 0.5500, Loss: 0.7350, Test Acc: 0.3000\n",
      "Epoch: 046, Loss: 0.6918, Train Acc: 0.5437, Loss: 0.7326, Test Acc: 0.2750\n",
      "Epoch: 047, Loss: 0.6916, Train Acc: 0.5250, Loss: 0.7294, Test Acc: 0.3150\n",
      "Epoch: 048, Loss: 0.6913, Train Acc: 0.5312, Loss: 0.7410, Test Acc: 0.3500\n",
      "Epoch: 049, Loss: 0.6917, Train Acc: 0.5375, Loss: 0.7303, Test Acc: 0.3150\n",
      "Epoch: 050, Loss: 0.6909, Train Acc: 0.5500, Loss: 0.7330, Test Acc: 0.3000\n",
      "Epoch: 051, Loss: 0.6913, Train Acc: 0.5375, Loss: 0.7398, Test Acc: 0.3250\n",
      "Epoch: 052, Loss: 0.6911, Train Acc: 0.5437, Loss: 0.7355, Test Acc: 0.3000\n",
      "Epoch: 053, Loss: 0.6909, Train Acc: 0.5375, Loss: 0.7326, Test Acc: 0.3000\n",
      "Epoch: 054, Loss: 0.6910, Train Acc: 0.5375, Loss: 0.7439, Test Acc: 0.3000\n",
      "Epoch: 055, Loss: 0.6913, Train Acc: 0.5312, Loss: 0.7469, Test Acc: 0.3250\n",
      "Epoch: 056, Loss: 0.6911, Train Acc: 0.5375, Loss: 0.7383, Test Acc: 0.3150\n",
      "Epoch: 057, Loss: 0.6908, Train Acc: 0.5437, Loss: 0.7298, Test Acc: 0.3150\n",
      "Epoch: 058, Loss: 0.6901, Train Acc: 0.5375, Loss: 0.7474, Test Acc: 0.3250\n",
      "Epoch: 059, Loss: 0.6909, Train Acc: 0.5375, Loss: 0.7446, Test Acc: 0.3750\n",
      "Epoch: 060, Loss: 0.6906, Train Acc: 0.5312, Loss: 0.7300, Test Acc: 0.3150\n",
      "Epoch: 061, Loss: 0.6895, Train Acc: 0.5312, Loss: 0.7478, Test Acc: 0.3750\n",
      "Epoch: 062, Loss: 0.6906, Train Acc: 0.5312, Loss: 0.7418, Test Acc: 0.3250\n",
      "Epoch: 063, Loss: 0.6896, Train Acc: 0.5375, Loss: 0.7416, Test Acc: 0.3500\n",
      "Epoch: 064, Loss: 0.6899, Train Acc: 0.5312, Loss: 0.7424, Test Acc: 0.3250\n",
      "Epoch: 065, Loss: 0.6895, Train Acc: 0.5312, Loss: 0.7476, Test Acc: 0.3750\n",
      "Epoch: 066, Loss: 0.6900, Train Acc: 0.5437, Loss: 0.7348, Test Acc: 0.3150\n",
      "Epoch: 067, Loss: 0.6887, Train Acc: 0.5312, Loss: 0.7471, Test Acc: 0.3750\n",
      "Epoch: 068, Loss: 0.6895, Train Acc: 0.5312, Loss: 0.7444, Test Acc: 0.3000\n",
      "Epoch: 069, Loss: 0.6888, Train Acc: 0.5375, Loss: 0.7410, Test Acc: 0.3250\n",
      "Epoch: 070, Loss: 0.6887, Train Acc: 0.5250, Loss: 0.7463, Test Acc: 0.3000\n",
      "Epoch: 071, Loss: 0.6886, Train Acc: 0.5312, Loss: 0.7481, Test Acc: 0.3750\n",
      "Epoch: 072, Loss: 0.6888, Train Acc: 0.5312, Loss: 0.7391, Test Acc: 0.3650\n",
      "Epoch: 073, Loss: 0.6879, Train Acc: 0.5437, Loss: 0.7505, Test Acc: 0.4000\n",
      "Epoch: 074, Loss: 0.6886, Train Acc: 0.5312, Loss: 0.7433, Test Acc: 0.3000\n",
      "Epoch: 075, Loss: 0.6876, Train Acc: 0.5312, Loss: 0.7478, Test Acc: 0.3750\n",
      "Epoch: 076, Loss: 0.6882, Train Acc: 0.5250, Loss: 0.7443, Test Acc: 0.3250\n",
      "Epoch: 077, Loss: 0.6876, Train Acc: 0.5500, Loss: 0.7574, Test Acc: 0.4000\n",
      "Epoch: 078, Loss: 0.6882, Train Acc: 0.5437, Loss: 0.7369, Test Acc: 0.3650\n",
      "Epoch: 079, Loss: 0.6866, Train Acc: 0.5250, Loss: 0.7437, Test Acc: 0.3750\n",
      "Epoch: 080, Loss: 0.6870, Train Acc: 0.5250, Loss: 0.7489, Test Acc: 0.4000\n",
      "Epoch: 081, Loss: 0.6869, Train Acc: 0.5250, Loss: 0.7431, Test Acc: 0.3500\n",
      "Epoch: 082, Loss: 0.6863, Train Acc: 0.5188, Loss: 0.7439, Test Acc: 0.3750\n",
      "Epoch: 083, Loss: 0.6864, Train Acc: 0.5250, Loss: 0.7510, Test Acc: 0.4000\n",
      "Epoch: 084, Loss: 0.6864, Train Acc: 0.5188, Loss: 0.7432, Test Acc: 0.4000\n",
      "Epoch: 085, Loss: 0.6856, Train Acc: 0.5250, Loss: 0.7437, Test Acc: 0.4000\n",
      "Epoch: 086, Loss: 0.6856, Train Acc: 0.5500, Loss: 0.7536, Test Acc: 0.4000\n",
      "Epoch: 087, Loss: 0.6859, Train Acc: 0.5250, Loss: 0.7447, Test Acc: 0.4250\n",
      "Epoch: 088, Loss: 0.6851, Train Acc: 0.5375, Loss: 0.7488, Test Acc: 0.4000\n",
      "Epoch: 089, Loss: 0.6851, Train Acc: 0.5312, Loss: 0.7471, Test Acc: 0.4250\n",
      "Epoch: 090, Loss: 0.6848, Train Acc: 0.5437, Loss: 0.7451, Test Acc: 0.4250\n",
      "Epoch: 091, Loss: 0.6845, Train Acc: 0.5312, Loss: 0.7462, Test Acc: 0.4250\n",
      "Epoch: 092, Loss: 0.6842, Train Acc: 0.5500, Loss: 0.7500, Test Acc: 0.4250\n",
      "Epoch: 093, Loss: 0.6844, Train Acc: 0.5437, Loss: 0.7461, Test Acc: 0.4250\n",
      "Epoch: 094, Loss: 0.6838, Train Acc: 0.5312, Loss: 0.7452, Test Acc: 0.4250\n",
      "Epoch: 095, Loss: 0.6835, Train Acc: 0.5500, Loss: 0.7476, Test Acc: 0.4000\n",
      "Epoch: 096, Loss: 0.6836, Train Acc: 0.5375, Loss: 0.7441, Test Acc: 0.4250\n",
      "Epoch: 097, Loss: 0.6828, Train Acc: 0.5625, Loss: 0.7535, Test Acc: 0.4000\n",
      "Epoch: 098, Loss: 0.6833, Train Acc: 0.5563, Loss: 0.7494, Test Acc: 0.4250\n",
      "Epoch: 099, Loss: 0.6825, Train Acc: 0.5375, Loss: 0.7434, Test Acc: 0.4000\n",
      "Epoch: 100, Loss: 0.6822, Train Acc: 0.5625, Loss: 0.7458, Test Acc: 0.4250\n",
      "Epoch: 101, Loss: 0.6818, Train Acc: 0.5625, Loss: 0.7517, Test Acc: 0.4000\n",
      "Epoch: 102, Loss: 0.6821, Train Acc: 0.5625, Loss: 0.7468, Test Acc: 0.4000\n",
      "Epoch: 103, Loss: 0.6813, Train Acc: 0.5750, Loss: 0.7484, Test Acc: 0.4000\n",
      "Epoch: 104, Loss: 0.6813, Train Acc: 0.5563, Loss: 0.7494, Test Acc: 0.4000\n",
      "Epoch: 105, Loss: 0.6810, Train Acc: 0.5750, Loss: 0.7455, Test Acc: 0.4000\n",
      "Epoch: 106, Loss: 0.6804, Train Acc: 0.5625, Loss: 0.7512, Test Acc: 0.4000\n",
      "Epoch: 107, Loss: 0.6806, Train Acc: 0.5563, Loss: 0.7426, Test Acc: 0.3750\n",
      "Epoch: 108, Loss: 0.6793, Train Acc: 0.5625, Loss: 0.7475, Test Acc: 0.4000\n",
      "Epoch: 109, Loss: 0.6799, Train Acc: 0.5438, Loss: 0.7415, Test Acc: 0.3750\n",
      "Epoch: 110, Loss: 0.6791, Train Acc: 0.5437, Loss: 0.7482, Test Acc: 0.4000\n",
      "Epoch: 111, Loss: 0.6792, Train Acc: 0.5688, Loss: 0.7410, Test Acc: 0.3750\n",
      "Epoch: 112, Loss: 0.6780, Train Acc: 0.5500, Loss: 0.7430, Test Acc: 0.4250\n",
      "Epoch: 113, Loss: 0.6796, Train Acc: 0.5562, Loss: 0.7424, Test Acc: 0.4000\n",
      "Epoch: 114, Loss: 0.6786, Train Acc: 0.5563, Loss: 0.7597, Test Acc: 0.4250\n",
      "Epoch: 115, Loss: 0.6802, Train Acc: 0.5750, Loss: 0.7250, Test Acc: 0.3900\n",
      "Epoch: 116, Loss: 0.6769, Train Acc: 0.5625, Loss: 0.7588, Test Acc: 0.4250\n",
      "Epoch: 117, Loss: 0.6813, Train Acc: 0.6437, Loss: 0.6709, Test Acc: 0.5350\n",
      "Epoch: 118, Loss: 0.6766, Train Acc: 0.5625, Loss: 0.7453, Test Acc: 0.4000\n",
      "Epoch: 119, Loss: 0.6779, Train Acc: 0.5375, Loss: 0.7748, Test Acc: 0.4000\n",
      "Epoch: 120, Loss: 0.6772, Train Acc: 0.5813, Loss: 0.7147, Test Acc: 0.4400\n",
      "Epoch: 121, Loss: 0.6747, Train Acc: 0.5750, Loss: 0.7477, Test Acc: 0.3750\n",
      "Epoch: 122, Loss: 0.6768, Train Acc: 0.5375, Loss: 0.7806, Test Acc: 0.4250\n",
      "Epoch: 123, Loss: 0.6771, Train Acc: 0.6000, Loss: 0.7159, Test Acc: 0.4400\n",
      "Epoch: 124, Loss: 0.6725, Train Acc: 0.5687, Loss: 0.7426, Test Acc: 0.4250\n",
      "Epoch: 125, Loss: 0.6782, Train Acc: 0.6062, Loss: 0.6955, Test Acc: 0.4850\n",
      "Epoch: 126, Loss: 0.6716, Train Acc: 0.5500, Loss: 0.7887, Test Acc: 0.4250\n",
      "Epoch: 127, Loss: 0.6776, Train Acc: 0.5938, Loss: 0.7138, Test Acc: 0.3900\n",
      "Epoch: 128, Loss: 0.6712, Train Acc: 0.5563, Loss: 0.7612, Test Acc: 0.4250\n",
      "Epoch: 129, Loss: 0.6775, Train Acc: 0.5938, Loss: 0.6847, Test Acc: 0.5100\n",
      "Epoch: 130, Loss: 0.6685, Train Acc: 0.5687, Loss: 0.7729, Test Acc: 0.4250\n",
      "Epoch: 131, Loss: 0.6766, Train Acc: 0.5750, Loss: 0.7025, Test Acc: 0.4700\n",
      "Epoch: 132, Loss: 0.6678, Train Acc: 0.5625, Loss: 0.7571, Test Acc: 0.4250\n",
      "Epoch: 133, Loss: 0.6752, Train Acc: 0.6000, Loss: 0.6991, Test Acc: 0.4700\n",
      "Epoch: 134, Loss: 0.6664, Train Acc: 0.5625, Loss: 0.7536, Test Acc: 0.4000\n",
      "Epoch: 135, Loss: 0.6740, Train Acc: 0.5938, Loss: 0.7005, Test Acc: 0.4450\n",
      "Epoch: 136, Loss: 0.6660, Train Acc: 0.5687, Loss: 0.7602, Test Acc: 0.4250\n",
      "Epoch: 137, Loss: 0.6730, Train Acc: 0.5938, Loss: 0.6860, Test Acc: 0.4450\n",
      "Epoch: 138, Loss: 0.6643, Train Acc: 0.5812, Loss: 0.7766, Test Acc: 0.4250\n",
      "Epoch: 139, Loss: 0.6718, Train Acc: 0.6188, Loss: 0.7045, Test Acc: 0.4450\n",
      "Epoch: 140, Loss: 0.6637, Train Acc: 0.5625, Loss: 0.7629, Test Acc: 0.4250\n",
      "Epoch: 141, Loss: 0.6710, Train Acc: 0.5938, Loss: 0.6789, Test Acc: 0.4450\n",
      "Epoch: 142, Loss: 0.6615, Train Acc: 0.5687, Loss: 0.7725, Test Acc: 0.4250\n",
      "Epoch: 143, Loss: 0.6699, Train Acc: 0.6063, Loss: 0.6769, Test Acc: 0.4450\n",
      "Epoch: 144, Loss: 0.6594, Train Acc: 0.5687, Loss: 0.7737, Test Acc: 0.4250\n",
      "Epoch: 145, Loss: 0.6699, Train Acc: 0.6188, Loss: 0.6800, Test Acc: 0.4700\n",
      "Epoch: 146, Loss: 0.6576, Train Acc: 0.6000, Loss: 0.7419, Test Acc: 0.5300\n",
      "Epoch: 147, Loss: 0.6634, Train Acc: 0.6188, Loss: 0.7276, Test Acc: 0.4550\n",
      "Epoch: 148, Loss: 0.6593, Train Acc: 0.6312, Loss: 0.7371, Test Acc: 0.5300\n",
      "Epoch: 149, Loss: 0.6624, Train Acc: 0.6125, Loss: 0.7228, Test Acc: 0.4700\n",
      "Epoch: 150, Loss: 0.6580, Train Acc: 0.6250, Loss: 0.7418, Test Acc: 0.5300\n",
      "Epoch: 151, Loss: 0.6596, Train Acc: 0.6250, Loss: 0.6916, Test Acc: 0.5200\n",
      "Epoch: 152, Loss: 0.6552, Train Acc: 0.6375, Loss: 0.7376, Test Acc: 0.5300\n",
      "Epoch: 153, Loss: 0.6558, Train Acc: 0.6312, Loss: 0.7062, Test Acc: 0.5200\n",
      "Epoch: 154, Loss: 0.6558, Train Acc: 0.6250, Loss: 0.7331, Test Acc: 0.4950\n",
      "Epoch: 155, Loss: 0.6543, Train Acc: 0.6313, Loss: 0.7405, Test Acc: 0.5450\n",
      "Epoch: 156, Loss: 0.6550, Train Acc: 0.6500, Loss: 0.7070, Test Acc: 0.5200\n",
      "Epoch: 157, Loss: 0.6535, Train Acc: 0.6312, Loss: 0.7343, Test Acc: 0.4950\n",
      "Epoch: 158, Loss: 0.6523, Train Acc: 0.6438, Loss: 0.7265, Test Acc: 0.5450\n",
      "Epoch: 159, Loss: 0.6524, Train Acc: 0.6375, Loss: 0.7154, Test Acc: 0.5450\n",
      "Epoch: 160, Loss: 0.6515, Train Acc: 0.6313, Loss: 0.7205, Test Acc: 0.5450\n",
      "Epoch: 161, Loss: 0.6484, Train Acc: 0.6562, Loss: 0.7286, Test Acc: 0.5450\n",
      "Epoch: 162, Loss: 0.6515, Train Acc: 0.6313, Loss: 0.7242, Test Acc: 0.5450\n",
      "Epoch: 163, Loss: 0.6487, Train Acc: 0.6313, Loss: 0.7074, Test Acc: 0.5450\n",
      "Epoch: 164, Loss: 0.6458, Train Acc: 0.6500, Loss: 0.7315, Test Acc: 0.5450\n",
      "Epoch: 165, Loss: 0.6473, Train Acc: 0.6500, Loss: 0.7224, Test Acc: 0.5450\n",
      "Epoch: 166, Loss: 0.6460, Train Acc: 0.6375, Loss: 0.7223, Test Acc: 0.5450\n",
      "Epoch: 167, Loss: 0.6437, Train Acc: 0.6562, Loss: 0.7137, Test Acc: 0.5450\n",
      "Epoch: 168, Loss: 0.6438, Train Acc: 0.6750, Loss: 0.7382, Test Acc: 0.5450\n",
      "Epoch: 169, Loss: 0.6453, Train Acc: 0.6313, Loss: 0.7230, Test Acc: 0.5450\n",
      "Epoch: 170, Loss: 0.6413, Train Acc: 0.6313, Loss: 0.7005, Test Acc: 0.5200\n",
      "Epoch: 171, Loss: 0.6433, Train Acc: 0.6687, Loss: 0.7465, Test Acc: 0.5450\n",
      "Epoch: 172, Loss: 0.6436, Train Acc: 0.6375, Loss: 0.7124, Test Acc: 0.5200\n",
      "Epoch: 173, Loss: 0.6375, Train Acc: 0.6687, Loss: 0.7334, Test Acc: 0.5450\n",
      "Epoch: 174, Loss: 0.6397, Train Acc: 0.6687, Loss: 0.7579, Test Acc: 0.5050\n",
      "Epoch: 175, Loss: 0.6394, Train Acc: 0.6375, Loss: 0.7082, Test Acc: 0.5200\n",
      "Epoch: 176, Loss: 0.6347, Train Acc: 0.6625, Loss: 0.7457, Test Acc: 0.5050\n",
      "Epoch: 177, Loss: 0.6362, Train Acc: 0.6562, Loss: 0.7353, Test Acc: 0.5200\n",
      "Epoch: 178, Loss: 0.6356, Train Acc: 0.6375, Loss: 0.7266, Test Acc: 0.5200\n",
      "Epoch: 179, Loss: 0.6323, Train Acc: 0.6562, Loss: 0.7173, Test Acc: 0.5200\n",
      "Epoch: 180, Loss: 0.6350, Train Acc: 0.6438, Loss: 0.7911, Test Acc: 0.5050\n",
      "Epoch: 181, Loss: 0.6400, Train Acc: 0.6687, Loss: 0.6809, Test Acc: 0.4700\n",
      "Epoch: 182, Loss: 0.6275, Train Acc: 0.6250, Loss: 0.7873, Test Acc: 0.5050\n",
      "Epoch: 183, Loss: 0.6356, Train Acc: 0.6625, Loss: 0.7217, Test Acc: 0.5200\n",
      "Epoch: 184, Loss: 0.6329, Train Acc: 0.6562, Loss: 0.7170, Test Acc: 0.4700\n",
      "Epoch: 185, Loss: 0.6257, Train Acc: 0.6375, Loss: 0.7523, Test Acc: 0.4800\n",
      "Epoch: 186, Loss: 0.6314, Train Acc: 0.6625, Loss: 0.7158, Test Acc: 0.5200\n",
      "Epoch: 187, Loss: 0.6307, Train Acc: 0.6562, Loss: 0.7140, Test Acc: 0.4800\n",
      "Epoch: 188, Loss: 0.6215, Train Acc: 0.6625, Loss: 0.7387, Test Acc: 0.4800\n",
      "Epoch: 189, Loss: 0.6235, Train Acc: 0.6500, Loss: 0.7367, Test Acc: 0.4800\n",
      "Epoch: 190, Loss: 0.6301, Train Acc: 0.6750, Loss: 0.7121, Test Acc: 0.5200\n",
      "Epoch: 191, Loss: 0.6186, Train Acc: 0.6625, Loss: 0.7180, Test Acc: 0.4800\n",
      "Epoch: 192, Loss: 0.6206, Train Acc: 0.6500, Loss: 0.7649, Test Acc: 0.4800\n",
      "Epoch: 193, Loss: 0.6192, Train Acc: 0.6687, Loss: 0.7248, Test Acc: 0.4550\n",
      "Epoch: 194, Loss: 0.6133, Train Acc: 0.6562, Loss: 0.7668, Test Acc: 0.4800\n",
      "Epoch: 195, Loss: 0.6173, Train Acc: 0.6625, Loss: 0.7304, Test Acc: 0.4800\n",
      "Epoch: 196, Loss: 0.6130, Train Acc: 0.6562, Loss: 0.7490, Test Acc: 0.4550\n",
      "Epoch: 197, Loss: 0.6109, Train Acc: 0.6687, Loss: 0.7499, Test Acc: 0.4550\n",
      "Epoch: 198, Loss: 0.6090, Train Acc: 0.6687, Loss: 0.7629, Test Acc: 0.4550\n",
      "Epoch: 199, Loss: 0.6074, Train Acc: 0.6687, Loss: 0.7691, Test Acc: 0.4550\n",
      "Epoch: 200, Loss: 0.6098, Train Acc: 0.6625, Loss: 0.7556, Test Acc: 0.4550\n",
      "Epoch: 201, Loss: 0.6050, Train Acc: 0.6812, Loss: 0.7256, Test Acc: 0.5050\n",
      "Epoch: 202, Loss: 0.6024, Train Acc: 0.6750, Loss: 0.7885, Test Acc: 0.4550\n",
      "Epoch: 203, Loss: 0.6071, Train Acc: 0.6937, Loss: 0.7316, Test Acc: 0.5050\n",
      "Epoch: 204, Loss: 0.5993, Train Acc: 0.6750, Loss: 0.7611, Test Acc: 0.4800\n",
      "Epoch: 205, Loss: 0.5996, Train Acc: 0.6562, Loss: 0.7752, Test Acc: 0.4800\n",
      "Epoch: 206, Loss: 0.6027, Train Acc: 0.6938, Loss: 0.7339, Test Acc: 0.5050\n",
      "Epoch: 207, Loss: 0.5968, Train Acc: 0.6938, Loss: 0.7297, Test Acc: 0.5050\n",
      "Epoch: 208, Loss: 0.5920, Train Acc: 0.6688, Loss: 0.7795, Test Acc: 0.4800\n",
      "Epoch: 209, Loss: 0.5947, Train Acc: 0.6625, Loss: 0.7986, Test Acc: 0.4550\n",
      "Epoch: 210, Loss: 0.5985, Train Acc: 0.7000, Loss: 0.7370, Test Acc: 0.5050\n",
      "Epoch: 211, Loss: 0.5897, Train Acc: 0.6813, Loss: 0.7777, Test Acc: 0.4800\n",
      "Epoch: 212, Loss: 0.5888, Train Acc: 0.7063, Loss: 0.7343, Test Acc: 0.5050\n",
      "Epoch: 213, Loss: 0.5845, Train Acc: 0.7000, Loss: 0.8011, Test Acc: 0.4800\n",
      "Epoch: 214, Loss: 0.5869, Train Acc: 0.6688, Loss: 0.7828, Test Acc: 0.4800\n",
      "Epoch: 215, Loss: 0.5861, Train Acc: 0.6750, Loss: 0.8185, Test Acc: 0.4550\n",
      "Epoch: 216, Loss: 0.5882, Train Acc: 0.7062, Loss: 0.7626, Test Acc: 0.5050\n",
      "Epoch: 217, Loss: 0.5828, Train Acc: 0.7062, Loss: 0.7480, Test Acc: 0.5050\n",
      "Epoch: 218, Loss: 0.5779, Train Acc: 0.7063, Loss: 0.7335, Test Acc: 0.5050\n",
      "Epoch: 219, Loss: 0.5759, Train Acc: 0.7125, Loss: 0.7197, Test Acc: 0.5050\n",
      "Epoch: 220, Loss: 0.5729, Train Acc: 0.7188, Loss: 0.7888, Test Acc: 0.4800\n",
      "Epoch: 221, Loss: 0.5711, Train Acc: 0.7250, Loss: 0.7752, Test Acc: 0.4800\n",
      "Epoch: 222, Loss: 0.5715, Train Acc: 0.7125, Loss: 0.8464, Test Acc: 0.4800\n",
      "Epoch: 223, Loss: 0.5711, Train Acc: 0.7000, Loss: 0.8130, Test Acc: 0.4800\n",
      "Epoch: 224, Loss: 0.5740, Train Acc: 0.7000, Loss: 0.8502, Test Acc: 0.4800\n",
      "Epoch: 225, Loss: 0.5805, Train Acc: 0.6875, Loss: 0.7270, Test Acc: 0.4550\n",
      "Epoch: 226, Loss: 0.5704, Train Acc: 0.7375, Loss: 0.7385, Test Acc: 0.5050\n",
      "Epoch: 227, Loss: 0.5610, Train Acc: 0.7187, Loss: 0.7909, Test Acc: 0.5050\n",
      "Epoch: 228, Loss: 0.5571, Train Acc: 0.7375, Loss: 0.8039, Test Acc: 0.4800\n",
      "Epoch: 229, Loss: 0.5569, Train Acc: 0.7313, Loss: 0.8090, Test Acc: 0.4800\n",
      "Epoch: 230, Loss: 0.5518, Train Acc: 0.7438, Loss: 0.8524, Test Acc: 0.4800\n",
      "Epoch: 231, Loss: 0.5529, Train Acc: 0.7188, Loss: 0.8533, Test Acc: 0.4800\n",
      "Epoch: 232, Loss: 0.5509, Train Acc: 0.7188, Loss: 0.8593, Test Acc: 0.4800\n",
      "Epoch: 233, Loss: 0.5472, Train Acc: 0.7375, Loss: 0.8327, Test Acc: 0.4800\n",
      "Epoch: 234, Loss: 0.5501, Train Acc: 0.7312, Loss: 0.8758, Test Acc: 0.4800\n",
      "Epoch: 235, Loss: 0.5492, Train Acc: 0.7187, Loss: 0.7501, Test Acc: 0.4800\n",
      "Epoch: 236, Loss: 0.5505, Train Acc: 0.7125, Loss: 0.7049, Test Acc: 0.4800\n",
      "Epoch: 237, Loss: 0.5459, Train Acc: 0.7125, Loss: 0.6712, Test Acc: 0.4050\n",
      "Epoch: 238, Loss: 0.5472, Train Acc: 0.7500, Loss: 0.7633, Test Acc: 0.4800\n",
      "Epoch: 239, Loss: 0.5378, Train Acc: 0.7312, Loss: 0.9190, Test Acc: 0.4900\n",
      "Epoch: 240, Loss: 0.5451, Train Acc: 0.6250, Loss: 1.1016, Test Acc: 0.4400\n",
      "Epoch: 241, Loss: 0.5859, Train Acc: 0.6812, Loss: 0.9480, Test Acc: 0.4800\n",
      "Epoch: 242, Loss: 0.5661, Train Acc: 0.7500, Loss: 0.8824, Test Acc: 0.4800\n",
      "Epoch: 243, Loss: 0.5643, Train Acc: 0.7250, Loss: 0.7459, Test Acc: 0.4550\n",
      "Epoch: 244, Loss: 0.5387, Train Acc: 0.7375, Loss: 0.9117, Test Acc: 0.4800\n",
      "Epoch: 245, Loss: 0.5469, Train Acc: 0.7063, Loss: 0.8761, Test Acc: 0.5050\n",
      "Epoch: 246, Loss: 0.5510, Train Acc: 0.6875, Loss: 1.0207, Test Acc: 0.4400\n",
      "Epoch: 247, Loss: 0.5475, Train Acc: 0.7250, Loss: 0.8298, Test Acc: 0.4300\n",
      "Epoch: 248, Loss: 0.5377, Train Acc: 0.7312, Loss: 0.7410, Test Acc: 0.4300\n",
      "Epoch: 249, Loss: 0.5318, Train Acc: 0.7625, Loss: 0.7224, Test Acc: 0.4800\n",
      "Epoch: 250, Loss: 0.5248, Train Acc: 0.7562, Loss: 0.9037, Test Acc: 0.4800\n",
      "Epoch: 251, Loss: 0.5182, Train Acc: 0.7312, Loss: 0.9619, Test Acc: 0.5300\n",
      "Epoch: 252, Loss: 0.5231, Train Acc: 0.7625, Loss: 0.9569, Test Acc: 0.5050\n",
      "Epoch: 253, Loss: 0.5135, Train Acc: 0.7437, Loss: 0.8160, Test Acc: 0.4300\n",
      "Epoch: 254, Loss: 0.5036, Train Acc: 0.7187, Loss: 0.6950, Test Acc: 0.4550\n",
      "Epoch: 255, Loss: 0.4979, Train Acc: 0.7312, Loss: 0.8285, Test Acc: 0.4300\n",
      "Epoch: 256, Loss: 0.4964, Train Acc: 0.7875, Loss: 0.8854, Test Acc: 0.5050\n",
      "Epoch: 257, Loss: 0.4854, Train Acc: 0.7625, Loss: 1.1885, Test Acc: 0.4000\n",
      "Epoch: 258, Loss: 0.4870, Train Acc: 0.7187, Loss: 0.9785, Test Acc: 0.4650\n",
      "Epoch: 259, Loss: 0.4949, Train Acc: 0.7500, Loss: 1.2993, Test Acc: 0.4250\n",
      "Epoch: 260, Loss: 0.4921, Train Acc: 0.8000, Loss: 0.9565, Test Acc: 0.4300\n",
      "Epoch: 261, Loss: 0.4881, Train Acc: 0.7562, Loss: 0.6632, Test Acc: 0.5050\n",
      "Epoch: 262, Loss: 0.5029, Train Acc: 0.7063, Loss: 0.6610, Test Acc: 0.4550\n",
      "Epoch: 263, Loss: 0.5251, Train Acc: 0.7188, Loss: 0.6138, Test Acc: 0.4800\n",
      "Epoch: 264, Loss: 0.5060, Train Acc: 0.7500, Loss: 1.1789, Test Acc: 0.4400\n",
      "Epoch: 265, Loss: 0.4950, Train Acc: 0.6937, Loss: 1.3728, Test Acc: 0.4000\n",
      "Epoch: 266, Loss: 0.5087, Train Acc: 0.7937, Loss: 1.1938, Test Acc: 0.3650\n",
      "Epoch: 267, Loss: 0.4679, Train Acc: 0.7562, Loss: 0.7896, Test Acc: 0.3800\n",
      "Epoch: 268, Loss: 0.4694, Train Acc: 0.7375, Loss: 0.8380, Test Acc: 0.4300\n",
      "Epoch: 269, Loss: 0.4872, Train Acc: 0.8000, Loss: 1.0736, Test Acc: 0.5050\n",
      "Epoch: 270, Loss: 0.4771, Train Acc: 0.7625, Loss: 1.4509, Test Acc: 0.4250\n",
      "Epoch: 271, Loss: 0.4904, Train Acc: 0.8125, Loss: 1.1391, Test Acc: 0.4650\n",
      "Epoch: 272, Loss: 0.4451, Train Acc: 0.7812, Loss: 1.0246, Test Acc: 0.4050\n",
      "Epoch: 273, Loss: 0.4427, Train Acc: 0.7875, Loss: 0.6987, Test Acc: 0.4550\n",
      "Epoch: 274, Loss: 0.4387, Train Acc: 0.8250, Loss: 1.1283, Test Acc: 0.4050\n",
      "Epoch: 275, Loss: 0.4265, Train Acc: 0.8188, Loss: 1.5316, Test Acc: 0.3750\n",
      "Epoch: 276, Loss: 0.4186, Train Acc: 0.8250, Loss: 1.3016, Test Acc: 0.4150\n",
      "Epoch: 277, Loss: 0.4086, Train Acc: 0.8313, Loss: 1.1605, Test Acc: 0.3650\n",
      "Epoch: 278, Loss: 0.4192, Train Acc: 0.7875, Loss: 1.1915, Test Acc: 0.3150\n",
      "Epoch: 279, Loss: 0.4292, Train Acc: 0.7625, Loss: 0.6924, Test Acc: 0.5200\n",
      "Epoch: 280, Loss: 0.4558, Train Acc: 0.7875, Loss: 0.6662, Test Acc: 0.5200\n",
      "Epoch: 281, Loss: 0.4382, Train Acc: 0.8062, Loss: 1.6127, Test Acc: 0.2750\n",
      "Epoch: 282, Loss: 0.4366, Train Acc: 0.7437, Loss: 1.6845, Test Acc: 0.3750\n",
      "Epoch: 283, Loss: 0.4588, Train Acc: 0.8250, Loss: 1.8251, Test Acc: 0.3250\n",
      "Epoch: 284, Loss: 0.4270, Train Acc: 0.7937, Loss: 0.9854, Test Acc: 0.4050\n",
      "Epoch: 285, Loss: 0.4122, Train Acc: 0.8375, Loss: 1.0282, Test Acc: 0.4450\n",
      "Epoch: 286, Loss: 0.3996, Train Acc: 0.8438, Loss: 1.8228, Test Acc: 0.3500\n",
      "Epoch: 287, Loss: 0.3830, Train Acc: 0.8375, Loss: 1.4911, Test Acc: 0.4150\n",
      "Epoch: 288, Loss: 0.3739, Train Acc: 0.8187, Loss: 0.8816, Test Acc: 0.4450\n",
      "Epoch: 289, Loss: 0.3866, Train Acc: 0.8187, Loss: 1.0168, Test Acc: 0.4700\n",
      "Epoch: 290, Loss: 0.3804, Train Acc: 0.8688, Loss: 1.5300, Test Acc: 0.3900\n",
      "Epoch: 291, Loss: 0.3699, Train Acc: 0.8375, Loss: 2.0150, Test Acc: 0.3000\n",
      "Epoch: 292, Loss: 0.3778, Train Acc: 0.8000, Loss: 1.8477, Test Acc: 0.3650\n",
      "Epoch: 293, Loss: 0.3842, Train Acc: 0.7562, Loss: 1.9860, Test Acc: 0.4650\n",
      "Epoch: 294, Loss: 0.4144, Train Acc: 0.7688, Loss: 2.1347, Test Acc: 0.3750\n",
      "Epoch: 295, Loss: 0.4052, Train Acc: 0.8438, Loss: 1.7014, Test Acc: 0.3500\n",
      "Epoch: 296, Loss: 0.4324, Train Acc: 0.7812, Loss: 0.7080, Test Acc: 0.4700\n",
      "Epoch: 297, Loss: 0.4862, Train Acc: 0.8375, Loss: 1.0841, Test Acc: 0.4300\n",
      "Epoch: 298, Loss: 0.4223, Train Acc: 0.7688, Loss: 2.5412, Test Acc: 0.3750\n",
      "Epoch: 299, Loss: 0.4218, Train Acc: 0.8562, Loss: 1.2453, Test Acc: 0.4300\n",
      "Epoch: 300, Loss: 0.3645, Train Acc: 0.7937, Loss: 1.0915, Test Acc: 0.5550\n",
      "Epoch: 301, Loss: 0.3748, Train Acc: 0.8562, Loss: 1.3411, Test Acc: 0.4550\n",
      "Epoch: 302, Loss: 0.3460, Train Acc: 0.8563, Loss: 1.9843, Test Acc: 0.3250\n",
      "Epoch: 303, Loss: 0.3463, Train Acc: 0.8563, Loss: 1.7809, Test Acc: 0.3250\n",
      "Epoch: 304, Loss: 0.3279, Train Acc: 0.8750, Loss: 1.0201, Test Acc: 0.4550\n",
      "Epoch: 305, Loss: 0.3240, Train Acc: 0.8750, Loss: 1.2500, Test Acc: 0.4800\n",
      "Epoch: 306, Loss: 0.3225, Train Acc: 0.8625, Loss: 1.4766, Test Acc: 0.4150\n",
      "Epoch: 307, Loss: 0.3302, Train Acc: 0.8875, Loss: 1.3231, Test Acc: 0.4800\n",
      "Epoch: 308, Loss: 0.3021, Train Acc: 0.8812, Loss: 1.9455, Test Acc: 0.3000\n",
      "Epoch: 309, Loss: 0.3141, Train Acc: 0.8625, Loss: 2.1598, Test Acc: 0.3250\n",
      "Epoch: 310, Loss: 0.3002, Train Acc: 0.8813, Loss: 1.8949, Test Acc: 0.4300\n",
      "Epoch: 311, Loss: 0.2855, Train Acc: 0.8875, Loss: 1.5811, Test Acc: 0.4550\n",
      "Epoch: 312, Loss: 0.2807, Train Acc: 0.8875, Loss: 1.8459, Test Acc: 0.3900\n",
      "Epoch: 313, Loss: 0.3061, Train Acc: 0.8813, Loss: 1.1187, Test Acc: 0.4800\n",
      "Epoch: 314, Loss: 0.3496, Train Acc: 0.8625, Loss: 0.6634, Test Acc: 0.4800\n",
      "Epoch: 315, Loss: 0.3347, Train Acc: 0.8875, Loss: 2.2664, Test Acc: 0.3650\n",
      "Epoch: 316, Loss: 0.3359, Train Acc: 0.8625, Loss: 1.5570, Test Acc: 0.3750\n",
      "Epoch: 317, Loss: 0.3184, Train Acc: 0.9062, Loss: 1.2015, Test Acc: 0.4300\n",
      "Epoch: 318, Loss: 0.2840, Train Acc: 0.9000, Loss: 2.0250, Test Acc: 0.3000\n",
      "Epoch: 319, Loss: 0.2769, Train Acc: 0.9187, Loss: 1.2698, Test Acc: 0.4300\n",
      "Epoch: 320, Loss: 0.2751, Train Acc: 0.9187, Loss: 1.8393, Test Acc: 0.4550\n",
      "Epoch: 321, Loss: 0.2631, Train Acc: 0.8813, Loss: 2.3219, Test Acc: 0.3500\n",
      "Epoch: 322, Loss: 0.2642, Train Acc: 0.9125, Loss: 1.6804, Test Acc: 0.4550\n",
      "Epoch: 323, Loss: 0.2414, Train Acc: 0.9187, Loss: 1.9244, Test Acc: 0.4300\n",
      "Epoch: 324, Loss: 0.2392, Train Acc: 0.8812, Loss: 2.6573, Test Acc: 0.3000\n",
      "Epoch: 325, Loss: 0.2603, Train Acc: 0.9000, Loss: 2.4737, Test Acc: 0.3250\n",
      "Epoch: 326, Loss: 0.2441, Train Acc: 0.8938, Loss: 2.2706, Test Acc: 0.4300\n",
      "Epoch: 327, Loss: 0.2536, Train Acc: 0.8438, Loss: 2.4882, Test Acc: 0.3400\n",
      "Epoch: 328, Loss: 0.2739, Train Acc: 0.8438, Loss: 3.3350, Test Acc: 0.2750\n",
      "Epoch: 329, Loss: 0.2754, Train Acc: 0.8438, Loss: 3.2114, Test Acc: 0.3250\n",
      "Epoch: 330, Loss: 0.2687, Train Acc: 0.7875, Loss: 3.1378, Test Acc: 0.4400\n",
      "Epoch: 331, Loss: 0.3154, Train Acc: 0.7750, Loss: 3.1317, Test Acc: 0.5050\n",
      "Epoch: 332, Loss: 0.3215, Train Acc: 0.7937, Loss: 3.4525, Test Acc: 0.3250\n",
      "Epoch: 333, Loss: 0.3387, Train Acc: 0.9375, Loss: 1.6405, Test Acc: 0.4550\n",
      "Epoch: 334, Loss: 0.4494, Train Acc: 0.8687, Loss: 2.0413, Test Acc: 0.5050\n",
      "Epoch: 335, Loss: 0.4539, Train Acc: 0.8875, Loss: 1.6053, Test Acc: 0.4150\n",
      "Epoch: 336, Loss: 0.3722, Train Acc: 0.9125, Loss: 1.1345, Test Acc: 0.4300\n",
      "Epoch: 337, Loss: 0.3055, Train Acc: 0.9187, Loss: 1.3181, Test Acc: 0.4300\n",
      "Epoch: 338, Loss: 0.2522, Train Acc: 0.8937, Loss: 2.1361, Test Acc: 0.3650\n",
      "Epoch: 339, Loss: 0.2579, Train Acc: 0.9563, Loss: 1.6369, Test Acc: 0.3900\n",
      "Epoch: 340, Loss: 0.2197, Train Acc: 0.9500, Loss: 1.5887, Test Acc: 0.3900\n",
      "Epoch: 341, Loss: 0.2275, Train Acc: 0.9125, Loss: 0.8340, Test Acc: 0.5050\n",
      "Epoch: 342, Loss: 0.3015, Train Acc: 0.8438, Loss: 1.1104, Test Acc: 0.5700\n",
      "Epoch: 343, Loss: 0.2863, Train Acc: 0.8875, Loss: 1.1674, Test Acc: 0.4800\n",
      "Epoch: 344, Loss: 0.2828, Train Acc: 0.9375, Loss: 2.3584, Test Acc: 0.4050\n",
      "Epoch: 345, Loss: 0.2285, Train Acc: 0.8937, Loss: 3.1201, Test Acc: 0.3250\n",
      "Epoch: 346, Loss: 0.2394, Train Acc: 0.8812, Loss: 2.6972, Test Acc: 0.4150\n",
      "Epoch: 347, Loss: 0.2209, Train Acc: 0.9562, Loss: 1.8663, Test Acc: 0.3900\n",
      "Epoch: 348, Loss: 0.1940, Train Acc: 0.9125, Loss: 0.8681, Test Acc: 0.5700\n",
      "Epoch: 349, Loss: 0.2305, Train Acc: 0.8937, Loss: 1.1824, Test Acc: 0.5550\n",
      "Epoch: 350, Loss: 0.2137, Train Acc: 0.9500, Loss: 1.9595, Test Acc: 0.4550\n",
      "Epoch: 351, Loss: 0.1782, Train Acc: 0.9125, Loss: 2.9775, Test Acc: 0.3250\n",
      "Epoch: 352, Loss: 0.1904, Train Acc: 0.9563, Loss: 2.5688, Test Acc: 0.3900\n",
      "Epoch: 353, Loss: 0.1730, Train Acc: 0.9688, Loss: 1.8270, Test Acc: 0.4150\n",
      "Epoch: 354, Loss: 0.1801, Train Acc: 0.9250, Loss: 0.9684, Test Acc: 0.5450\n",
      "Epoch: 355, Loss: 0.2168, Train Acc: 0.8500, Loss: 0.7394, Test Acc: 0.5850\n",
      "Epoch: 356, Loss: 0.2385, Train Acc: 0.9500, Loss: 1.9848, Test Acc: 0.4300\n",
      "Epoch: 357, Loss: 0.1812, Train Acc: 0.8812, Loss: 3.7050, Test Acc: 0.3250\n",
      "Epoch: 358, Loss: 0.2156, Train Acc: 0.9187, Loss: 3.2427, Test Acc: 0.3900\n",
      "Epoch: 359, Loss: 0.1849, Train Acc: 0.9688, Loss: 1.9437, Test Acc: 0.5450\n",
      "Epoch: 360, Loss: 0.1789, Train Acc: 0.9250, Loss: 1.0358, Test Acc: 0.5700\n",
      "Epoch: 361, Loss: 0.2275, Train Acc: 0.8687, Loss: 0.9621, Test Acc: 0.6200\n",
      "Epoch: 362, Loss: 0.2004, Train Acc: 0.9625, Loss: 2.1152, Test Acc: 0.3650\n",
      "Epoch: 363, Loss: 0.1680, Train Acc: 0.9250, Loss: 3.5522, Test Acc: 0.3750\n",
      "Epoch: 364, Loss: 0.1672, Train Acc: 0.9688, Loss: 2.5208, Test Acc: 0.4800\n",
      "Epoch: 365, Loss: 0.1572, Train Acc: 0.9500, Loss: 1.3820, Test Acc: 0.5200\n",
      "Epoch: 366, Loss: 0.1713, Train Acc: 0.9312, Loss: 0.9393, Test Acc: 0.5700\n",
      "Epoch: 367, Loss: 0.1800, Train Acc: 0.9500, Loss: 1.8754, Test Acc: 0.4800\n",
      "Epoch: 368, Loss: 0.1538, Train Acc: 0.9312, Loss: 3.6386, Test Acc: 0.3500\n",
      "Epoch: 369, Loss: 0.1477, Train Acc: 0.9688, Loss: 3.2641, Test Acc: 0.3900\n",
      "Epoch: 370, Loss: 0.1402, Train Acc: 0.9563, Loss: 1.7919, Test Acc: 0.5450\n",
      "Epoch: 371, Loss: 0.1517, Train Acc: 0.9437, Loss: 0.9380, Test Acc: 0.5200\n",
      "Epoch: 372, Loss: 0.1630, Train Acc: 0.9750, Loss: 1.7370, Test Acc: 0.4300\n",
      "Epoch: 373, Loss: 0.1383, Train Acc: 0.9062, Loss: 3.5861, Test Acc: 0.3900\n",
      "Epoch: 374, Loss: 0.1679, Train Acc: 0.9312, Loss: 3.8052, Test Acc: 0.3500\n",
      "Epoch: 375, Loss: 0.1550, Train Acc: 0.9500, Loss: 2.1740, Test Acc: 0.5700\n",
      "Epoch: 376, Loss: 0.1734, Train Acc: 0.9750, Loss: 1.9677, Test Acc: 0.5450\n",
      "Epoch: 377, Loss: 0.1418, Train Acc: 0.9187, Loss: 0.9576, Test Acc: 0.4950\n",
      "Epoch: 378, Loss: 0.1700, Train Acc: 0.9750, Loss: 1.8682, Test Acc: 0.4800\n",
      "Epoch: 379, Loss: 0.1209, Train Acc: 0.9437, Loss: 3.5585, Test Acc: 0.3250\n",
      "Epoch: 380, Loss: 0.1360, Train Acc: 0.9563, Loss: 3.0888, Test Acc: 0.3900\n",
      "Epoch: 381, Loss: 0.1251, Train Acc: 0.9812, Loss: 2.5690, Test Acc: 0.4150\n",
      "Epoch: 382, Loss: 0.1182, Train Acc: 0.9625, Loss: 1.4339, Test Acc: 0.5700\n",
      "Epoch: 383, Loss: 0.1433, Train Acc: 0.9563, Loss: 1.5978, Test Acc: 0.4650\n",
      "Epoch: 384, Loss: 0.1259, Train Acc: 0.9812, Loss: 2.2498, Test Acc: 0.4550\n",
      "Epoch: 385, Loss: 0.1063, Train Acc: 0.9625, Loss: 3.5176, Test Acc: 0.4000\n",
      "Epoch: 386, Loss: 0.1164, Train Acc: 0.9812, Loss: 3.2945, Test Acc: 0.4150\n",
      "Epoch: 387, Loss: 0.1152, Train Acc: 0.9500, Loss: 2.4533, Test Acc: 0.5550\n",
      "Epoch: 388, Loss: 0.1267, Train Acc: 0.9688, Loss: 2.2949, Test Acc: 0.3900\n",
      "Epoch: 389, Loss: 0.0980, Train Acc: 0.9688, Loss: 1.4471, Test Acc: 0.4700\n",
      "Epoch: 390, Loss: 0.1099, Train Acc: 0.9812, Loss: 2.7028, Test Acc: 0.4300\n",
      "Epoch: 391, Loss: 0.1042, Train Acc: 0.9437, Loss: 1.1004, Test Acc: 0.5450\n",
      "Epoch: 392, Loss: 0.1216, Train Acc: 0.9688, Loss: 2.4679, Test Acc: 0.4150\n",
      "Epoch: 393, Loss: 0.1037, Train Acc: 0.9812, Loss: 3.7524, Test Acc: 0.3900\n",
      "Epoch: 394, Loss: 0.0985, Train Acc: 0.9688, Loss: 3.8849, Test Acc: 0.3900\n",
      "Epoch: 395, Loss: 0.1004, Train Acc: 0.9875, Loss: 2.9826, Test Acc: 0.4400\n",
      "Epoch: 396, Loss: 0.0945, Train Acc: 0.9625, Loss: 1.6555, Test Acc: 0.5450\n",
      "Epoch: 397, Loss: 0.1027, Train Acc: 0.9937, Loss: 2.5263, Test Acc: 0.4550\n",
      "Epoch: 398, Loss: 0.0844, Train Acc: 0.9375, Loss: 4.2433, Test Acc: 0.3900\n",
      "Epoch: 399, Loss: 0.1349, Train Acc: 0.8812, Loss: 5.7822, Test Acc: 0.3500\n",
      "Epoch: 400, Loss: 0.1871, Train Acc: 0.8687, Loss: 5.1250, Test Acc: 0.3500\n",
      "Epoch: 401, Loss: 0.2038, Train Acc: 0.7438, Loss: 5.0133, Test Acc: 0.5050\n",
      "Epoch: 402, Loss: 0.5170, Train Acc: 0.7438, Loss: 7.1053, Test Acc: 0.3250\n",
      "Epoch: 403, Loss: 0.5881, Train Acc: 0.8438, Loss: 1.8600, Test Acc: 0.6450\n",
      "Epoch: 404, Loss: 0.5472, Train Acc: 0.8000, Loss: 4.3449, Test Acc: 0.3650\n",
      "Epoch: 405, Loss: 0.3610, Train Acc: 0.9375, Loss: 1.8154, Test Acc: 0.5450\n",
      "Epoch: 406, Loss: 0.2386, Train Acc: 0.8500, Loss: 1.8129, Test Acc: 0.4950\n",
      "Epoch: 407, Loss: 0.2436, Train Acc: 0.9625, Loss: 1.9396, Test Acc: 0.5050\n",
      "Epoch: 408, Loss: 0.1872, Train Acc: 0.9688, Loss: 1.7584, Test Acc: 0.4150\n",
      "Epoch: 409, Loss: 0.1529, Train Acc: 0.9688, Loss: 2.4404, Test Acc: 0.3650\n",
      "Epoch: 410, Loss: 0.1286, Train Acc: 0.9625, Loss: 1.6048, Test Acc: 0.5450\n",
      "Epoch: 411, Loss: 0.1258, Train Acc: 0.9750, Loss: 2.9198, Test Acc: 0.3900\n",
      "Epoch: 412, Loss: 0.1027, Train Acc: 0.9875, Loss: 2.8875, Test Acc: 0.4650\n",
      "Epoch: 413, Loss: 0.0982, Train Acc: 0.9875, Loss: 1.8637, Test Acc: 0.5200\n",
      "Epoch: 414, Loss: 0.0890, Train Acc: 0.9750, Loss: 3.4036, Test Acc: 0.4150\n",
      "Epoch: 415, Loss: 0.0803, Train Acc: 0.9875, Loss: 2.0098, Test Acc: 0.5450\n",
      "Epoch: 416, Loss: 0.0898, Train Acc: 0.9875, Loss: 3.0491, Test Acc: 0.4150\n",
      "Epoch: 417, Loss: 0.0740, Train Acc: 0.9937, Loss: 2.7204, Test Acc: 0.4650\n",
      "Epoch: 418, Loss: 0.0681, Train Acc: 0.9937, Loss: 2.3062, Test Acc: 0.4650\n",
      "Epoch: 419, Loss: 0.0656, Train Acc: 0.9875, Loss: 3.6738, Test Acc: 0.4150\n",
      "Epoch: 420, Loss: 0.0643, Train Acc: 1.0000, Loss: 2.0477, Test Acc: 0.5700\n",
      "Epoch: 421, Loss: 0.0613, Train Acc: 1.0000, Loss: 3.3366, Test Acc: 0.4150\n",
      "Epoch: 422, Loss: 0.0553, Train Acc: 1.0000, Loss: 2.8485, Test Acc: 0.4650\n",
      "Epoch: 423, Loss: 0.0563, Train Acc: 1.0000, Loss: 2.2844, Test Acc: 0.4900\n",
      "Epoch: 424, Loss: 0.0546, Train Acc: 1.0000, Loss: 3.8586, Test Acc: 0.4150\n",
      "Epoch: 425, Loss: 0.0540, Train Acc: 1.0000, Loss: 1.8567, Test Acc: 0.5700\n",
      "Epoch: 426, Loss: 0.0554, Train Acc: 1.0000, Loss: 3.0660, Test Acc: 0.4150\n",
      "Epoch: 427, Loss: 0.0495, Train Acc: 1.0000, Loss: 3.2476, Test Acc: 0.4650\n",
      "Epoch: 428, Loss: 0.0505, Train Acc: 1.0000, Loss: 1.9852, Test Acc: 0.4900\n",
      "Epoch: 429, Loss: 0.0512, Train Acc: 1.0000, Loss: 4.1013, Test Acc: 0.4150\n",
      "Epoch: 430, Loss: 0.0464, Train Acc: 1.0000, Loss: 1.9952, Test Acc: 0.5300\n",
      "Epoch: 431, Loss: 0.0514, Train Acc: 1.0000, Loss: 3.0867, Test Acc: 0.4150\n",
      "Epoch: 432, Loss: 0.0471, Train Acc: 1.0000, Loss: 3.6598, Test Acc: 0.4650\n",
      "Epoch: 433, Loss: 0.0477, Train Acc: 0.9937, Loss: 1.9165, Test Acc: 0.4900\n",
      "Epoch: 434, Loss: 0.0519, Train Acc: 1.0000, Loss: 3.4247, Test Acc: 0.4400\n",
      "Epoch: 435, Loss: 0.0396, Train Acc: 1.0000, Loss: 2.9196, Test Acc: 0.4650\n",
      "Epoch: 436, Loss: 0.0389, Train Acc: 1.0000, Loss: 2.5945, Test Acc: 0.4900\n",
      "Epoch: 437, Loss: 0.0347, Train Acc: 1.0000, Loss: 4.2742, Test Acc: 0.4150\n",
      "Epoch: 438, Loss: 0.0393, Train Acc: 1.0000, Loss: 1.9593, Test Acc: 0.4900\n",
      "Epoch: 439, Loss: 0.0394, Train Acc: 1.0000, Loss: 3.3893, Test Acc: 0.4400\n",
      "Epoch: 440, Loss: 0.0349, Train Acc: 1.0000, Loss: 3.2211, Test Acc: 0.4900\n",
      "Epoch: 441, Loss: 0.0371, Train Acc: 1.0000, Loss: 2.2420, Test Acc: 0.4900\n",
      "Epoch: 442, Loss: 0.0344, Train Acc: 1.0000, Loss: 4.1005, Test Acc: 0.4400\n",
      "Epoch: 443, Loss: 0.0345, Train Acc: 1.0000, Loss: 2.1943, Test Acc: 0.4900\n",
      "Epoch: 444, Loss: 0.0348, Train Acc: 1.0000, Loss: 3.5608, Test Acc: 0.4400\n",
      "Epoch: 445, Loss: 0.0311, Train Acc: 1.0000, Loss: 3.4744, Test Acc: 0.4650\n",
      "Epoch: 446, Loss: 0.0318, Train Acc: 1.0000, Loss: 2.3483, Test Acc: 0.4900\n",
      "Epoch: 447, Loss: 0.0284, Train Acc: 1.0000, Loss: 4.1785, Test Acc: 0.4400\n",
      "Epoch: 448, Loss: 0.0309, Train Acc: 1.0000, Loss: 2.0023, Test Acc: 0.4900\n",
      "Epoch: 449, Loss: 0.0341, Train Acc: 1.0000, Loss: 3.5151, Test Acc: 0.4400\n",
      "Epoch: 450, Loss: 0.0291, Train Acc: 1.0000, Loss: 3.8687, Test Acc: 0.4650\n",
      "Epoch: 451, Loss: 0.0320, Train Acc: 1.0000, Loss: 2.1959, Test Acc: 0.4900\n",
      "Epoch: 452, Loss: 0.0299, Train Acc: 1.0000, Loss: 4.5377, Test Acc: 0.4400\n",
      "Epoch: 453, Loss: 0.0319, Train Acc: 1.0000, Loss: 2.2786, Test Acc: 0.4900\n",
      "Epoch: 454, Loss: 0.0307, Train Acc: 1.0000, Loss: 3.4726, Test Acc: 0.4400\n",
      "Epoch: 455, Loss: 0.0257, Train Acc: 1.0000, Loss: 3.9704, Test Acc: 0.4400\n",
      "Epoch: 456, Loss: 0.0301, Train Acc: 1.0000, Loss: 2.1129, Test Acc: 0.4900\n",
      "Epoch: 457, Loss: 0.0285, Train Acc: 1.0000, Loss: 4.3711, Test Acc: 0.4400\n",
      "Epoch: 458, Loss: 0.0274, Train Acc: 1.0000, Loss: 2.4581, Test Acc: 0.4650\n",
      "Epoch: 459, Loss: 0.0261, Train Acc: 1.0000, Loss: 3.4910, Test Acc: 0.4650\n",
      "Epoch: 460, Loss: 0.0216, Train Acc: 1.0000, Loss: 3.9595, Test Acc: 0.4150\n",
      "Epoch: 461, Loss: 0.0242, Train Acc: 1.0000, Loss: 2.1593, Test Acc: 0.4900\n",
      "Epoch: 462, Loss: 0.0218, Train Acc: 1.0000, Loss: 5.0062, Test Acc: 0.4150\n",
      "Epoch: 463, Loss: 0.0285, Train Acc: 1.0000, Loss: 2.1806, Test Acc: 0.4650\n",
      "Epoch: 464, Loss: 0.0303, Train Acc: 1.0000, Loss: 3.1844, Test Acc: 0.4400\n",
      "Epoch: 465, Loss: 0.0238, Train Acc: 1.0000, Loss: 4.7442, Test Acc: 0.4150\n",
      "Epoch: 466, Loss: 0.0297, Train Acc: 1.0000, Loss: 1.9176, Test Acc: 0.4650\n",
      "Epoch: 467, Loss: 0.0291, Train Acc: 1.0000, Loss: 4.7647, Test Acc: 0.4400\n",
      "Epoch: 468, Loss: 0.0275, Train Acc: 1.0000, Loss: 2.9619, Test Acc: 0.4900\n",
      "Epoch: 469, Loss: 0.0253, Train Acc: 1.0000, Loss: 3.0464, Test Acc: 0.4650\n",
      "Epoch: 470, Loss: 0.0238, Train Acc: 1.0000, Loss: 5.1057, Test Acc: 0.4150\n",
      "Epoch: 471, Loss: 0.0321, Train Acc: 1.0000, Loss: 1.7607, Test Acc: 0.5050\n",
      "Epoch: 472, Loss: 0.0296, Train Acc: 1.0000, Loss: 4.9091, Test Acc: 0.4400\n",
      "Epoch: 473, Loss: 0.0291, Train Acc: 1.0000, Loss: 3.2066, Test Acc: 0.4900\n",
      "Epoch: 474, Loss: 0.0232, Train Acc: 1.0000, Loss: 2.9649, Test Acc: 0.4900\n",
      "Epoch: 475, Loss: 0.0209, Train Acc: 1.0000, Loss: 5.3111, Test Acc: 0.3750\n",
      "Epoch: 476, Loss: 0.0295, Train Acc: 0.9875, Loss: 1.5599, Test Acc: 0.5050\n",
      "Epoch: 477, Loss: 0.0304, Train Acc: 1.0000, Loss: 4.7161, Test Acc: 0.4400\n",
      "Epoch: 478, Loss: 0.0297, Train Acc: 1.0000, Loss: 3.6047, Test Acc: 0.4900\n",
      "Epoch: 479, Loss: 0.0237, Train Acc: 1.0000, Loss: 2.9814, Test Acc: 0.5150\n",
      "Epoch: 480, Loss: 0.0206, Train Acc: 1.0000, Loss: 6.0260, Test Acc: 0.4000\n",
      "Epoch: 481, Loss: 0.0257, Train Acc: 1.0000, Loss: 1.6558, Test Acc: 0.5050\n",
      "Epoch: 482, Loss: 0.0256, Train Acc: 1.0000, Loss: 4.7630, Test Acc: 0.4400\n",
      "Epoch: 483, Loss: 0.0268, Train Acc: 1.0000, Loss: 3.3660, Test Acc: 0.4900\n",
      "Epoch: 484, Loss: 0.0214, Train Acc: 1.0000, Loss: 3.2405, Test Acc: 0.5400\n",
      "Epoch: 485, Loss: 0.0175, Train Acc: 1.0000, Loss: 6.4738, Test Acc: 0.3750\n",
      "Epoch: 486, Loss: 0.0209, Train Acc: 0.9875, Loss: 1.5592, Test Acc: 0.5050\n",
      "Epoch: 487, Loss: 0.0291, Train Acc: 0.9937, Loss: 4.7282, Test Acc: 0.4400\n",
      "Epoch: 488, Loss: 0.0265, Train Acc: 1.0000, Loss: 3.7932, Test Acc: 0.4650\n",
      "Epoch: 489, Loss: 0.0215, Train Acc: 1.0000, Loss: 2.9175, Test Acc: 0.5150\n",
      "Epoch: 490, Loss: 0.0163, Train Acc: 1.0000, Loss: 6.3883, Test Acc: 0.3750\n",
      "Epoch: 491, Loss: 0.0243, Train Acc: 0.9875, Loss: 1.5285, Test Acc: 0.5050\n",
      "Epoch: 492, Loss: 0.0283, Train Acc: 1.0000, Loss: 4.7326, Test Acc: 0.4000\n",
      "Epoch: 493, Loss: 0.0293, Train Acc: 1.0000, Loss: 4.4980, Test Acc: 0.4400\n",
      "Epoch: 494, Loss: 0.0229, Train Acc: 1.0000, Loss: 3.2816, Test Acc: 0.5150\n",
      "Epoch: 495, Loss: 0.0162, Train Acc: 0.9937, Loss: 6.4978, Test Acc: 0.3500\n",
      "Epoch: 496, Loss: 0.0228, Train Acc: 1.0000, Loss: 2.2443, Test Acc: 0.5300\n",
      "Epoch: 497, Loss: 0.0185, Train Acc: 1.0000, Loss: 5.5876, Test Acc: 0.4150\n",
      "Epoch: 498, Loss: 0.0179, Train Acc: 1.0000, Loss: 1.6551, Test Acc: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 499, Loss: 0.0156, Train Acc: 1.0000, Loss: 5.2833, Test Acc: 0.4150\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  31  35  36  37  38  39\n",
      "  42  70  72  73  75  76  77  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180] TEST: [30 32 33 34 40 41 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n",
      " 61 62 63 64 65 66 67 68 69 71 74 78]\n",
      "145\n",
      "36\n",
      "Epoch: 001, Loss: 0.8464, Train Acc: 0.4967, Loss: 0.6016, Test Acc: 0.5000\n",
      "Epoch: 002, Loss: 0.7738, Train Acc: 0.5033, Loss: 0.8411, Test Acc: 0.5000\n",
      "Epoch: 003, Loss: 0.7033, Train Acc: 0.4901, Loss: 0.7063, Test Acc: 0.5250\n",
      "Epoch: 004, Loss: 0.7079, Train Acc: 0.4967, Loss: 0.6364, Test Acc: 0.5000\n",
      "Epoch: 005, Loss: 0.7111, Train Acc: 0.4776, Loss: 0.7017, Test Acc: 0.5500\n",
      "Epoch: 006, Loss: 0.6961, Train Acc: 0.5151, Loss: 0.7335, Test Acc: 0.5000\n",
      "Epoch: 007, Loss: 0.6947, Train Acc: 0.4926, Loss: 0.6892, Test Acc: 0.5750\n",
      "Epoch: 008, Loss: 0.6995, Train Acc: 0.4967, Loss: 0.6726, Test Acc: 0.5000\n",
      "Epoch: 009, Loss: 0.6999, Train Acc: 0.5018, Loss: 0.6955, Test Acc: 0.5250\n",
      "Epoch: 010, Loss: 0.6956, Train Acc: 0.5151, Loss: 0.7088, Test Acc: 0.5000\n",
      "Epoch: 011, Loss: 0.6948, Train Acc: 0.4901, Loss: 0.6964, Test Acc: 0.5250\n",
      "Epoch: 012, Loss: 0.6965, Train Acc: 0.4982, Loss: 0.6897, Test Acc: 0.5750\n",
      "Epoch: 013, Loss: 0.6969, Train Acc: 0.4956, Loss: 0.6966, Test Acc: 0.5250\n",
      "Epoch: 014, Loss: 0.6957, Train Acc: 0.5026, Loss: 0.7010, Test Acc: 0.5500\n",
      "Epoch: 015, Loss: 0.6952, Train Acc: 0.4838, Loss: 0.6974, Test Acc: 0.5250\n",
      "Epoch: 016, Loss: 0.6957, Train Acc: 0.5081, Loss: 0.6949, Test Acc: 0.5250\n",
      "Epoch: 017, Loss: 0.6959, Train Acc: 0.4963, Loss: 0.6983, Test Acc: 0.5250\n",
      "Epoch: 018, Loss: 0.6953, Train Acc: 0.5151, Loss: 0.7026, Test Acc: 0.5500\n",
      "Epoch: 019, Loss: 0.6949, Train Acc: 0.5088, Loss: 0.7004, Test Acc: 0.5250\n",
      "Epoch: 020, Loss: 0.6953, Train Acc: 0.5018, Loss: 0.6977, Test Acc: 0.5500\n",
      "Epoch: 021, Loss: 0.6954, Train Acc: 0.5206, Loss: 0.6992, Test Acc: 0.5250\n",
      "Epoch: 022, Loss: 0.6951, Train Acc: 0.5088, Loss: 0.7003, Test Acc: 0.5500\n",
      "Epoch: 023, Loss: 0.6948, Train Acc: 0.5088, Loss: 0.7005, Test Acc: 0.5250\n",
      "Epoch: 024, Loss: 0.6946, Train Acc: 0.5206, Loss: 0.7003, Test Acc: 0.5000\n",
      "Epoch: 025, Loss: 0.6946, Train Acc: 0.5206, Loss: 0.7011, Test Acc: 0.5000\n",
      "Epoch: 026, Loss: 0.6943, Train Acc: 0.5206, Loss: 0.7011, Test Acc: 0.5000\n",
      "Epoch: 027, Loss: 0.6943, Train Acc: 0.5206, Loss: 0.7011, Test Acc: 0.5000\n",
      "Epoch: 028, Loss: 0.6942, Train Acc: 0.5206, Loss: 0.7015, Test Acc: 0.5000\n",
      "Epoch: 029, Loss: 0.6941, Train Acc: 0.5268, Loss: 0.7020, Test Acc: 0.5000\n",
      "Epoch: 030, Loss: 0.6940, Train Acc: 0.5393, Loss: 0.7021, Test Acc: 0.5000\n",
      "Epoch: 031, Loss: 0.6939, Train Acc: 0.5456, Loss: 0.7023, Test Acc: 0.5250\n",
      "Epoch: 032, Loss: 0.6938, Train Acc: 0.5456, Loss: 0.7025, Test Acc: 0.5250\n",
      "Epoch: 033, Loss: 0.6938, Train Acc: 0.5393, Loss: 0.7030, Test Acc: 0.5000\n",
      "Epoch: 034, Loss: 0.6936, Train Acc: 0.5393, Loss: 0.7032, Test Acc: 0.5000\n",
      "Epoch: 035, Loss: 0.6936, Train Acc: 0.5206, Loss: 0.7032, Test Acc: 0.5000\n",
      "Epoch: 036, Loss: 0.6935, Train Acc: 0.5331, Loss: 0.7034, Test Acc: 0.5000\n",
      "Epoch: 037, Loss: 0.6935, Train Acc: 0.5393, Loss: 0.7040, Test Acc: 0.5000\n",
      "Epoch: 038, Loss: 0.6933, Train Acc: 0.5393, Loss: 0.7042, Test Acc: 0.5000\n",
      "Epoch: 039, Loss: 0.6932, Train Acc: 0.5449, Loss: 0.7039, Test Acc: 0.5000\n",
      "Epoch: 040, Loss: 0.6932, Train Acc: 0.5511, Loss: 0.7041, Test Acc: 0.5000\n",
      "Epoch: 041, Loss: 0.6931, Train Acc: 0.5511, Loss: 0.7045, Test Acc: 0.5000\n",
      "Epoch: 042, Loss: 0.6931, Train Acc: 0.5511, Loss: 0.7047, Test Acc: 0.4500\n",
      "Epoch: 043, Loss: 0.6932, Train Acc: 0.5449, Loss: 0.7049, Test Acc: 0.4750\n",
      "Epoch: 044, Loss: 0.6930, Train Acc: 0.5449, Loss: 0.7051, Test Acc: 0.4250\n",
      "Epoch: 045, Loss: 0.6929, Train Acc: 0.5449, Loss: 0.7049, Test Acc: 0.4250\n",
      "Epoch: 046, Loss: 0.6928, Train Acc: 0.5449, Loss: 0.7050, Test Acc: 0.4250\n",
      "Epoch: 047, Loss: 0.6927, Train Acc: 0.5449, Loss: 0.7057, Test Acc: 0.5000\n",
      "Epoch: 048, Loss: 0.6925, Train Acc: 0.5574, Loss: 0.7052, Test Acc: 0.5000\n",
      "Epoch: 049, Loss: 0.6926, Train Acc: 0.5643, Loss: 0.7047, Test Acc: 0.4000\n",
      "Epoch: 050, Loss: 0.6926, Train Acc: 0.5518, Loss: 0.7054, Test Acc: 0.4250\n",
      "Epoch: 051, Loss: 0.6924, Train Acc: 0.5456, Loss: 0.7062, Test Acc: 0.4250\n",
      "Epoch: 052, Loss: 0.6922, Train Acc: 0.5526, Loss: 0.7061, Test Acc: 0.4250\n",
      "Epoch: 053, Loss: 0.6922, Train Acc: 0.5643, Loss: 0.7057, Test Acc: 0.4000\n",
      "Epoch: 054, Loss: 0.6921, Train Acc: 0.5879, Loss: 0.7053, Test Acc: 0.4250\n",
      "Epoch: 055, Loss: 0.6920, Train Acc: 0.5879, Loss: 0.7060, Test Acc: 0.4250\n",
      "Epoch: 056, Loss: 0.6919, Train Acc: 0.5879, Loss: 0.7063, Test Acc: 0.4250\n",
      "Epoch: 057, Loss: 0.6918, Train Acc: 0.5699, Loss: 0.7052, Test Acc: 0.4000\n",
      "Epoch: 058, Loss: 0.6918, Train Acc: 0.5761, Loss: 0.7054, Test Acc: 0.4250\n",
      "Epoch: 059, Loss: 0.6918, Train Acc: 0.5871, Loss: 0.7061, Test Acc: 0.4250\n",
      "Epoch: 060, Loss: 0.6915, Train Acc: 0.5761, Loss: 0.7056, Test Acc: 0.4000\n",
      "Epoch: 061, Loss: 0.6915, Train Acc: 0.5636, Loss: 0.7061, Test Acc: 0.4250\n",
      "Epoch: 062, Loss: 0.6912, Train Acc: 0.5574, Loss: 0.7065, Test Acc: 0.4250\n",
      "Epoch: 063, Loss: 0.6912, Train Acc: 0.5824, Loss: 0.7060, Test Acc: 0.4000\n",
      "Epoch: 064, Loss: 0.6911, Train Acc: 0.6184, Loss: 0.7053, Test Acc: 0.4250\n",
      "Epoch: 065, Loss: 0.6912, Train Acc: 0.5699, Loss: 0.7060, Test Acc: 0.4000\n",
      "Epoch: 066, Loss: 0.6910, Train Acc: 0.5574, Loss: 0.7061, Test Acc: 0.3750\n",
      "Epoch: 067, Loss: 0.6909, Train Acc: 0.5511, Loss: 0.7067, Test Acc: 0.4000\n",
      "Epoch: 068, Loss: 0.6906, Train Acc: 0.5511, Loss: 0.7069, Test Acc: 0.3750\n",
      "Epoch: 069, Loss: 0.6906, Train Acc: 0.5761, Loss: 0.7056, Test Acc: 0.4750\n",
      "Epoch: 070, Loss: 0.6904, Train Acc: 0.5949, Loss: 0.7052, Test Acc: 0.4750\n",
      "Epoch: 071, Loss: 0.6904, Train Acc: 0.5574, Loss: 0.7062, Test Acc: 0.4250\n",
      "Epoch: 072, Loss: 0.6902, Train Acc: 0.5574, Loss: 0.7062, Test Acc: 0.4500\n",
      "Epoch: 073, Loss: 0.6902, Train Acc: 0.5636, Loss: 0.7068, Test Acc: 0.4250\n",
      "Epoch: 074, Loss: 0.6897, Train Acc: 0.5886, Loss: 0.7062, Test Acc: 0.4500\n",
      "Epoch: 075, Loss: 0.6896, Train Acc: 0.6011, Loss: 0.7042, Test Acc: 0.4500\n",
      "Epoch: 076, Loss: 0.6900, Train Acc: 0.5449, Loss: 0.7043, Test Acc: 0.4250\n",
      "Epoch: 077, Loss: 0.6896, Train Acc: 0.5699, Loss: 0.7064, Test Acc: 0.4500\n",
      "Epoch: 078, Loss: 0.6890, Train Acc: 0.5949, Loss: 0.7075, Test Acc: 0.4750\n",
      "Epoch: 079, Loss: 0.6890, Train Acc: 0.6004, Loss: 0.7050, Test Acc: 0.4750\n",
      "Epoch: 080, Loss: 0.6891, Train Acc: 0.5871, Loss: 0.7037, Test Acc: 0.4250\n",
      "Epoch: 081, Loss: 0.6891, Train Acc: 0.5879, Loss: 0.7059, Test Acc: 0.4500\n",
      "Epoch: 082, Loss: 0.6884, Train Acc: 0.5699, Loss: 0.7072, Test Acc: 0.4500\n",
      "Epoch: 083, Loss: 0.6882, Train Acc: 0.5879, Loss: 0.7062, Test Acc: 0.4750\n",
      "Epoch: 084, Loss: 0.6880, Train Acc: 0.5934, Loss: 0.7048, Test Acc: 0.4500\n",
      "Epoch: 085, Loss: 0.6882, Train Acc: 0.6051, Loss: 0.7052, Test Acc: 0.4750\n",
      "Epoch: 086, Loss: 0.6878, Train Acc: 0.6051, Loss: 0.7050, Test Acc: 0.4750\n",
      "Epoch: 087, Loss: 0.6876, Train Acc: 0.6114, Loss: 0.7045, Test Acc: 0.4750\n",
      "Epoch: 088, Loss: 0.6875, Train Acc: 0.6169, Loss: 0.7047, Test Acc: 0.5500\n",
      "Epoch: 089, Loss: 0.6873, Train Acc: 0.6246, Loss: 0.7057, Test Acc: 0.5250\n",
      "Epoch: 090, Loss: 0.6873, Train Acc: 0.6051, Loss: 0.7060, Test Acc: 0.5500\n",
      "Epoch: 091, Loss: 0.6870, Train Acc: 0.5864, Loss: 0.7075, Test Acc: 0.4250\n",
      "Epoch: 092, Loss: 0.6867, Train Acc: 0.5871, Loss: 0.7072, Test Acc: 0.4500\n",
      "Epoch: 093, Loss: 0.6861, Train Acc: 0.5926, Loss: 0.7061, Test Acc: 0.4500\n",
      "Epoch: 094, Loss: 0.6857, Train Acc: 0.6426, Loss: 0.7028, Test Acc: 0.4750\n",
      "Epoch: 095, Loss: 0.6864, Train Acc: 0.6287, Loss: 0.6998, Test Acc: 0.5750\n",
      "Epoch: 096, Loss: 0.6865, Train Acc: 0.6294, Loss: 0.7051, Test Acc: 0.5500\n",
      "Epoch: 097, Loss: 0.6853, Train Acc: 0.5746, Loss: 0.7090, Test Acc: 0.4250\n",
      "Epoch: 098, Loss: 0.6849, Train Acc: 0.6169, Loss: 0.7074, Test Acc: 0.5250\n",
      "Epoch: 099, Loss: 0.6844, Train Acc: 0.6607, Loss: 0.7042, Test Acc: 0.5000\n",
      "Epoch: 100, Loss: 0.6841, Train Acc: 0.6169, Loss: 0.7017, Test Acc: 0.5250\n",
      "Epoch: 101, Loss: 0.6847, Train Acc: 0.6287, Loss: 0.7033, Test Acc: 0.5250\n",
      "Epoch: 102, Loss: 0.6839, Train Acc: 0.6357, Loss: 0.7041, Test Acc: 0.5750\n",
      "Epoch: 103, Loss: 0.6829, Train Acc: 0.6419, Loss: 0.7051, Test Acc: 0.5500\n",
      "Epoch: 104, Loss: 0.6822, Train Acc: 0.6114, Loss: 0.7047, Test Acc: 0.5500\n",
      "Epoch: 105, Loss: 0.6823, Train Acc: 0.6169, Loss: 0.7023, Test Acc: 0.6000\n",
      "Epoch: 106, Loss: 0.6814, Train Acc: 0.6107, Loss: 0.7005, Test Acc: 0.5250\n",
      "Epoch: 107, Loss: 0.6818, Train Acc: 0.6169, Loss: 0.7002, Test Acc: 0.5750\n",
      "Epoch: 108, Loss: 0.6815, Train Acc: 0.6162, Loss: 0.7010, Test Acc: 0.5750\n",
      "Epoch: 109, Loss: 0.6808, Train Acc: 0.6294, Loss: 0.7036, Test Acc: 0.5750\n",
      "Epoch: 110, Loss: 0.6802, Train Acc: 0.6357, Loss: 0.7042, Test Acc: 0.6000\n",
      "Epoch: 111, Loss: 0.6796, Train Acc: 0.6232, Loss: 0.7050, Test Acc: 0.5500\n",
      "Epoch: 112, Loss: 0.6793, Train Acc: 0.6287, Loss: 0.7019, Test Acc: 0.5500\n",
      "Epoch: 113, Loss: 0.6787, Train Acc: 0.6412, Loss: 0.7026, Test Acc: 0.6000\n",
      "Epoch: 114, Loss: 0.6769, Train Acc: 0.6482, Loss: 0.7092, Test Acc: 0.5250\n",
      "Epoch: 115, Loss: 0.6768, Train Acc: 0.6342, Loss: 0.6998, Test Acc: 0.6250\n",
      "Epoch: 116, Loss: 0.6770, Train Acc: 0.6349, Loss: 0.7064, Test Acc: 0.5750\n",
      "Epoch: 117, Loss: 0.6753, Train Acc: 0.6294, Loss: 0.7055, Test Acc: 0.6000\n",
      "Epoch: 118, Loss: 0.6742, Train Acc: 0.6467, Loss: 0.7028, Test Acc: 0.5500\n",
      "Epoch: 119, Loss: 0.6752, Train Acc: 0.6342, Loss: 0.7092, Test Acc: 0.6250\n",
      "Epoch: 120, Loss: 0.6734, Train Acc: 0.6224, Loss: 0.7016, Test Acc: 0.5750\n",
      "Epoch: 121, Loss: 0.6723, Train Acc: 0.6162, Loss: 0.7094, Test Acc: 0.6000\n",
      "Epoch: 122, Loss: 0.6714, Train Acc: 0.6099, Loss: 0.7067, Test Acc: 0.6000\n",
      "Epoch: 123, Loss: 0.6707, Train Acc: 0.5912, Loss: 0.7067, Test Acc: 0.5500\n",
      "Epoch: 124, Loss: 0.6705, Train Acc: 0.6162, Loss: 0.7017, Test Acc: 0.5750\n",
      "Epoch: 125, Loss: 0.6689, Train Acc: 0.6224, Loss: 0.7130, Test Acc: 0.5750\n",
      "Epoch: 126, Loss: 0.6671, Train Acc: 0.6279, Loss: 0.7068, Test Acc: 0.6000\n",
      "Epoch: 127, Loss: 0.6678, Train Acc: 0.6037, Loss: 0.7079, Test Acc: 0.6000\n",
      "Epoch: 128, Loss: 0.6656, Train Acc: 0.6224, Loss: 0.7195, Test Acc: 0.5500\n",
      "Epoch: 129, Loss: 0.6698, Train Acc: 0.6092, Loss: 0.7118, Test Acc: 0.5250\n",
      "Epoch: 130, Loss: 0.6686, Train Acc: 0.6099, Loss: 0.6876, Test Acc: 0.5250\n",
      "Epoch: 131, Loss: 0.6641, Train Acc: 0.6522, Loss: 0.7282, Test Acc: 0.5500\n",
      "Epoch: 132, Loss: 0.6616, Train Acc: 0.6092, Loss: 0.6932, Test Acc: 0.4500\n",
      "Epoch: 133, Loss: 0.6635, Train Acc: 0.5974, Loss: 0.7216, Test Acc: 0.5750\n",
      "Epoch: 134, Loss: 0.6607, Train Acc: 0.5787, Loss: 0.7170, Test Acc: 0.4750\n",
      "Epoch: 135, Loss: 0.6638, Train Acc: 0.6342, Loss: 0.7051, Test Acc: 0.4500\n",
      "Epoch: 136, Loss: 0.6665, Train Acc: 0.6599, Loss: 0.7461, Test Acc: 0.6250\n",
      "Epoch: 137, Loss: 0.6595, Train Acc: 0.6404, Loss: 0.7349, Test Acc: 0.4500\n",
      "Epoch: 138, Loss: 0.6611, Train Acc: 0.6404, Loss: 0.7199, Test Acc: 0.4250\n",
      "Epoch: 139, Loss: 0.6599, Train Acc: 0.6460, Loss: 0.7547, Test Acc: 0.5500\n",
      "Epoch: 140, Loss: 0.6589, Train Acc: 0.5801, Loss: 0.6860, Test Acc: 0.5000\n",
      "Epoch: 141, Loss: 0.6592, Train Acc: 0.6710, Loss: 0.7567, Test Acc: 0.4500\n",
      "Epoch: 142, Loss: 0.6565, Train Acc: 0.6397, Loss: 0.7200, Test Acc: 0.4250\n",
      "Epoch: 143, Loss: 0.6480, Train Acc: 0.6335, Loss: 0.7634, Test Acc: 0.5000\n",
      "Epoch: 144, Loss: 0.6508, Train Acc: 0.6452, Loss: 0.7401, Test Acc: 0.4000\n",
      "Epoch: 145, Loss: 0.6420, Train Acc: 0.6632, Loss: 0.7739, Test Acc: 0.4500\n",
      "Epoch: 146, Loss: 0.6408, Train Acc: 0.6335, Loss: 0.7561, Test Acc: 0.4250\n",
      "Epoch: 147, Loss: 0.6392, Train Acc: 0.6577, Loss: 0.7611, Test Acc: 0.4500\n",
      "Epoch: 148, Loss: 0.6367, Train Acc: 0.6515, Loss: 0.7670, Test Acc: 0.4500\n",
      "Epoch: 149, Loss: 0.6320, Train Acc: 0.6335, Loss: 0.7676, Test Acc: 0.4500\n",
      "Epoch: 150, Loss: 0.6391, Train Acc: 0.6279, Loss: 0.7856, Test Acc: 0.4500\n",
      "Epoch: 151, Loss: 0.6314, Train Acc: 0.6397, Loss: 0.7677, Test Acc: 0.4500\n",
      "Epoch: 152, Loss: 0.6663, Train Acc: 0.6585, Loss: 0.7861, Test Acc: 0.4500\n",
      "Epoch: 153, Loss: 0.6516, Train Acc: 0.6349, Loss: 0.7246, Test Acc: 0.5500\n",
      "Epoch: 154, Loss: 0.6459, Train Acc: 0.6710, Loss: 0.7825, Test Acc: 0.5750\n",
      "Epoch: 155, Loss: 0.6487, Train Acc: 0.6467, Loss: 0.7246, Test Acc: 0.4500\n",
      "Epoch: 156, Loss: 0.6314, Train Acc: 0.6779, Loss: 0.7753, Test Acc: 0.5000\n",
      "Epoch: 157, Loss: 0.6304, Train Acc: 0.7007, Loss: 0.7842, Test Acc: 0.4750\n",
      "Epoch: 158, Loss: 0.6224, Train Acc: 0.6765, Loss: 0.8128, Test Acc: 0.4750\n",
      "Epoch: 159, Loss: 0.6176, Train Acc: 0.6952, Loss: 0.7810, Test Acc: 0.4500\n",
      "Epoch: 160, Loss: 0.6137, Train Acc: 0.6585, Loss: 0.7695, Test Acc: 0.4500\n",
      "Epoch: 161, Loss: 0.6108, Train Acc: 0.6890, Loss: 0.8353, Test Acc: 0.4250\n",
      "Epoch: 162, Loss: 0.6086, Train Acc: 0.6397, Loss: 0.7133, Test Acc: 0.4250\n",
      "Epoch: 163, Loss: 0.6103, Train Acc: 0.7077, Loss: 0.8413, Test Acc: 0.4500\n",
      "Epoch: 164, Loss: 0.6117, Train Acc: 0.7202, Loss: 0.8243, Test Acc: 0.3750\n",
      "Epoch: 165, Loss: 0.6111, Train Acc: 0.6460, Loss: 0.7066, Test Acc: 0.4500\n",
      "Epoch: 166, Loss: 0.6240, Train Acc: 0.7452, Loss: 0.8591, Test Acc: 0.5250\n",
      "Epoch: 167, Loss: 0.6182, Train Acc: 0.6772, Loss: 0.7168, Test Acc: 0.5000\n",
      "Epoch: 168, Loss: 0.6044, Train Acc: 0.7085, Loss: 0.8477, Test Acc: 0.6500\n",
      "Epoch: 169, Loss: 0.5947, Train Acc: 0.6272, Loss: 0.6950, Test Acc: 0.4500\n",
      "Epoch: 170, Loss: 0.6115, Train Acc: 0.7265, Loss: 0.9450, Test Acc: 0.5750\n",
      "Epoch: 171, Loss: 0.6037, Train Acc: 0.6890, Loss: 0.7005, Test Acc: 0.4500\n",
      "Epoch: 172, Loss: 0.5961, Train Acc: 0.7265, Loss: 0.7374, Test Acc: 0.4750\n",
      "Epoch: 173, Loss: 0.6037, Train Acc: 0.7217, Loss: 0.8543, Test Acc: 0.5750\n",
      "Epoch: 174, Loss: 0.5908, Train Acc: 0.7265, Loss: 0.8321, Test Acc: 0.4750\n",
      "Epoch: 175, Loss: 0.5776, Train Acc: 0.7202, Loss: 0.6986, Test Acc: 0.4750\n",
      "Epoch: 176, Loss: 0.5757, Train Acc: 0.6897, Loss: 0.6859, Test Acc: 0.4750\n",
      "Epoch: 177, Loss: 0.5835, Train Acc: 0.7515, Loss: 0.9360, Test Acc: 0.5000\n",
      "Epoch: 178, Loss: 0.5795, Train Acc: 0.6585, Loss: 0.6718, Test Acc: 0.4750\n",
      "Epoch: 179, Loss: 0.5790, Train Acc: 0.6710, Loss: 0.6669, Test Acc: 0.4500\n",
      "Epoch: 180, Loss: 0.6036, Train Acc: 0.7522, Loss: 0.9647, Test Acc: 0.5500\n",
      "Epoch: 181, Loss: 0.6052, Train Acc: 0.7265, Loss: 0.8644, Test Acc: 0.5500\n",
      "Epoch: 182, Loss: 0.5775, Train Acc: 0.7210, Loss: 0.7104, Test Acc: 0.5000\n",
      "Epoch: 183, Loss: 0.5820, Train Acc: 0.7202, Loss: 0.7282, Test Acc: 0.4250\n",
      "Epoch: 184, Loss: 0.5822, Train Acc: 0.7077, Loss: 0.9156, Test Acc: 0.5000\n",
      "Epoch: 185, Loss: 0.5967, Train Acc: 0.6640, Loss: 0.6981, Test Acc: 0.5000\n",
      "Epoch: 186, Loss: 0.5688, Train Acc: 0.7577, Loss: 0.7734, Test Acc: 0.5250\n",
      "Epoch: 187, Loss: 0.5653, Train Acc: 0.7515, Loss: 0.7816, Test Acc: 0.5000\n",
      "Epoch: 188, Loss: 0.5654, Train Acc: 0.7147, Loss: 0.6350, Test Acc: 0.5250\n",
      "Epoch: 189, Loss: 0.5560, Train Acc: 0.7695, Loss: 0.7885, Test Acc: 0.5250\n",
      "Epoch: 190, Loss: 0.5677, Train Acc: 0.7265, Loss: 0.7551, Test Acc: 0.5250\n",
      "Epoch: 191, Loss: 0.5673, Train Acc: 0.7015, Loss: 0.7294, Test Acc: 0.5250\n",
      "Epoch: 192, Loss: 0.5579, Train Acc: 0.7702, Loss: 0.8333, Test Acc: 0.5500\n",
      "Epoch: 193, Loss: 0.5570, Train Acc: 0.7452, Loss: 0.7101, Test Acc: 0.5000\n",
      "Epoch: 194, Loss: 0.5505, Train Acc: 0.7210, Loss: 0.7521, Test Acc: 0.5000\n",
      "Epoch: 195, Loss: 0.5433, Train Acc: 0.7820, Loss: 0.7802, Test Acc: 0.5750\n",
      "Epoch: 196, Loss: 0.5367, Train Acc: 0.7452, Loss: 0.8460, Test Acc: 0.5500\n",
      "Epoch: 197, Loss: 0.5461, Train Acc: 0.7640, Loss: 0.7085, Test Acc: 0.5250\n",
      "Epoch: 198, Loss: 0.5271, Train Acc: 0.7952, Loss: 0.7588, Test Acc: 0.5500\n",
      "Epoch: 199, Loss: 0.5084, Train Acc: 0.7695, Loss: 0.8977, Test Acc: 0.4750\n",
      "Epoch: 200, Loss: 0.5166, Train Acc: 0.7695, Loss: 0.6511, Test Acc: 0.5250\n",
      "Epoch: 201, Loss: 0.5117, Train Acc: 0.7632, Loss: 0.6439, Test Acc: 0.4500\n",
      "Epoch: 202, Loss: 0.5044, Train Acc: 0.7757, Loss: 0.9280, Test Acc: 0.5750\n",
      "Epoch: 203, Loss: 0.5170, Train Acc: 0.7945, Loss: 0.6668, Test Acc: 0.5500\n",
      "Epoch: 204, Loss: 0.5007, Train Acc: 0.7390, Loss: 0.6040, Test Acc: 0.5500\n",
      "Epoch: 205, Loss: 0.5013, Train Acc: 0.7702, Loss: 1.1195, Test Acc: 0.7000\n",
      "Epoch: 206, Loss: 0.5519, Train Acc: 0.7460, Loss: 0.5561, Test Acc: 0.7000\n",
      "Epoch: 207, Loss: 0.4908, Train Acc: 0.7625, Loss: 0.6758, Test Acc: 0.4500\n",
      "Epoch: 208, Loss: 0.5169, Train Acc: 0.7640, Loss: 1.2271, Test Acc: 0.6250\n",
      "Epoch: 209, Loss: 0.5640, Train Acc: 0.7272, Loss: 0.4289, Test Acc: 0.6250\n",
      "Epoch: 210, Loss: 0.5108, Train Acc: 0.8250, Loss: 0.7198, Test Acc: 0.5500\n",
      "Epoch: 211, Loss: 0.4916, Train Acc: 0.7695, Loss: 0.9259, Test Acc: 0.6000\n",
      "Epoch: 212, Loss: 0.4774, Train Acc: 0.8382, Loss: 0.7108, Test Acc: 0.5250\n",
      "Epoch: 213, Loss: 0.4843, Train Acc: 0.7577, Loss: 0.4843, Test Acc: 0.5750\n",
      "Epoch: 214, Loss: 0.4720, Train Acc: 0.8257, Loss: 0.7203, Test Acc: 0.5500\n",
      "Epoch: 215, Loss: 0.4693, Train Acc: 0.8015, Loss: 0.8570, Test Acc: 0.6000\n",
      "Epoch: 216, Loss: 0.4506, Train Acc: 0.8257, Loss: 0.4617, Test Acc: 0.5750\n",
      "Epoch: 217, Loss: 0.4556, Train Acc: 0.8007, Loss: 0.5219, Test Acc: 0.5500\n",
      "Epoch: 218, Loss: 0.4480, Train Acc: 0.8320, Loss: 0.8227, Test Acc: 0.5750\n",
      "Epoch: 219, Loss: 0.4345, Train Acc: 0.8132, Loss: 0.6430, Test Acc: 0.5750\n",
      "Epoch: 220, Loss: 0.4223, Train Acc: 0.8195, Loss: 0.4984, Test Acc: 0.6000\n",
      "Epoch: 221, Loss: 0.4318, Train Acc: 0.8007, Loss: 0.5465, Test Acc: 0.5250\n",
      "Epoch: 222, Loss: 0.4331, Train Acc: 0.8195, Loss: 0.4473, Test Acc: 0.6000\n",
      "Epoch: 223, Loss: 0.4190, Train Acc: 0.8382, Loss: 0.5414, Test Acc: 0.5250\n",
      "Epoch: 224, Loss: 0.4157, Train Acc: 0.8445, Loss: 0.7267, Test Acc: 0.5500\n",
      "Epoch: 225, Loss: 0.4193, Train Acc: 0.8257, Loss: 0.8728, Test Acc: 0.6500\n",
      "Epoch: 226, Loss: 0.4321, Train Acc: 0.8382, Loss: 0.4150, Test Acc: 0.6250\n",
      "Epoch: 227, Loss: 0.4366, Train Acc: 0.7820, Loss: 0.5195, Test Acc: 0.6000\n",
      "Epoch: 228, Loss: 0.4372, Train Acc: 0.8202, Loss: 0.4917, Test Acc: 0.6000\n",
      "Epoch: 229, Loss: 0.4351, Train Acc: 0.7702, Loss: 0.5016, Test Acc: 0.6000\n",
      "Epoch: 230, Loss: 0.4084, Train Acc: 0.8382, Loss: 0.7911, Test Acc: 0.6500\n",
      "Epoch: 231, Loss: 0.4197, Train Acc: 0.8320, Loss: 0.4640, Test Acc: 0.6500\n",
      "Epoch: 232, Loss: 0.4340, Train Acc: 0.8070, Loss: 0.3536, Test Acc: 0.6500\n",
      "Epoch: 233, Loss: 0.3908, Train Acc: 0.8250, Loss: 0.7689, Test Acc: 0.5750\n",
      "Epoch: 234, Loss: 0.4171, Train Acc: 0.7765, Loss: 0.3878, Test Acc: 0.7250\n",
      "Epoch: 235, Loss: 0.4088, Train Acc: 0.8625, Loss: 0.3454, Test Acc: 0.6000\n",
      "Epoch: 236, Loss: 0.4367, Train Acc: 0.8382, Loss: 1.0567, Test Acc: 0.7000\n",
      "Epoch: 237, Loss: 0.4698, Train Acc: 0.8022, Loss: 0.3610, Test Acc: 0.6750\n",
      "Epoch: 238, Loss: 0.4092, Train Acc: 0.8000, Loss: 0.4422, Test Acc: 0.6250\n",
      "Epoch: 239, Loss: 0.4314, Train Acc: 0.8140, Loss: 0.4049, Test Acc: 0.6250\n",
      "Epoch: 240, Loss: 0.3944, Train Acc: 0.8507, Loss: 1.1097, Test Acc: 0.5250\n",
      "Epoch: 241, Loss: 0.3978, Train Acc: 0.8320, Loss: 0.4145, Test Acc: 0.7000\n",
      "Epoch: 242, Loss: 0.4010, Train Acc: 0.8507, Loss: 0.3016, Test Acc: 0.6250\n",
      "Epoch: 243, Loss: 0.3664, Train Acc: 0.8313, Loss: 0.5097, Test Acc: 0.6250\n",
      "Epoch: 244, Loss: 0.3870, Train Acc: 0.8632, Loss: 0.5086, Test Acc: 0.6000\n",
      "Epoch: 245, Loss: 0.3731, Train Acc: 0.8320, Loss: 0.4770, Test Acc: 0.6500\n",
      "Epoch: 246, Loss: 0.3586, Train Acc: 0.8757, Loss: 0.8468, Test Acc: 0.6000\n",
      "Epoch: 247, Loss: 0.3682, Train Acc: 0.8195, Loss: 0.9179, Test Acc: 0.6500\n",
      "Epoch: 248, Loss: 0.3869, Train Acc: 0.8820, Loss: 0.2633, Test Acc: 0.6000\n",
      "Epoch: 249, Loss: 0.3769, Train Acc: 0.7577, Loss: 0.4613, Test Acc: 0.5250\n",
      "Epoch: 250, Loss: 0.4341, Train Acc: 0.8820, Loss: 0.5783, Test Acc: 0.5500\n",
      "Epoch: 251, Loss: 0.3984, Train Acc: 0.8265, Loss: 1.3516, Test Acc: 0.6250\n",
      "Epoch: 252, Loss: 0.4293, Train Acc: 0.8320, Loss: 0.3014, Test Acc: 0.7250\n",
      "Epoch: 253, Loss: 0.3511, Train Acc: 0.8687, Loss: 0.2693, Test Acc: 0.6250\n",
      "Epoch: 254, Loss: 0.3538, Train Acc: 0.8945, Loss: 0.3576, Test Acc: 0.6250\n",
      "Epoch: 255, Loss: 0.3498, Train Acc: 0.8570, Loss: 1.1827, Test Acc: 0.6250\n",
      "Epoch: 256, Loss: 0.3893, Train Acc: 0.8452, Loss: 0.2050, Test Acc: 0.7000\n",
      "Epoch: 257, Loss: 0.3213, Train Acc: 0.8875, Loss: 0.4989, Test Acc: 0.5750\n",
      "Epoch: 258, Loss: 0.3367, Train Acc: 0.8327, Loss: 0.6106, Test Acc: 0.6500\n",
      "Epoch: 259, Loss: 0.3485, Train Acc: 0.8695, Loss: 0.6686, Test Acc: 0.6000\n",
      "Epoch: 260, Loss: 0.3163, Train Acc: 0.8875, Loss: 0.2773, Test Acc: 0.6000\n",
      "Epoch: 261, Loss: 0.3183, Train Acc: 0.8625, Loss: 0.3045, Test Acc: 0.6000\n",
      "Epoch: 262, Loss: 0.3286, Train Acc: 0.8945, Loss: 0.7788, Test Acc: 0.6000\n",
      "Epoch: 263, Loss: 0.3297, Train Acc: 0.8507, Loss: 0.6698, Test Acc: 0.6750\n",
      "Epoch: 264, Loss: 0.3261, Train Acc: 0.9062, Loss: 0.2285, Test Acc: 0.6500\n",
      "Epoch: 265, Loss: 0.2780, Train Acc: 0.9000, Loss: 0.3339, Test Acc: 0.6000\n",
      "Epoch: 266, Loss: 0.2822, Train Acc: 0.8882, Loss: 0.6721, Test Acc: 0.6500\n",
      "Epoch: 267, Loss: 0.3011, Train Acc: 0.9007, Loss: 0.6162, Test Acc: 0.6250\n",
      "Epoch: 268, Loss: 0.2814, Train Acc: 0.9187, Loss: 0.2664, Test Acc: 0.6250\n",
      "Epoch: 269, Loss: 0.2689, Train Acc: 0.9125, Loss: 0.2260, Test Acc: 0.6500\n",
      "Epoch: 270, Loss: 0.2767, Train Acc: 0.9250, Loss: 0.4095, Test Acc: 0.5500\n",
      "Epoch: 271, Loss: 0.2783, Train Acc: 0.8820, Loss: 1.4296, Test Acc: 0.6500\n",
      "Epoch: 272, Loss: 0.3422, Train Acc: 0.8570, Loss: 0.3122, Test Acc: 0.6750\n",
      "Epoch: 273, Loss: 0.3362, Train Acc: 0.9125, Loss: 0.2056, Test Acc: 0.6750\n",
      "Epoch: 274, Loss: 0.2773, Train Acc: 0.8382, Loss: 0.4847, Test Acc: 0.6500\n",
      "Epoch: 275, Loss: 0.3113, Train Acc: 0.9007, Loss: 0.1642, Test Acc: 0.6500\n",
      "Epoch: 276, Loss: 0.3042, Train Acc: 0.9070, Loss: 0.6340, Test Acc: 0.5250\n",
      "Epoch: 277, Loss: 0.3298, Train Acc: 0.8695, Loss: 1.8441, Test Acc: 0.6250\n",
      "Epoch: 278, Loss: 0.4447, Train Acc: 0.7897, Loss: 0.1573, Test Acc: 0.7250\n",
      "Epoch: 279, Loss: 0.3443, Train Acc: 0.7882, Loss: 0.5863, Test Acc: 0.6000\n",
      "Epoch: 280, Loss: 0.4684, Train Acc: 0.8077, Loss: 1.6184, Test Acc: 0.7000\n",
      "Epoch: 281, Loss: 0.4774, Train Acc: 0.8820, Loss: 0.5859, Test Acc: 0.6250\n",
      "Epoch: 282, Loss: 0.3025, Train Acc: 0.8195, Loss: 0.5683, Test Acc: 0.5250\n",
      "Epoch: 283, Loss: 0.3366, Train Acc: 0.8382, Loss: 0.3332, Test Acc: 0.7000\n",
      "Epoch: 284, Loss: 0.3420, Train Acc: 0.9062, Loss: 0.6922, Test Acc: 0.5500\n",
      "Epoch: 285, Loss: 0.2804, Train Acc: 0.8882, Loss: 0.4170, Test Acc: 0.6000\n",
      "Epoch: 286, Loss: 0.2622, Train Acc: 0.9313, Loss: 0.5330, Test Acc: 0.5750\n",
      "Epoch: 287, Loss: 0.2858, Train Acc: 0.8820, Loss: 0.8044, Test Acc: 0.6750\n",
      "Epoch: 288, Loss: 0.2699, Train Acc: 0.9187, Loss: 0.3368, Test Acc: 0.6250\n",
      "Epoch: 289, Loss: 0.2377, Train Acc: 0.9187, Loss: 0.4770, Test Acc: 0.5000\n",
      "Epoch: 290, Loss: 0.2749, Train Acc: 0.8632, Loss: 0.8085, Test Acc: 0.6500\n",
      "Epoch: 291, Loss: 0.2901, Train Acc: 0.9187, Loss: 0.3212, Test Acc: 0.6250\n",
      "Epoch: 292, Loss: 0.2460, Train Acc: 0.9000, Loss: 0.4105, Test Acc: 0.6250\n",
      "Epoch: 293, Loss: 0.2553, Train Acc: 0.9132, Loss: 0.9251, Test Acc: 0.6250\n",
      "Epoch: 294, Loss: 0.2868, Train Acc: 0.8702, Loss: 0.4394, Test Acc: 0.6250\n",
      "Epoch: 295, Loss: 0.2469, Train Acc: 0.9375, Loss: 0.4191, Test Acc: 0.6000\n",
      "Epoch: 296, Loss: 0.2153, Train Acc: 0.9132, Loss: 0.3863, Test Acc: 0.6750\n",
      "Epoch: 297, Loss: 0.2434, Train Acc: 0.9007, Loss: 0.7587, Test Acc: 0.6500\n",
      "Epoch: 298, Loss: 0.2443, Train Acc: 0.9312, Loss: 0.2797, Test Acc: 0.6250\n",
      "Epoch: 299, Loss: 0.2136, Train Acc: 0.9500, Loss: 0.5891, Test Acc: 0.5750\n",
      "Epoch: 300, Loss: 0.2156, Train Acc: 0.8945, Loss: 1.0405, Test Acc: 0.6500\n",
      "Epoch: 301, Loss: 0.2608, Train Acc: 0.9007, Loss: 0.1619, Test Acc: 0.6750\n",
      "Epoch: 302, Loss: 0.2150, Train Acc: 0.9438, Loss: 0.5602, Test Acc: 0.5500\n",
      "Epoch: 303, Loss: 0.1974, Train Acc: 0.9195, Loss: 0.3540, Test Acc: 0.6500\n",
      "Epoch: 304, Loss: 0.2242, Train Acc: 0.9132, Loss: 0.3922, Test Acc: 0.6750\n",
      "Epoch: 305, Loss: 0.1931, Train Acc: 0.9375, Loss: 0.3072, Test Acc: 0.6000\n",
      "Epoch: 306, Loss: 0.1921, Train Acc: 0.9438, Loss: 0.2879, Test Acc: 0.6000\n",
      "Epoch: 307, Loss: 0.1936, Train Acc: 0.9375, Loss: 0.7240, Test Acc: 0.6500\n",
      "Epoch: 308, Loss: 0.2348, Train Acc: 0.9070, Loss: 0.5439, Test Acc: 0.6750\n",
      "Epoch: 309, Loss: 0.2306, Train Acc: 0.9500, Loss: 0.2541, Test Acc: 0.6250\n",
      "Epoch: 310, Loss: 0.1891, Train Acc: 0.9125, Loss: 0.6478, Test Acc: 0.5250\n",
      "Epoch: 311, Loss: 0.1987, Train Acc: 0.9257, Loss: 0.7391, Test Acc: 0.6500\n",
      "Epoch: 312, Loss: 0.2320, Train Acc: 0.9070, Loss: 0.2390, Test Acc: 0.7000\n",
      "Epoch: 313, Loss: 0.1919, Train Acc: 0.9500, Loss: 0.5153, Test Acc: 0.5500\n",
      "Epoch: 314, Loss: 0.1628, Train Acc: 0.9625, Loss: 0.3432, Test Acc: 0.6500\n",
      "Epoch: 315, Loss: 0.1816, Train Acc: 0.9257, Loss: 0.4139, Test Acc: 0.6500\n",
      "Epoch: 316, Loss: 0.1598, Train Acc: 0.9625, Loss: 0.3260, Test Acc: 0.6250\n",
      "Epoch: 317, Loss: 0.1625, Train Acc: 0.9625, Loss: 0.3745, Test Acc: 0.6000\n",
      "Epoch: 318, Loss: 0.1591, Train Acc: 0.9313, Loss: 0.7215, Test Acc: 0.5750\n",
      "Epoch: 319, Loss: 0.2005, Train Acc: 0.9007, Loss: 0.2421, Test Acc: 0.7500\n",
      "Epoch: 320, Loss: 0.2026, Train Acc: 0.9563, Loss: 0.2951, Test Acc: 0.6250\n",
      "Epoch: 321, Loss: 0.1524, Train Acc: 0.9375, Loss: 0.7507, Test Acc: 0.5500\n",
      "Epoch: 322, Loss: 0.1715, Train Acc: 0.9375, Loss: 0.2356, Test Acc: 0.7000\n",
      "Epoch: 323, Loss: 0.1838, Train Acc: 0.9625, Loss: 0.2345, Test Acc: 0.6750\n",
      "Epoch: 324, Loss: 0.1551, Train Acc: 0.9688, Loss: 0.5735, Test Acc: 0.6000\n",
      "Epoch: 325, Loss: 0.1495, Train Acc: 0.9257, Loss: 0.4250, Test Acc: 0.6750\n",
      "Epoch: 326, Loss: 0.1864, Train Acc: 0.9500, Loss: 0.2890, Test Acc: 0.6500\n",
      "Epoch: 327, Loss: 0.1450, Train Acc: 0.9625, Loss: 0.4230, Test Acc: 0.6000\n",
      "Epoch: 328, Loss: 0.1435, Train Acc: 0.9125, Loss: 0.2742, Test Acc: 0.6000\n",
      "Epoch: 329, Loss: 0.1800, Train Acc: 0.9688, Loss: 0.3443, Test Acc: 0.6250\n",
      "Epoch: 330, Loss: 0.1495, Train Acc: 0.9563, Loss: 0.2814, Test Acc: 0.7000\n",
      "Epoch: 331, Loss: 0.1598, Train Acc: 0.9257, Loss: 0.2834, Test Acc: 0.7250\n",
      "Epoch: 332, Loss: 0.1459, Train Acc: 0.9625, Loss: 0.3111, Test Acc: 0.6000\n",
      "Epoch: 333, Loss: 0.1249, Train Acc: 0.9750, Loss: 0.3669, Test Acc: 0.6000\n",
      "Epoch: 334, Loss: 0.1161, Train Acc: 0.9750, Loss: 0.3250, Test Acc: 0.6750\n",
      "Epoch: 335, Loss: 0.1215, Train Acc: 0.9563, Loss: 0.2930, Test Acc: 0.7000\n",
      "Epoch: 336, Loss: 0.1192, Train Acc: 0.9750, Loss: 0.2748, Test Acc: 0.6250\n",
      "Epoch: 337, Loss: 0.1068, Train Acc: 0.9875, Loss: 0.3946, Test Acc: 0.6250\n",
      "Epoch: 338, Loss: 0.1035, Train Acc: 0.9750, Loss: 0.3141, Test Acc: 0.6250\n",
      "Epoch: 339, Loss: 0.1107, Train Acc: 0.9625, Loss: 0.3344, Test Acc: 0.6250\n",
      "Epoch: 340, Loss: 0.1117, Train Acc: 0.9688, Loss: 0.3591, Test Acc: 0.6500\n",
      "Epoch: 341, Loss: 0.1052, Train Acc: 0.9688, Loss: 0.2728, Test Acc: 0.6500\n",
      "Epoch: 342, Loss: 0.1169, Train Acc: 0.9625, Loss: 0.3782, Test Acc: 0.6250\n",
      "Epoch: 343, Loss: 0.1245, Train Acc: 0.9563, Loss: 0.5097, Test Acc: 0.5250\n",
      "Epoch: 344, Loss: 0.1163, Train Acc: 0.9875, Loss: 0.4047, Test Acc: 0.5750\n",
      "Epoch: 345, Loss: 0.1349, Train Acc: 0.9445, Loss: 0.2648, Test Acc: 0.7000\n",
      "Epoch: 346, Loss: 0.1866, Train Acc: 0.9312, Loss: 1.1590, Test Acc: 0.6250\n",
      "Epoch: 347, Loss: 0.2073, Train Acc: 0.8820, Loss: 0.7438, Test Acc: 0.5500\n",
      "Epoch: 348, Loss: 0.2202, Train Acc: 0.8257, Loss: 0.2018, Test Acc: 0.6750\n",
      "Epoch: 349, Loss: 0.3414, Train Acc: 0.9625, Loss: 0.1923, Test Acc: 0.7250\n",
      "Epoch: 350, Loss: 0.2810, Train Acc: 0.7702, Loss: 0.5427, Test Acc: 0.5750\n",
      "Epoch: 351, Loss: 0.4307, Train Acc: 0.7327, Loss: 1.0919, Test Acc: 0.6500\n",
      "Epoch: 352, Loss: 0.3652, Train Acc: 0.9132, Loss: 0.8271, Test Acc: 0.5750\n",
      "Epoch: 353, Loss: 0.2923, Train Acc: 0.8452, Loss: 1.5066, Test Acc: 0.6750\n",
      "Epoch: 354, Loss: 0.3853, Train Acc: 0.9070, Loss: 0.3261, Test Acc: 0.6500\n",
      "Epoch: 355, Loss: 0.3340, Train Acc: 0.8507, Loss: 0.5723, Test Acc: 0.6000\n",
      "Epoch: 356, Loss: 0.5342, Train Acc: 0.8320, Loss: 0.8429, Test Acc: 0.6750\n",
      "Epoch: 357, Loss: 0.5699, Train Acc: 0.7154, Loss: 0.1497, Test Acc: 0.6750\n",
      "Epoch: 358, Loss: 0.6581, Train Acc: 0.7397, Loss: 0.4205, Test Acc: 0.6750\n",
      "Epoch: 359, Loss: 0.5846, Train Acc: 0.7452, Loss: 0.9935, Test Acc: 0.6500\n",
      "Epoch: 360, Loss: 0.5166, Train Acc: 0.7710, Loss: 1.3828, Test Acc: 0.4750\n",
      "Epoch: 361, Loss: 0.4494, Train Acc: 0.8452, Loss: 1.1719, Test Acc: 0.5750\n",
      "Epoch: 362, Loss: 0.3605, Train Acc: 0.9070, Loss: 0.4116, Test Acc: 0.6500\n",
      "Epoch: 363, Loss: 0.3257, Train Acc: 0.9062, Loss: 0.4667, Test Acc: 0.6000\n",
      "Epoch: 364, Loss: 0.2850, Train Acc: 0.9312, Loss: 0.7047, Test Acc: 0.5750\n",
      "Epoch: 365, Loss: 0.2585, Train Acc: 0.9187, Loss: 0.5189, Test Acc: 0.6500\n",
      "Epoch: 366, Loss: 0.2280, Train Acc: 0.9375, Loss: 0.7071, Test Acc: 0.4750\n",
      "Epoch: 367, Loss: 0.2279, Train Acc: 0.9438, Loss: 0.4362, Test Acc: 0.6500\n",
      "Epoch: 368, Loss: 0.1891, Train Acc: 0.9375, Loss: 0.9892, Test Acc: 0.4750\n",
      "Epoch: 369, Loss: 0.2136, Train Acc: 0.9257, Loss: 0.4130, Test Acc: 0.6250\n",
      "Epoch: 370, Loss: 0.1785, Train Acc: 0.9312, Loss: 0.7977, Test Acc: 0.5500\n",
      "Epoch: 371, Loss: 0.1919, Train Acc: 0.9320, Loss: 0.3817, Test Acc: 0.6250\n",
      "Epoch: 372, Loss: 0.1708, Train Acc: 0.9437, Loss: 0.7615, Test Acc: 0.5750\n",
      "Epoch: 373, Loss: 0.1737, Train Acc: 0.9445, Loss: 0.4216, Test Acc: 0.6250\n",
      "Epoch: 374, Loss: 0.1471, Train Acc: 0.9563, Loss: 0.6564, Test Acc: 0.5750\n",
      "Epoch: 375, Loss: 0.1353, Train Acc: 0.9507, Loss: 0.4200, Test Acc: 0.6250\n",
      "Epoch: 376, Loss: 0.1230, Train Acc: 0.9750, Loss: 0.5566, Test Acc: 0.6000\n",
      "Epoch: 377, Loss: 0.1139, Train Acc: 0.9625, Loss: 0.4739, Test Acc: 0.6500\n",
      "Epoch: 378, Loss: 0.1130, Train Acc: 0.9750, Loss: 0.4383, Test Acc: 0.6000\n",
      "Epoch: 379, Loss: 0.1042, Train Acc: 0.9875, Loss: 0.5749, Test Acc: 0.6250\n",
      "Epoch: 380, Loss: 0.1027, Train Acc: 0.9750, Loss: 0.3855, Test Acc: 0.6250\n",
      "Epoch: 381, Loss: 0.1003, Train Acc: 0.9875, Loss: 0.5434, Test Acc: 0.6000\n",
      "Epoch: 382, Loss: 0.0938, Train Acc: 0.9750, Loss: 0.3685, Test Acc: 0.6250\n",
      "Epoch: 383, Loss: 0.0947, Train Acc: 0.9875, Loss: 0.5701, Test Acc: 0.6250\n",
      "Epoch: 384, Loss: 0.0864, Train Acc: 0.9750, Loss: 0.3860, Test Acc: 0.6250\n",
      "Epoch: 385, Loss: 0.0873, Train Acc: 0.9875, Loss: 0.5188, Test Acc: 0.6250\n",
      "Epoch: 386, Loss: 0.0823, Train Acc: 0.9812, Loss: 0.3952, Test Acc: 0.6250\n",
      "Epoch: 387, Loss: 0.0824, Train Acc: 0.9875, Loss: 0.5023, Test Acc: 0.6250\n",
      "Epoch: 388, Loss: 0.0794, Train Acc: 0.9812, Loss: 0.4062, Test Acc: 0.6250\n",
      "Epoch: 389, Loss: 0.0783, Train Acc: 0.9875, Loss: 0.5002, Test Acc: 0.6250\n",
      "Epoch: 390, Loss: 0.0752, Train Acc: 0.9875, Loss: 0.3687, Test Acc: 0.6250\n",
      "Epoch: 391, Loss: 0.0753, Train Acc: 0.9875, Loss: 0.4842, Test Acc: 0.6250\n",
      "Epoch: 392, Loss: 0.0710, Train Acc: 0.9938, Loss: 0.4333, Test Acc: 0.6250\n",
      "Epoch: 393, Loss: 0.0705, Train Acc: 0.9875, Loss: 0.4542, Test Acc: 0.6250\n",
      "Epoch: 394, Loss: 0.0681, Train Acc: 0.9938, Loss: 0.4402, Test Acc: 0.6250\n",
      "Epoch: 395, Loss: 0.0664, Train Acc: 0.9938, Loss: 0.4562, Test Acc: 0.6250\n",
      "Epoch: 396, Loss: 0.0644, Train Acc: 0.9875, Loss: 0.4455, Test Acc: 0.6250\n",
      "Epoch: 397, Loss: 0.0631, Train Acc: 1.0000, Loss: 0.4631, Test Acc: 0.6250\n",
      "Epoch: 398, Loss: 0.0619, Train Acc: 0.9875, Loss: 0.4690, Test Acc: 0.6250\n",
      "Epoch: 399, Loss: 0.0603, Train Acc: 0.9938, Loss: 0.4649, Test Acc: 0.6250\n",
      "Epoch: 400, Loss: 0.0596, Train Acc: 1.0000, Loss: 0.4776, Test Acc: 0.6250\n",
      "Epoch: 401, Loss: 0.0582, Train Acc: 1.0000, Loss: 0.4803, Test Acc: 0.6250\n",
      "Epoch: 402, Loss: 0.0565, Train Acc: 1.0000, Loss: 0.5151, Test Acc: 0.6250\n",
      "Epoch: 403, Loss: 0.0551, Train Acc: 1.0000, Loss: 0.4915, Test Acc: 0.6250\n",
      "Epoch: 404, Loss: 0.0551, Train Acc: 1.0000, Loss: 0.5123, Test Acc: 0.6250\n",
      "Epoch: 405, Loss: 0.0535, Train Acc: 1.0000, Loss: 0.5251, Test Acc: 0.6250\n",
      "Epoch: 406, Loss: 0.0518, Train Acc: 1.0000, Loss: 0.5085, Test Acc: 0.6250\n",
      "Epoch: 407, Loss: 0.0518, Train Acc: 1.0000, Loss: 0.5091, Test Acc: 0.6250\n",
      "Epoch: 408, Loss: 0.0508, Train Acc: 1.0000, Loss: 0.5281, Test Acc: 0.6250\n",
      "Epoch: 409, Loss: 0.0493, Train Acc: 1.0000, Loss: 0.5256, Test Acc: 0.6250\n",
      "Epoch: 410, Loss: 0.0487, Train Acc: 1.0000, Loss: 0.5277, Test Acc: 0.6250\n",
      "Epoch: 411, Loss: 0.0479, Train Acc: 1.0000, Loss: 0.5583, Test Acc: 0.6250\n",
      "Epoch: 412, Loss: 0.0464, Train Acc: 1.0000, Loss: 0.5567, Test Acc: 0.6250\n",
      "Epoch: 413, Loss: 0.0459, Train Acc: 1.0000, Loss: 0.5505, Test Acc: 0.6250\n",
      "Epoch: 414, Loss: 0.0455, Train Acc: 1.0000, Loss: 0.5896, Test Acc: 0.6250\n",
      "Epoch: 415, Loss: 0.0440, Train Acc: 1.0000, Loss: 0.5748, Test Acc: 0.6250\n",
      "Epoch: 416, Loss: 0.0435, Train Acc: 1.0000, Loss: 0.5696, Test Acc: 0.6250\n",
      "Epoch: 417, Loss: 0.0438, Train Acc: 1.0000, Loss: 0.5827, Test Acc: 0.6250\n",
      "Epoch: 418, Loss: 0.0425, Train Acc: 1.0000, Loss: 0.6279, Test Acc: 0.6250\n",
      "Epoch: 419, Loss: 0.0408, Train Acc: 1.0000, Loss: 0.5650, Test Acc: 0.6250\n",
      "Epoch: 420, Loss: 0.0413, Train Acc: 1.0000, Loss: 0.5996, Test Acc: 0.6250\n",
      "Epoch: 421, Loss: 0.0405, Train Acc: 1.0000, Loss: 0.6433, Test Acc: 0.6250\n",
      "Epoch: 422, Loss: 0.0386, Train Acc: 1.0000, Loss: 0.6185, Test Acc: 0.6250\n",
      "Epoch: 423, Loss: 0.0387, Train Acc: 1.0000, Loss: 0.6340, Test Acc: 0.6250\n",
      "Epoch: 424, Loss: 0.0378, Train Acc: 1.0000, Loss: 0.6828, Test Acc: 0.6250\n",
      "Epoch: 425, Loss: 0.0366, Train Acc: 1.0000, Loss: 0.6608, Test Acc: 0.6250\n",
      "Epoch: 426, Loss: 0.0362, Train Acc: 1.0000, Loss: 0.6117, Test Acc: 0.6250\n",
      "Epoch: 427, Loss: 0.0377, Train Acc: 1.0000, Loss: 0.6797, Test Acc: 0.6250\n",
      "Epoch: 428, Loss: 0.0355, Train Acc: 1.0000, Loss: 0.6914, Test Acc: 0.6250\n",
      "Epoch: 429, Loss: 0.0340, Train Acc: 1.0000, Loss: 0.5996, Test Acc: 0.6250\n",
      "Epoch: 430, Loss: 0.0360, Train Acc: 1.0000, Loss: 0.7150, Test Acc: 0.6250\n",
      "Epoch: 431, Loss: 0.0340, Train Acc: 1.0000, Loss: 0.7716, Test Acc: 0.6000\n",
      "Epoch: 432, Loss: 0.0331, Train Acc: 1.0000, Loss: 0.5431, Test Acc: 0.6250\n",
      "Epoch: 433, Loss: 0.0375, Train Acc: 1.0000, Loss: 0.7452, Test Acc: 0.6250\n",
      "Epoch: 434, Loss: 0.0345, Train Acc: 1.0000, Loss: 0.8699, Test Acc: 0.6250\n",
      "Epoch: 435, Loss: 0.0340, Train Acc: 1.0000, Loss: 0.5257, Test Acc: 0.6000\n",
      "Epoch: 436, Loss: 0.0382, Train Acc: 0.9812, Loss: 0.7205, Test Acc: 0.6500\n",
      "Epoch: 437, Loss: 0.0396, Train Acc: 1.0000, Loss: 0.8332, Test Acc: 0.6500\n",
      "Epoch: 438, Loss: 0.0330, Train Acc: 1.0000, Loss: 0.5319, Test Acc: 0.6250\n",
      "Epoch: 439, Loss: 0.0346, Train Acc: 0.9938, Loss: 0.6998, Test Acc: 0.6250\n",
      "Epoch: 440, Loss: 0.0369, Train Acc: 1.0000, Loss: 0.8787, Test Acc: 0.6250\n",
      "Epoch: 441, Loss: 0.0283, Train Acc: 1.0000, Loss: 0.5930, Test Acc: 0.6250\n",
      "Epoch: 442, Loss: 0.0326, Train Acc: 1.0000, Loss: 0.7592, Test Acc: 0.6500\n",
      "Epoch: 443, Loss: 0.0317, Train Acc: 1.0000, Loss: 1.0384, Test Acc: 0.6500\n",
      "Epoch: 444, Loss: 0.0299, Train Acc: 1.0000, Loss: 0.5783, Test Acc: 0.6250\n",
      "Epoch: 445, Loss: 0.0349, Train Acc: 0.9875, Loss: 0.7213, Test Acc: 0.6500\n",
      "Epoch: 446, Loss: 0.0383, Train Acc: 1.0000, Loss: 0.9973, Test Acc: 0.6500\n",
      "Epoch: 447, Loss: 0.0296, Train Acc: 0.9937, Loss: 0.6629, Test Acc: 0.6000\n",
      "Epoch: 448, Loss: 0.0336, Train Acc: 0.9875, Loss: 0.6139, Test Acc: 0.6750\n",
      "Epoch: 449, Loss: 0.0453, Train Acc: 0.9938, Loss: 0.8482, Test Acc: 0.6500\n",
      "Epoch: 450, Loss: 0.0316, Train Acc: 0.9875, Loss: 0.9846, Test Acc: 0.5750\n",
      "Epoch: 451, Loss: 0.0347, Train Acc: 1.0000, Loss: 0.5768, Test Acc: 0.6500\n",
      "Epoch: 452, Loss: 0.0368, Train Acc: 0.9812, Loss: 0.8275, Test Acc: 0.6750\n",
      "Epoch: 453, Loss: 0.0356, Train Acc: 0.9937, Loss: 1.1536, Test Acc: 0.6000\n",
      "Epoch: 454, Loss: 0.0353, Train Acc: 1.0000, Loss: 0.5937, Test Acc: 0.6250\n",
      "Epoch: 455, Loss: 0.0275, Train Acc: 1.0000, Loss: 0.8822, Test Acc: 0.6500\n",
      "Epoch: 456, Loss: 0.0328, Train Acc: 1.0000, Loss: 0.8956, Test Acc: 0.6500\n",
      "Epoch: 457, Loss: 0.0274, Train Acc: 0.9937, Loss: 1.0299, Test Acc: 0.5750\n",
      "Epoch: 458, Loss: 0.0279, Train Acc: 1.0000, Loss: 0.6941, Test Acc: 0.6750\n",
      "Epoch: 459, Loss: 0.0267, Train Acc: 1.0000, Loss: 1.1041, Test Acc: 0.6250\n",
      "Epoch: 460, Loss: 0.0211, Train Acc: 1.0000, Loss: 0.8913, Test Acc: 0.6250\n",
      "Epoch: 461, Loss: 0.0220, Train Acc: 1.0000, Loss: 0.7778, Test Acc: 0.6500\n",
      "Epoch: 462, Loss: 0.0312, Train Acc: 0.9938, Loss: 1.0251, Test Acc: 0.6750\n",
      "Epoch: 463, Loss: 0.0274, Train Acc: 0.9937, Loss: 1.3021, Test Acc: 0.5750\n",
      "Epoch: 464, Loss: 0.0371, Train Acc: 0.9937, Loss: 0.4266, Test Acc: 0.6750\n",
      "Epoch: 465, Loss: 0.0382, Train Acc: 1.0000, Loss: 0.9246, Test Acc: 0.6750\n",
      "Epoch: 466, Loss: 0.0289, Train Acc: 1.0000, Loss: 1.2336, Test Acc: 0.6000\n",
      "Epoch: 467, Loss: 0.0314, Train Acc: 0.9938, Loss: 0.8791, Test Acc: 0.6250\n",
      "Epoch: 468, Loss: 0.0245, Train Acc: 0.9938, Loss: 0.9403, Test Acc: 0.6750\n",
      "Epoch: 469, Loss: 0.0405, Train Acc: 0.9875, Loss: 0.8401, Test Acc: 0.7000\n",
      "Epoch: 470, Loss: 0.0325, Train Acc: 0.9937, Loss: 1.2947, Test Acc: 0.6000\n",
      "Epoch: 471, Loss: 0.0400, Train Acc: 0.9875, Loss: 0.7907, Test Acc: 0.6500\n",
      "Epoch: 472, Loss: 0.0369, Train Acc: 1.0000, Loss: 0.6407, Test Acc: 0.6250\n",
      "Epoch: 473, Loss: 0.0588, Train Acc: 0.9938, Loss: 1.5556, Test Acc: 0.6250\n",
      "Epoch: 474, Loss: 0.0872, Train Acc: 0.8445, Loss: 0.3734, Test Acc: 0.7000\n",
      "Epoch: 475, Loss: 0.1956, Train Acc: 0.8882, Loss: 0.4600, Test Acc: 0.7000\n",
      "Epoch: 476, Loss: 0.1192, Train Acc: 0.9688, Loss: 1.6129, Test Acc: 0.5500\n",
      "Epoch: 477, Loss: 0.1888, Train Acc: 0.8757, Loss: 0.3638, Test Acc: 0.6250\n",
      "Epoch: 478, Loss: 0.6469, Train Acc: 0.6926, Loss: 0.0096, Test Acc: 0.7750\n",
      "Epoch: 479, Loss: 0.7323, Train Acc: 0.8882, Loss: 0.7326, Test Acc: 0.6000\n",
      "Epoch: 480, Loss: 0.3230, Train Acc: 0.8077, Loss: 2.6325, Test Acc: 0.5500\n",
      "Epoch: 481, Loss: 0.3506, Train Acc: 0.8577, Loss: 1.4932, Test Acc: 0.6250\n",
      "Epoch: 482, Loss: 0.3031, Train Acc: 0.9250, Loss: 0.2458, Test Acc: 0.7000\n",
      "Epoch: 483, Loss: 0.2169, Train Acc: 0.9077, Loss: 0.8507, Test Acc: 0.6250\n",
      "Epoch: 484, Loss: 0.1853, Train Acc: 0.9750, Loss: 0.3615, Test Acc: 0.6750\n",
      "Epoch: 485, Loss: 0.1896, Train Acc: 0.9132, Loss: 0.8116, Test Acc: 0.6750\n",
      "Epoch: 486, Loss: 0.1807, Train Acc: 0.9563, Loss: 0.9656, Test Acc: 0.5250\n",
      "Epoch: 487, Loss: 0.1157, Train Acc: 0.9688, Loss: 0.5909, Test Acc: 0.6250\n",
      "Epoch: 488, Loss: 0.0789, Train Acc: 0.9875, Loss: 0.3609, Test Acc: 0.6250\n",
      "Epoch: 489, Loss: 0.0675, Train Acc: 0.9812, Loss: 0.2548, Test Acc: 0.6500\n",
      "Epoch: 490, Loss: 0.0684, Train Acc: 1.0000, Loss: 0.4785, Test Acc: 0.6250\n",
      "Epoch: 491, Loss: 0.0509, Train Acc: 0.9938, Loss: 0.5848, Test Acc: 0.6250\n",
      "Epoch: 492, Loss: 0.0481, Train Acc: 1.0000, Loss: 0.5230, Test Acc: 0.6500\n",
      "Epoch: 493, Loss: 0.0447, Train Acc: 0.9937, Loss: 0.5055, Test Acc: 0.6250\n",
      "Epoch: 494, Loss: 0.0451, Train Acc: 0.9938, Loss: 0.3861, Test Acc: 0.6500\n",
      "Epoch: 495, Loss: 0.0409, Train Acc: 1.0000, Loss: 0.5825, Test Acc: 0.6250\n",
      "Epoch: 496, Loss: 0.0437, Train Acc: 1.0000, Loss: 0.3886, Test Acc: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 497, Loss: 0.0455, Train Acc: 0.9938, Loss: 0.5283, Test Acc: 0.6250\n",
      "Epoch: 498, Loss: 0.0376, Train Acc: 1.0000, Loss: 0.6557, Test Acc: 0.6250\n",
      "Epoch: 499, Loss: 0.0357, Train Acc: 1.0000, Loss: 0.5827, Test Acc: 0.6500\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  71  74\n",
      "  78 107 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180] TEST: [ 70  72  73  75  76  77  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 108 109]\n",
      "145\n",
      "36\n",
      "Epoch: 001, Loss: 0.9202, Train Acc: 0.5151, Loss: 0.7023, Test Acc: 0.5250\n",
      "Epoch: 002, Loss: 0.8160, Train Acc: 0.5033, Loss: 1.2625, Test Acc: 0.4750\n",
      "Epoch: 003, Loss: 0.7271, Train Acc: 0.4967, Loss: 0.4911, Test Acc: 0.5250\n",
      "Epoch: 004, Loss: 0.7419, Train Acc: 0.5033, Loss: 0.9200, Test Acc: 0.4750\n",
      "Epoch: 005, Loss: 0.6992, Train Acc: 0.4901, Loss: 0.7113, Test Acc: 0.4750\n",
      "Epoch: 006, Loss: 0.7213, Train Acc: 0.5460, Loss: 0.6799, Test Acc: 0.5750\n",
      "Epoch: 007, Loss: 0.7015, Train Acc: 0.5033, Loss: 0.8329, Test Acc: 0.4750\n",
      "Epoch: 008, Loss: 0.7017, Train Acc: 0.5460, Loss: 0.6747, Test Acc: 0.5500\n",
      "Epoch: 009, Loss: 0.7091, Train Acc: 0.5151, Loss: 0.7576, Test Acc: 0.4750\n",
      "Epoch: 010, Loss: 0.6992, Train Acc: 0.5151, Loss: 0.7535, Test Acc: 0.4750\n",
      "Epoch: 011, Loss: 0.7041, Train Acc: 0.5136, Loss: 0.7039, Test Acc: 0.5000\n",
      "Epoch: 012, Loss: 0.7033, Train Acc: 0.5151, Loss: 0.7593, Test Acc: 0.4750\n",
      "Epoch: 013, Loss: 0.7006, Train Acc: 0.5213, Loss: 0.7261, Test Acc: 0.4750\n",
      "Epoch: 014, Loss: 0.7038, Train Acc: 0.5213, Loss: 0.7278, Test Acc: 0.4750\n",
      "Epoch: 015, Loss: 0.7015, Train Acc: 0.5151, Loss: 0.7457, Test Acc: 0.4750\n",
      "Epoch: 016, Loss: 0.7016, Train Acc: 0.5276, Loss: 0.7245, Test Acc: 0.4750\n",
      "Epoch: 017, Loss: 0.7026, Train Acc: 0.5213, Loss: 0.7360, Test Acc: 0.4750\n",
      "Epoch: 018, Loss: 0.7013, Train Acc: 0.5151, Loss: 0.7350, Test Acc: 0.4750\n",
      "Epoch: 019, Loss: 0.7018, Train Acc: 0.5151, Loss: 0.7287, Test Acc: 0.4750\n",
      "Epoch: 020, Loss: 0.7016, Train Acc: 0.5151, Loss: 0.7350, Test Acc: 0.4750\n",
      "Epoch: 021, Loss: 0.7013, Train Acc: 0.5151, Loss: 0.7321, Test Acc: 0.4750\n",
      "Epoch: 022, Loss: 0.7015, Train Acc: 0.5151, Loss: 0.7324, Test Acc: 0.4750\n",
      "Epoch: 023, Loss: 0.7012, Train Acc: 0.5151, Loss: 0.7325, Test Acc: 0.4750\n",
      "Epoch: 024, Loss: 0.7012, Train Acc: 0.5151, Loss: 0.7319, Test Acc: 0.4750\n",
      "Epoch: 025, Loss: 0.7010, Train Acc: 0.5151, Loss: 0.7296, Test Acc: 0.4750\n",
      "Epoch: 026, Loss: 0.7009, Train Acc: 0.5151, Loss: 0.7307, Test Acc: 0.4750\n",
      "Epoch: 027, Loss: 0.7006, Train Acc: 0.5151, Loss: 0.7304, Test Acc: 0.4750\n",
      "Epoch: 028, Loss: 0.7006, Train Acc: 0.5151, Loss: 0.7292, Test Acc: 0.4750\n",
      "Epoch: 029, Loss: 0.7006, Train Acc: 0.5151, Loss: 0.7305, Test Acc: 0.4750\n",
      "Epoch: 030, Loss: 0.7016, Train Acc: 0.5151, Loss: 0.7378, Test Acc: 0.4750\n",
      "Epoch: 031, Loss: 0.7004, Train Acc: 0.5213, Loss: 0.7245, Test Acc: 0.4750\n",
      "Epoch: 032, Loss: 0.7008, Train Acc: 0.5151, Loss: 0.7321, Test Acc: 0.4750\n",
      "Epoch: 033, Loss: 0.7000, Train Acc: 0.5151, Loss: 0.7299, Test Acc: 0.4750\n",
      "Epoch: 034, Loss: 0.7002, Train Acc: 0.5151, Loss: 0.7255, Test Acc: 0.4750\n",
      "Epoch: 035, Loss: 0.7000, Train Acc: 0.5151, Loss: 0.7293, Test Acc: 0.4750\n",
      "Epoch: 036, Loss: 0.6997, Train Acc: 0.5151, Loss: 0.7283, Test Acc: 0.4750\n",
      "Epoch: 037, Loss: 0.6998, Train Acc: 0.5151, Loss: 0.7266, Test Acc: 0.4750\n",
      "Epoch: 038, Loss: 0.6996, Train Acc: 0.5151, Loss: 0.7273, Test Acc: 0.4750\n",
      "Epoch: 039, Loss: 0.6994, Train Acc: 0.5151, Loss: 0.7264, Test Acc: 0.4750\n",
      "Epoch: 040, Loss: 0.6995, Train Acc: 0.5151, Loss: 0.7270, Test Acc: 0.4750\n",
      "Epoch: 041, Loss: 0.6991, Train Acc: 0.5213, Loss: 0.7246, Test Acc: 0.4750\n",
      "Epoch: 042, Loss: 0.6992, Train Acc: 0.5151, Loss: 0.7262, Test Acc: 0.4750\n",
      "Epoch: 043, Loss: 0.6988, Train Acc: 0.5213, Loss: 0.7252, Test Acc: 0.4750\n",
      "Epoch: 044, Loss: 0.6989, Train Acc: 0.5213, Loss: 0.7230, Test Acc: 0.4750\n",
      "Epoch: 045, Loss: 0.6987, Train Acc: 0.5213, Loss: 0.7252, Test Acc: 0.4750\n",
      "Epoch: 046, Loss: 0.6984, Train Acc: 0.5213, Loss: 0.7233, Test Acc: 0.4750\n",
      "Epoch: 047, Loss: 0.6984, Train Acc: 0.5276, Loss: 0.7226, Test Acc: 0.4750\n",
      "Epoch: 048, Loss: 0.6983, Train Acc: 0.5213, Loss: 0.7249, Test Acc: 0.4750\n",
      "Epoch: 049, Loss: 0.6980, Train Acc: 0.5213, Loss: 0.7224, Test Acc: 0.4750\n",
      "Epoch: 050, Loss: 0.6980, Train Acc: 0.5276, Loss: 0.7207, Test Acc: 0.4750\n",
      "Epoch: 051, Loss: 0.6984, Train Acc: 0.5213, Loss: 0.7220, Test Acc: 0.4750\n",
      "Epoch: 052, Loss: 0.6979, Train Acc: 0.5276, Loss: 0.7177, Test Acc: 0.4750\n",
      "Epoch: 053, Loss: 0.6979, Train Acc: 0.5213, Loss: 0.7242, Test Acc: 0.4750\n",
      "Epoch: 054, Loss: 0.6972, Train Acc: 0.5213, Loss: 0.7218, Test Acc: 0.4750\n",
      "Epoch: 055, Loss: 0.6974, Train Acc: 0.5151, Loss: 0.7182, Test Acc: 0.4750\n",
      "Epoch: 056, Loss: 0.6971, Train Acc: 0.5276, Loss: 0.7182, Test Acc: 0.5000\n",
      "Epoch: 057, Loss: 0.6972, Train Acc: 0.5213, Loss: 0.7220, Test Acc: 0.4750\n",
      "Epoch: 058, Loss: 0.6966, Train Acc: 0.5213, Loss: 0.7202, Test Acc: 0.5000\n",
      "Epoch: 059, Loss: 0.6969, Train Acc: 0.5151, Loss: 0.7162, Test Acc: 0.4500\n",
      "Epoch: 060, Loss: 0.6967, Train Acc: 0.5151, Loss: 0.7182, Test Acc: 0.5000\n",
      "Epoch: 061, Loss: 0.6964, Train Acc: 0.5213, Loss: 0.7215, Test Acc: 0.5000\n",
      "Epoch: 062, Loss: 0.6960, Train Acc: 0.5088, Loss: 0.7139, Test Acc: 0.5000\n",
      "Epoch: 063, Loss: 0.6966, Train Acc: 0.5088, Loss: 0.7176, Test Acc: 0.4750\n",
      "Epoch: 064, Loss: 0.6957, Train Acc: 0.5151, Loss: 0.7187, Test Acc: 0.5000\n",
      "Epoch: 065, Loss: 0.6958, Train Acc: 0.5143, Loss: 0.7129, Test Acc: 0.5250\n",
      "Epoch: 066, Loss: 0.6958, Train Acc: 0.5088, Loss: 0.7183, Test Acc: 0.4500\n",
      "Epoch: 067, Loss: 0.6950, Train Acc: 0.5143, Loss: 0.7150, Test Acc: 0.4750\n",
      "Epoch: 068, Loss: 0.6953, Train Acc: 0.5081, Loss: 0.7141, Test Acc: 0.5000\n",
      "Epoch: 069, Loss: 0.6948, Train Acc: 0.5081, Loss: 0.7156, Test Acc: 0.5000\n",
      "Epoch: 070, Loss: 0.6950, Train Acc: 0.5081, Loss: 0.7129, Test Acc: 0.5250\n",
      "Epoch: 071, Loss: 0.6946, Train Acc: 0.5081, Loss: 0.7123, Test Acc: 0.5000\n",
      "Epoch: 072, Loss: 0.6944, Train Acc: 0.5199, Loss: 0.7129, Test Acc: 0.5250\n",
      "Epoch: 073, Loss: 0.6943, Train Acc: 0.5143, Loss: 0.7139, Test Acc: 0.5250\n",
      "Epoch: 074, Loss: 0.6938, Train Acc: 0.5434, Loss: 0.7097, Test Acc: 0.5750\n",
      "Epoch: 075, Loss: 0.6943, Train Acc: 0.5379, Loss: 0.7173, Test Acc: 0.5500\n",
      "Epoch: 076, Loss: 0.6933, Train Acc: 0.5434, Loss: 0.7119, Test Acc: 0.5250\n",
      "Epoch: 077, Loss: 0.6936, Train Acc: 0.5496, Loss: 0.7063, Test Acc: 0.5750\n",
      "Epoch: 078, Loss: 0.6934, Train Acc: 0.5434, Loss: 0.7114, Test Acc: 0.5750\n",
      "Epoch: 079, Loss: 0.6931, Train Acc: 0.5316, Loss: 0.7122, Test Acc: 0.5500\n",
      "Epoch: 080, Loss: 0.6929, Train Acc: 0.5559, Loss: 0.7057, Test Acc: 0.5250\n",
      "Epoch: 081, Loss: 0.6930, Train Acc: 0.5434, Loss: 0.7107, Test Acc: 0.6000\n",
      "Epoch: 082, Loss: 0.6923, Train Acc: 0.5379, Loss: 0.7104, Test Acc: 0.6000\n",
      "Epoch: 083, Loss: 0.6924, Train Acc: 0.5559, Loss: 0.7016, Test Acc: 0.5000\n",
      "Epoch: 084, Loss: 0.6928, Train Acc: 0.5316, Loss: 0.7129, Test Acc: 0.5750\n",
      "Epoch: 085, Loss: 0.6914, Train Acc: 0.5309, Loss: 0.7060, Test Acc: 0.5500\n",
      "Epoch: 086, Loss: 0.6929, Train Acc: 0.5316, Loss: 0.7056, Test Acc: 0.6000\n",
      "Epoch: 087, Loss: 0.6914, Train Acc: 0.5316, Loss: 0.7092, Test Acc: 0.5750\n",
      "Epoch: 088, Loss: 0.6915, Train Acc: 0.5371, Loss: 0.7039, Test Acc: 0.5250\n",
      "Epoch: 089, Loss: 0.6914, Train Acc: 0.5379, Loss: 0.7119, Test Acc: 0.5750\n",
      "Epoch: 090, Loss: 0.6906, Train Acc: 0.5746, Loss: 0.7003, Test Acc: 0.4750\n",
      "Epoch: 091, Loss: 0.6915, Train Acc: 0.5441, Loss: 0.7070, Test Acc: 0.5500\n",
      "Epoch: 092, Loss: 0.6899, Train Acc: 0.5379, Loss: 0.7054, Test Acc: 0.5250\n",
      "Epoch: 093, Loss: 0.6905, Train Acc: 0.5864, Loss: 0.7007, Test Acc: 0.5000\n",
      "Epoch: 094, Loss: 0.6898, Train Acc: 0.5441, Loss: 0.7039, Test Acc: 0.5000\n",
      "Epoch: 095, Loss: 0.6895, Train Acc: 0.5504, Loss: 0.7033, Test Acc: 0.5000\n",
      "Epoch: 096, Loss: 0.6893, Train Acc: 0.5864, Loss: 0.7007, Test Acc: 0.5000\n",
      "Epoch: 097, Loss: 0.6893, Train Acc: 0.5441, Loss: 0.7041, Test Acc: 0.5000\n",
      "Epoch: 098, Loss: 0.6886, Train Acc: 0.5864, Loss: 0.6993, Test Acc: 0.5000\n",
      "Epoch: 099, Loss: 0.6888, Train Acc: 0.5926, Loss: 0.6989, Test Acc: 0.5000\n",
      "Epoch: 100, Loss: 0.6881, Train Acc: 0.5621, Loss: 0.7036, Test Acc: 0.5000\n",
      "Epoch: 101, Loss: 0.6876, Train Acc: 0.6051, Loss: 0.6961, Test Acc: 0.5000\n",
      "Epoch: 102, Loss: 0.6883, Train Acc: 0.5684, Loss: 0.7014, Test Acc: 0.5000\n",
      "Epoch: 103, Loss: 0.6871, Train Acc: 0.6176, Loss: 0.6951, Test Acc: 0.5500\n",
      "Epoch: 104, Loss: 0.6878, Train Acc: 0.5746, Loss: 0.7043, Test Acc: 0.5000\n",
      "Epoch: 105, Loss: 0.6862, Train Acc: 0.6051, Loss: 0.6913, Test Acc: 0.5250\n",
      "Epoch: 106, Loss: 0.6872, Train Acc: 0.6114, Loss: 0.6966, Test Acc: 0.5500\n",
      "Epoch: 107, Loss: 0.6855, Train Acc: 0.5746, Loss: 0.7002, Test Acc: 0.5000\n",
      "Epoch: 108, Loss: 0.6859, Train Acc: 0.6044, Loss: 0.6890, Test Acc: 0.5500\n",
      "Epoch: 109, Loss: 0.6860, Train Acc: 0.5746, Loss: 0.7024, Test Acc: 0.5000\n",
      "Epoch: 110, Loss: 0.6845, Train Acc: 0.6176, Loss: 0.6945, Test Acc: 0.5500\n",
      "Epoch: 111, Loss: 0.6852, Train Acc: 0.5989, Loss: 0.6908, Test Acc: 0.5500\n",
      "Epoch: 112, Loss: 0.6845, Train Acc: 0.5871, Loss: 0.7019, Test Acc: 0.5000\n",
      "Epoch: 113, Loss: 0.6837, Train Acc: 0.6169, Loss: 0.6901, Test Acc: 0.5500\n",
      "Epoch: 114, Loss: 0.6843, Train Acc: 0.6059, Loss: 0.6965, Test Acc: 0.5500\n",
      "Epoch: 115, Loss: 0.6831, Train Acc: 0.6114, Loss: 0.6958, Test Acc: 0.5500\n",
      "Epoch: 116, Loss: 0.6836, Train Acc: 0.6114, Loss: 0.6936, Test Acc: 0.5500\n",
      "Epoch: 117, Loss: 0.6822, Train Acc: 0.6051, Loss: 0.6936, Test Acc: 0.5500\n",
      "Epoch: 118, Loss: 0.6832, Train Acc: 0.5934, Loss: 0.6984, Test Acc: 0.5000\n",
      "Epoch: 119, Loss: 0.6817, Train Acc: 0.6419, Loss: 0.6911, Test Acc: 0.5500\n",
      "Epoch: 120, Loss: 0.6825, Train Acc: 0.5934, Loss: 0.6998, Test Acc: 0.5000\n",
      "Epoch: 121, Loss: 0.6806, Train Acc: 0.6121, Loss: 0.6918, Test Acc: 0.5500\n",
      "Epoch: 122, Loss: 0.6821, Train Acc: 0.6059, Loss: 0.6992, Test Acc: 0.5000\n",
      "Epoch: 123, Loss: 0.6796, Train Acc: 0.6059, Loss: 0.6967, Test Acc: 0.5000\n",
      "Epoch: 124, Loss: 0.6804, Train Acc: 0.6121, Loss: 0.6932, Test Acc: 0.5000\n",
      "Epoch: 125, Loss: 0.6794, Train Acc: 0.6121, Loss: 0.6949, Test Acc: 0.5500\n",
      "Epoch: 126, Loss: 0.6798, Train Acc: 0.5996, Loss: 0.7041, Test Acc: 0.4750\n",
      "Epoch: 127, Loss: 0.6782, Train Acc: 0.6287, Loss: 0.6879, Test Acc: 0.5500\n",
      "Epoch: 128, Loss: 0.6799, Train Acc: 0.5746, Loss: 0.7106, Test Acc: 0.4750\n",
      "Epoch: 129, Loss: 0.6767, Train Acc: 0.6162, Loss: 0.6813, Test Acc: 0.6000\n",
      "Epoch: 130, Loss: 0.6811, Train Acc: 0.5871, Loss: 0.7140, Test Acc: 0.4750\n",
      "Epoch: 131, Loss: 0.6754, Train Acc: 0.6051, Loss: 0.6918, Test Acc: 0.5000\n",
      "Epoch: 132, Loss: 0.6777, Train Acc: 0.6349, Loss: 0.6864, Test Acc: 0.6000\n",
      "Epoch: 133, Loss: 0.6746, Train Acc: 0.6121, Loss: 0.7093, Test Acc: 0.4750\n",
      "Epoch: 134, Loss: 0.6753, Train Acc: 0.6349, Loss: 0.6785, Test Acc: 0.5750\n",
      "Epoch: 135, Loss: 0.6767, Train Acc: 0.5996, Loss: 0.7143, Test Acc: 0.4750\n",
      "Epoch: 136, Loss: 0.6733, Train Acc: 0.6224, Loss: 0.6822, Test Acc: 0.5750\n",
      "Epoch: 137, Loss: 0.6754, Train Acc: 0.6364, Loss: 0.7099, Test Acc: 0.4750\n",
      "Epoch: 138, Loss: 0.6716, Train Acc: 0.6357, Loss: 0.6975, Test Acc: 0.4750\n",
      "Epoch: 139, Loss: 0.6742, Train Acc: 0.6419, Loss: 0.6946, Test Acc: 0.5500\n",
      "Epoch: 140, Loss: 0.6710, Train Acc: 0.6364, Loss: 0.7036, Test Acc: 0.5500\n",
      "Epoch: 141, Loss: 0.6735, Train Acc: 0.6551, Loss: 0.7091, Test Acc: 0.5250\n",
      "Epoch: 142, Loss: 0.6703, Train Acc: 0.6482, Loss: 0.6915, Test Acc: 0.5750\n",
      "Epoch: 143, Loss: 0.6712, Train Acc: 0.6419, Loss: 0.7008, Test Acc: 0.5250\n",
      "Epoch: 144, Loss: 0.6695, Train Acc: 0.6551, Loss: 0.7116, Test Acc: 0.4750\n",
      "Epoch: 145, Loss: 0.6687, Train Acc: 0.6349, Loss: 0.6921, Test Acc: 0.5000\n",
      "Epoch: 146, Loss: 0.6691, Train Acc: 0.6419, Loss: 0.7000, Test Acc: 0.5000\n",
      "Epoch: 147, Loss: 0.6669, Train Acc: 0.6537, Loss: 0.7013, Test Acc: 0.5000\n",
      "Epoch: 148, Loss: 0.6677, Train Acc: 0.6537, Loss: 0.6961, Test Acc: 0.5000\n",
      "Epoch: 149, Loss: 0.6667, Train Acc: 0.6419, Loss: 0.7020, Test Acc: 0.5000\n",
      "Epoch: 150, Loss: 0.6663, Train Acc: 0.6474, Loss: 0.6938, Test Acc: 0.5000\n",
      "Epoch: 151, Loss: 0.6661, Train Acc: 0.6599, Loss: 0.6998, Test Acc: 0.5000\n",
      "Epoch: 152, Loss: 0.6645, Train Acc: 0.6787, Loss: 0.7085, Test Acc: 0.5250\n",
      "Epoch: 153, Loss: 0.6646, Train Acc: 0.6599, Loss: 0.6986, Test Acc: 0.5250\n",
      "Epoch: 154, Loss: 0.6634, Train Acc: 0.6669, Loss: 0.7074, Test Acc: 0.5250\n",
      "Epoch: 155, Loss: 0.6641, Train Acc: 0.6599, Loss: 0.7028, Test Acc: 0.5750\n",
      "Epoch: 156, Loss: 0.6619, Train Acc: 0.6669, Loss: 0.7183, Test Acc: 0.5000\n",
      "Epoch: 157, Loss: 0.6642, Train Acc: 0.6224, Loss: 0.6790, Test Acc: 0.6750\n",
      "Epoch: 158, Loss: 0.6642, Train Acc: 0.6059, Loss: 0.7457, Test Acc: 0.4750\n",
      "Epoch: 159, Loss: 0.6633, Train Acc: 0.5919, Loss: 0.6478, Test Acc: 0.6250\n",
      "Epoch: 160, Loss: 0.6647, Train Acc: 0.6059, Loss: 0.7587, Test Acc: 0.5250\n",
      "Epoch: 161, Loss: 0.6618, Train Acc: 0.5912, Loss: 0.6563, Test Acc: 0.6750\n",
      "Epoch: 162, Loss: 0.6648, Train Acc: 0.6121, Loss: 0.7585, Test Acc: 0.4750\n",
      "Epoch: 163, Loss: 0.6580, Train Acc: 0.5982, Loss: 0.6676, Test Acc: 0.6500\n",
      "Epoch: 164, Loss: 0.6620, Train Acc: 0.6184, Loss: 0.7599, Test Acc: 0.4750\n",
      "Epoch: 165, Loss: 0.6563, Train Acc: 0.6099, Loss: 0.6633, Test Acc: 0.6500\n",
      "Epoch: 166, Loss: 0.6601, Train Acc: 0.6426, Loss: 0.7499, Test Acc: 0.4750\n",
      "Epoch: 167, Loss: 0.6517, Train Acc: 0.6037, Loss: 0.6727, Test Acc: 0.6500\n",
      "Epoch: 168, Loss: 0.6600, Train Acc: 0.6184, Loss: 0.7642, Test Acc: 0.5000\n",
      "Epoch: 169, Loss: 0.6544, Train Acc: 0.6037, Loss: 0.6558, Test Acc: 0.6500\n",
      "Epoch: 170, Loss: 0.6551, Train Acc: 0.6544, Loss: 0.7382, Test Acc: 0.5000\n",
      "Epoch: 171, Loss: 0.6511, Train Acc: 0.6912, Loss: 0.7126, Test Acc: 0.5750\n",
      "Epoch: 172, Loss: 0.6527, Train Acc: 0.6974, Loss: 0.7163, Test Acc: 0.5750\n",
      "Epoch: 173, Loss: 0.6493, Train Acc: 0.6974, Loss: 0.7128, Test Acc: 0.5250\n",
      "Epoch: 174, Loss: 0.6499, Train Acc: 0.6787, Loss: 0.6970, Test Acc: 0.6250\n",
      "Epoch: 175, Loss: 0.6478, Train Acc: 0.6912, Loss: 0.7322, Test Acc: 0.5000\n",
      "Epoch: 176, Loss: 0.6477, Train Acc: 0.6912, Loss: 0.7065, Test Acc: 0.5750\n",
      "Epoch: 177, Loss: 0.6464, Train Acc: 0.6849, Loss: 0.7259, Test Acc: 0.5000\n",
      "Epoch: 178, Loss: 0.6451, Train Acc: 0.6849, Loss: 0.7192, Test Acc: 0.5500\n",
      "Epoch: 179, Loss: 0.6441, Train Acc: 0.6912, Loss: 0.7245, Test Acc: 0.5000\n",
      "Epoch: 180, Loss: 0.6430, Train Acc: 0.6912, Loss: 0.7170, Test Acc: 0.5000\n",
      "Epoch: 181, Loss: 0.6428, Train Acc: 0.6849, Loss: 0.7304, Test Acc: 0.5000\n",
      "Epoch: 182, Loss: 0.6409, Train Acc: 0.6912, Loss: 0.7211, Test Acc: 0.5000\n",
      "Epoch: 183, Loss: 0.6388, Train Acc: 0.6912, Loss: 0.7279, Test Acc: 0.5000\n",
      "Epoch: 184, Loss: 0.6406, Train Acc: 0.6912, Loss: 0.7339, Test Acc: 0.5000\n",
      "Epoch: 185, Loss: 0.6366, Train Acc: 0.6974, Loss: 0.7203, Test Acc: 0.5500\n",
      "Epoch: 186, Loss: 0.6366, Train Acc: 0.6849, Loss: 0.7453, Test Acc: 0.5250\n",
      "Epoch: 187, Loss: 0.6366, Train Acc: 0.6974, Loss: 0.7239, Test Acc: 0.5250\n",
      "Epoch: 188, Loss: 0.6341, Train Acc: 0.6974, Loss: 0.7432, Test Acc: 0.5000\n",
      "Epoch: 189, Loss: 0.6328, Train Acc: 0.6912, Loss: 0.7453, Test Acc: 0.5250\n",
      "Epoch: 190, Loss: 0.6345, Train Acc: 0.6912, Loss: 0.7274, Test Acc: 0.5500\n",
      "Epoch: 191, Loss: 0.6327, Train Acc: 0.6732, Loss: 0.7783, Test Acc: 0.5750\n",
      "Epoch: 192, Loss: 0.6393, Train Acc: 0.6732, Loss: 0.6836, Test Acc: 0.6000\n",
      "Epoch: 193, Loss: 0.6310, Train Acc: 0.6607, Loss: 0.8318, Test Acc: 0.5750\n",
      "Epoch: 194, Loss: 0.6403, Train Acc: 0.6849, Loss: 0.7022, Test Acc: 0.5500\n",
      "Epoch: 195, Loss: 0.6180, Train Acc: 0.6912, Loss: 0.7816, Test Acc: 0.5750\n",
      "Epoch: 196, Loss: 0.6326, Train Acc: 0.6912, Loss: 0.7372, Test Acc: 0.5750\n",
      "Epoch: 197, Loss: 0.6215, Train Acc: 0.6849, Loss: 0.7048, Test Acc: 0.6000\n",
      "Epoch: 198, Loss: 0.6336, Train Acc: 0.6419, Loss: 0.8624, Test Acc: 0.6250\n",
      "Epoch: 199, Loss: 0.6365, Train Acc: 0.6357, Loss: 0.6633, Test Acc: 0.5250\n",
      "Epoch: 200, Loss: 0.6163, Train Acc: 0.6732, Loss: 0.8620, Test Acc: 0.6000\n",
      "Epoch: 201, Loss: 0.6278, Train Acc: 0.6849, Loss: 0.7218, Test Acc: 0.5000\n",
      "Epoch: 202, Loss: 0.6128, Train Acc: 0.7099, Loss: 0.7341, Test Acc: 0.6250\n",
      "Epoch: 203, Loss: 0.6307, Train Acc: 0.6482, Loss: 0.9027, Test Acc: 0.6000\n",
      "Epoch: 204, Loss: 0.6287, Train Acc: 0.6482, Loss: 0.6512, Test Acc: 0.5000\n",
      "Epoch: 205, Loss: 0.6033, Train Acc: 0.6912, Loss: 0.7887, Test Acc: 0.6000\n",
      "Epoch: 206, Loss: 0.6208, Train Acc: 0.7287, Loss: 0.7866, Test Acc: 0.6000\n",
      "Epoch: 207, Loss: 0.6167, Train Acc: 0.7217, Loss: 0.7022, Test Acc: 0.5750\n",
      "Epoch: 208, Loss: 0.6070, Train Acc: 0.6974, Loss: 0.8455, Test Acc: 0.6250\n",
      "Epoch: 209, Loss: 0.6153, Train Acc: 0.7099, Loss: 0.7025, Test Acc: 0.6000\n",
      "Epoch: 210, Loss: 0.6031, Train Acc: 0.6974, Loss: 0.8699, Test Acc: 0.6000\n",
      "Epoch: 211, Loss: 0.6191, Train Acc: 0.7037, Loss: 0.7236, Test Acc: 0.5000\n",
      "Epoch: 212, Loss: 0.5865, Train Acc: 0.7217, Loss: 0.7611, Test Acc: 0.5500\n",
      "Epoch: 213, Loss: 0.6054, Train Acc: 0.7224, Loss: 0.8227, Test Acc: 0.6000\n",
      "Epoch: 214, Loss: 0.5942, Train Acc: 0.7279, Loss: 0.7131, Test Acc: 0.5750\n",
      "Epoch: 215, Loss: 0.5901, Train Acc: 0.7224, Loss: 0.8236, Test Acc: 0.6000\n",
      "Epoch: 216, Loss: 0.6013, Train Acc: 0.7279, Loss: 0.8000, Test Acc: 0.5500\n",
      "Epoch: 217, Loss: 0.5818, Train Acc: 0.7154, Loss: 0.7730, Test Acc: 0.5000\n",
      "Epoch: 218, Loss: 0.5937, Train Acc: 0.7287, Loss: 0.8237, Test Acc: 0.5750\n",
      "Epoch: 219, Loss: 0.5858, Train Acc: 0.7217, Loss: 0.7807, Test Acc: 0.5500\n",
      "Epoch: 220, Loss: 0.5783, Train Acc: 0.7342, Loss: 0.7927, Test Acc: 0.5500\n",
      "Epoch: 221, Loss: 0.5812, Train Acc: 0.7217, Loss: 0.7811, Test Acc: 0.5500\n",
      "Epoch: 222, Loss: 0.5768, Train Acc: 0.7467, Loss: 0.8165, Test Acc: 0.5750\n",
      "Epoch: 223, Loss: 0.5872, Train Acc: 0.7342, Loss: 0.7956, Test Acc: 0.5750\n",
      "Epoch: 224, Loss: 0.5698, Train Acc: 0.7404, Loss: 0.8189, Test Acc: 0.5750\n",
      "Epoch: 225, Loss: 0.5775, Train Acc: 0.7404, Loss: 0.7979, Test Acc: 0.5500\n",
      "Epoch: 226, Loss: 0.5738, Train Acc: 0.7404, Loss: 0.8704, Test Acc: 0.5750\n",
      "Epoch: 227, Loss: 0.5878, Train Acc: 0.7529, Loss: 0.8429, Test Acc: 0.5750\n",
      "Epoch: 228, Loss: 0.5573, Train Acc: 0.7279, Loss: 0.7907, Test Acc: 0.5750\n",
      "Epoch: 229, Loss: 0.5544, Train Acc: 0.7467, Loss: 0.7743, Test Acc: 0.5250\n",
      "Epoch: 230, Loss: 0.5733, Train Acc: 0.7279, Loss: 0.8840, Test Acc: 0.6000\n",
      "Epoch: 231, Loss: 0.5572, Train Acc: 0.7279, Loss: 0.7934, Test Acc: 0.5750\n",
      "Epoch: 232, Loss: 0.5542, Train Acc: 0.7467, Loss: 0.8527, Test Acc: 0.5750\n",
      "Epoch: 233, Loss: 0.5546, Train Acc: 0.7592, Loss: 0.7991, Test Acc: 0.5500\n",
      "Epoch: 234, Loss: 0.5375, Train Acc: 0.7342, Loss: 0.8050, Test Acc: 0.5500\n",
      "Epoch: 235, Loss: 0.5392, Train Acc: 0.7522, Loss: 0.8040, Test Acc: 0.5750\n",
      "Epoch: 236, Loss: 0.5492, Train Acc: 0.7654, Loss: 0.8329, Test Acc: 0.5250\n",
      "Epoch: 237, Loss: 0.5473, Train Acc: 0.7279, Loss: 0.8503, Test Acc: 0.5500\n",
      "Epoch: 238, Loss: 0.5377, Train Acc: 0.7842, Loss: 0.8230, Test Acc: 0.5250\n",
      "Epoch: 239, Loss: 0.5374, Train Acc: 0.7529, Loss: 0.7886, Test Acc: 0.5750\n",
      "Epoch: 240, Loss: 0.5464, Train Acc: 0.7342, Loss: 0.9204, Test Acc: 0.6000\n",
      "Epoch: 241, Loss: 0.5311, Train Acc: 0.7717, Loss: 0.7794, Test Acc: 0.6000\n",
      "Epoch: 242, Loss: 0.5181, Train Acc: 0.7897, Loss: 0.7807, Test Acc: 0.5750\n",
      "Epoch: 243, Loss: 0.5558, Train Acc: 0.7717, Loss: 0.9722, Test Acc: 0.6000\n",
      "Epoch: 244, Loss: 0.5722, Train Acc: 0.7342, Loss: 0.9299, Test Acc: 0.5500\n",
      "Epoch: 245, Loss: 0.5231, Train Acc: 0.7412, Loss: 0.7201, Test Acc: 0.5750\n",
      "Epoch: 246, Loss: 0.5133, Train Acc: 0.7897, Loss: 0.7617, Test Acc: 0.5750\n",
      "Epoch: 247, Loss: 0.5209, Train Acc: 0.7897, Loss: 0.8769, Test Acc: 0.6250\n",
      "Epoch: 248, Loss: 0.5068, Train Acc: 0.7897, Loss: 0.7849, Test Acc: 0.5500\n",
      "Epoch: 249, Loss: 0.4975, Train Acc: 0.7710, Loss: 0.8386, Test Acc: 0.5500\n",
      "Epoch: 250, Loss: 0.5054, Train Acc: 0.7835, Loss: 0.9331, Test Acc: 0.5750\n",
      "Epoch: 251, Loss: 0.4882, Train Acc: 0.8085, Loss: 0.8043, Test Acc: 0.5250\n",
      "Epoch: 252, Loss: 0.4914, Train Acc: 0.7897, Loss: 0.8247, Test Acc: 0.5500\n",
      "Epoch: 253, Loss: 0.5010, Train Acc: 0.8077, Loss: 0.8369, Test Acc: 0.5000\n",
      "Epoch: 254, Loss: 0.5250, Train Acc: 0.7397, Loss: 0.9164, Test Acc: 0.5250\n",
      "Epoch: 255, Loss: 0.5360, Train Acc: 0.7724, Loss: 1.0697, Test Acc: 0.6250\n",
      "Epoch: 256, Loss: 0.5167, Train Acc: 0.7960, Loss: 0.8457, Test Acc: 0.5250\n",
      "Epoch: 257, Loss: 0.4802, Train Acc: 0.7835, Loss: 0.8456, Test Acc: 0.5750\n",
      "Epoch: 258, Loss: 0.4923, Train Acc: 0.7960, Loss: 0.8620, Test Acc: 0.5250\n",
      "Epoch: 259, Loss: 0.5110, Train Acc: 0.7904, Loss: 0.9600, Test Acc: 0.5750\n",
      "Epoch: 260, Loss: 0.4962, Train Acc: 0.7960, Loss: 1.0598, Test Acc: 0.6000\n",
      "Epoch: 261, Loss: 0.4753, Train Acc: 0.7960, Loss: 0.8727, Test Acc: 0.5750\n",
      "Epoch: 262, Loss: 0.4563, Train Acc: 0.8210, Loss: 0.8134, Test Acc: 0.6000\n",
      "Epoch: 263, Loss: 0.4545, Train Acc: 0.8147, Loss: 0.8707, Test Acc: 0.5500\n",
      "Epoch: 264, Loss: 0.4584, Train Acc: 0.8085, Loss: 0.9293, Test Acc: 0.5500\n",
      "Epoch: 265, Loss: 0.4550, Train Acc: 0.8022, Loss: 1.0111, Test Acc: 0.5750\n",
      "Epoch: 266, Loss: 0.4527, Train Acc: 0.8022, Loss: 0.9783, Test Acc: 0.5250\n",
      "Epoch: 267, Loss: 0.4368, Train Acc: 0.8272, Loss: 0.9317, Test Acc: 0.5250\n",
      "Epoch: 268, Loss: 0.4299, Train Acc: 0.8147, Loss: 0.9078, Test Acc: 0.5250\n",
      "Epoch: 269, Loss: 0.4435, Train Acc: 0.8335, Loss: 1.0325, Test Acc: 0.5500\n",
      "Epoch: 270, Loss: 0.4379, Train Acc: 0.8085, Loss: 1.0540, Test Acc: 0.5750\n",
      "Epoch: 271, Loss: 0.4393, Train Acc: 0.8335, Loss: 1.0650, Test Acc: 0.6000\n",
      "Epoch: 272, Loss: 0.4234, Train Acc: 0.8327, Loss: 0.9019, Test Acc: 0.5000\n",
      "Epoch: 273, Loss: 0.4472, Train Acc: 0.8272, Loss: 1.0103, Test Acc: 0.5500\n",
      "Epoch: 274, Loss: 0.4729, Train Acc: 0.8452, Loss: 1.1311, Test Acc: 0.5250\n",
      "Epoch: 275, Loss: 0.4715, Train Acc: 0.7835, Loss: 0.9916, Test Acc: 0.5250\n",
      "Epoch: 276, Loss: 0.4523, Train Acc: 0.8522, Loss: 1.0742, Test Acc: 0.5250\n",
      "Epoch: 277, Loss: 0.4440, Train Acc: 0.8265, Loss: 0.9908, Test Acc: 0.5000\n",
      "Epoch: 278, Loss: 0.4325, Train Acc: 0.8210, Loss: 1.1436, Test Acc: 0.5500\n",
      "Epoch: 279, Loss: 0.4252, Train Acc: 0.8327, Loss: 1.0927, Test Acc: 0.5250\n",
      "Epoch: 280, Loss: 0.4255, Train Acc: 0.8445, Loss: 1.1545, Test Acc: 0.5250\n",
      "Epoch: 281, Loss: 0.4331, Train Acc: 0.8147, Loss: 1.3016, Test Acc: 0.5750\n",
      "Epoch: 282, Loss: 0.4159, Train Acc: 0.8265, Loss: 1.1543, Test Acc: 0.5250\n",
      "Epoch: 283, Loss: 0.4140, Train Acc: 0.8577, Loss: 1.0977, Test Acc: 0.5000\n",
      "Epoch: 284, Loss: 0.3988, Train Acc: 0.8577, Loss: 1.1186, Test Acc: 0.5500\n",
      "Epoch: 285, Loss: 0.3872, Train Acc: 0.8210, Loss: 1.1567, Test Acc: 0.5500\n",
      "Epoch: 286, Loss: 0.3990, Train Acc: 0.8460, Loss: 1.2565, Test Acc: 0.5250\n",
      "Epoch: 287, Loss: 0.3854, Train Acc: 0.8695, Loss: 1.1624, Test Acc: 0.4750\n",
      "Epoch: 288, Loss: 0.3773, Train Acc: 0.8272, Loss: 1.1000, Test Acc: 0.5250\n",
      "Epoch: 289, Loss: 0.3936, Train Acc: 0.8577, Loss: 1.2323, Test Acc: 0.5250\n",
      "Epoch: 290, Loss: 0.3918, Train Acc: 0.8452, Loss: 1.2975, Test Acc: 0.5750\n",
      "Epoch: 291, Loss: 0.3881, Train Acc: 0.8390, Loss: 1.3027, Test Acc: 0.5500\n",
      "Epoch: 292, Loss: 0.3718, Train Acc: 0.8695, Loss: 1.1623, Test Acc: 0.5250\n",
      "Epoch: 293, Loss: 0.3586, Train Acc: 0.8757, Loss: 1.1931, Test Acc: 0.5250\n",
      "Epoch: 294, Loss: 0.3955, Train Acc: 0.8702, Loss: 1.2669, Test Acc: 0.5750\n",
      "Epoch: 295, Loss: 0.4101, Train Acc: 0.8445, Loss: 1.3568, Test Acc: 0.5750\n",
      "Epoch: 296, Loss: 0.4274, Train Acc: 0.8022, Loss: 1.6603, Test Acc: 0.5750\n",
      "Epoch: 297, Loss: 0.4293, Train Acc: 0.8265, Loss: 1.3360, Test Acc: 0.5750\n",
      "Epoch: 298, Loss: 0.3683, Train Acc: 0.8335, Loss: 1.0447, Test Acc: 0.5250\n",
      "Epoch: 299, Loss: 0.3604, Train Acc: 0.8625, Loss: 0.9906, Test Acc: 0.5750\n",
      "Epoch: 300, Loss: 0.4202, Train Acc: 0.8507, Loss: 1.1296, Test Acc: 0.4750\n",
      "Epoch: 301, Loss: 0.4227, Train Acc: 0.8140, Loss: 1.2920, Test Acc: 0.6000\n",
      "Epoch: 302, Loss: 0.4467, Train Acc: 0.8154, Loss: 1.5644, Test Acc: 0.5500\n",
      "Epoch: 303, Loss: 0.4500, Train Acc: 0.6982, Loss: 1.4593, Test Acc: 0.5500\n",
      "Epoch: 304, Loss: 0.4277, Train Acc: 0.6967, Loss: 0.9379, Test Acc: 0.4250\n",
      "Epoch: 305, Loss: 0.4872, Train Acc: 0.8063, Loss: 1.1730, Test Acc: 0.5750\n",
      "Epoch: 306, Loss: 0.5287, Train Acc: 0.8257, Loss: 1.3199, Test Acc: 0.5250\n",
      "Epoch: 307, Loss: 0.4821, Train Acc: 0.7647, Loss: 1.6361, Test Acc: 0.5500\n",
      "Epoch: 308, Loss: 0.4519, Train Acc: 0.8265, Loss: 1.3064, Test Acc: 0.6500\n",
      "Epoch: 309, Loss: 0.3835, Train Acc: 0.8015, Loss: 1.0188, Test Acc: 0.4750\n",
      "Epoch: 310, Loss: 0.4122, Train Acc: 0.8077, Loss: 1.4558, Test Acc: 0.5500\n",
      "Epoch: 311, Loss: 0.4404, Train Acc: 0.8632, Loss: 1.3788, Test Acc: 0.5000\n",
      "Epoch: 312, Loss: 0.3920, Train Acc: 0.8390, Loss: 1.5652, Test Acc: 0.5500\n",
      "Epoch: 313, Loss: 0.3596, Train Acc: 0.8522, Loss: 1.5792, Test Acc: 0.5750\n",
      "Epoch: 314, Loss: 0.3313, Train Acc: 0.8882, Loss: 1.4914, Test Acc: 0.5750\n",
      "Epoch: 315, Loss: 0.3357, Train Acc: 0.8632, Loss: 1.6213, Test Acc: 0.5500\n",
      "Epoch: 316, Loss: 0.3385, Train Acc: 0.8382, Loss: 1.7313, Test Acc: 0.5000\n",
      "Epoch: 317, Loss: 0.3269, Train Acc: 0.8757, Loss: 1.5883, Test Acc: 0.5500\n",
      "Epoch: 318, Loss: 0.3082, Train Acc: 0.9125, Loss: 1.4261, Test Acc: 0.5750\n",
      "Epoch: 319, Loss: 0.3064, Train Acc: 0.8757, Loss: 1.6581, Test Acc: 0.5500\n",
      "Epoch: 320, Loss: 0.3254, Train Acc: 0.8507, Loss: 1.8861, Test Acc: 0.5250\n",
      "Epoch: 321, Loss: 0.3221, Train Acc: 0.8382, Loss: 1.8765, Test Acc: 0.5250\n",
      "Epoch: 322, Loss: 0.3068, Train Acc: 0.8938, Loss: 1.6172, Test Acc: 0.5750\n",
      "Epoch: 323, Loss: 0.2901, Train Acc: 0.9070, Loss: 1.5855, Test Acc: 0.5750\n",
      "Epoch: 324, Loss: 0.3060, Train Acc: 0.8757, Loss: 1.8627, Test Acc: 0.5250\n",
      "Epoch: 325, Loss: 0.3157, Train Acc: 0.8507, Loss: 2.0170, Test Acc: 0.5000\n",
      "Epoch: 326, Loss: 0.3154, Train Acc: 0.8570, Loss: 1.9041, Test Acc: 0.5250\n",
      "Epoch: 327, Loss: 0.2903, Train Acc: 0.9187, Loss: 1.5072, Test Acc: 0.6000\n",
      "Epoch: 328, Loss: 0.2743, Train Acc: 0.9250, Loss: 1.5164, Test Acc: 0.5750\n",
      "Epoch: 329, Loss: 0.3039, Train Acc: 0.8507, Loss: 2.0305, Test Acc: 0.5000\n",
      "Epoch: 330, Loss: 0.3226, Train Acc: 0.8382, Loss: 2.1900, Test Acc: 0.5250\n",
      "Epoch: 331, Loss: 0.3238, Train Acc: 0.8507, Loss: 1.9787, Test Acc: 0.5250\n",
      "Epoch: 332, Loss: 0.2787, Train Acc: 0.9062, Loss: 1.4055, Test Acc: 0.5500\n",
      "Epoch: 333, Loss: 0.2750, Train Acc: 0.9125, Loss: 1.3632, Test Acc: 0.5750\n",
      "Epoch: 334, Loss: 0.2987, Train Acc: 0.8632, Loss: 1.9883, Test Acc: 0.5250\n",
      "Epoch: 335, Loss: 0.3171, Train Acc: 0.8390, Loss: 2.2805, Test Acc: 0.5750\n",
      "Epoch: 336, Loss: 0.3181, Train Acc: 0.8632, Loss: 2.0089, Test Acc: 0.5500\n",
      "Epoch: 337, Loss: 0.2722, Train Acc: 0.9125, Loss: 1.4936, Test Acc: 0.5750\n",
      "Epoch: 338, Loss: 0.2613, Train Acc: 0.9375, Loss: 1.6652, Test Acc: 0.6000\n",
      "Epoch: 339, Loss: 0.2821, Train Acc: 0.8882, Loss: 2.1958, Test Acc: 0.5250\n",
      "Epoch: 340, Loss: 0.3039, Train Acc: 0.8445, Loss: 2.3451, Test Acc: 0.5500\n",
      "Epoch: 341, Loss: 0.2925, Train Acc: 0.8938, Loss: 1.8304, Test Acc: 0.5750\n",
      "Epoch: 342, Loss: 0.2520, Train Acc: 0.9187, Loss: 1.6023, Test Acc: 0.5750\n",
      "Epoch: 343, Loss: 0.2677, Train Acc: 0.8757, Loss: 2.1553, Test Acc: 0.5250\n",
      "Epoch: 344, Loss: 0.2699, Train Acc: 0.8750, Loss: 2.2605, Test Acc: 0.5750\n",
      "Epoch: 345, Loss: 0.2768, Train Acc: 0.8632, Loss: 2.3374, Test Acc: 0.5250\n",
      "Epoch: 346, Loss: 0.2619, Train Acc: 0.9062, Loss: 2.0920, Test Acc: 0.5500\n",
      "Epoch: 347, Loss: 0.2511, Train Acc: 0.9375, Loss: 1.9142, Test Acc: 0.5500\n",
      "Epoch: 348, Loss: 0.2418, Train Acc: 0.9313, Loss: 1.9274, Test Acc: 0.5750\n",
      "Epoch: 349, Loss: 0.2733, Train Acc: 0.8757, Loss: 2.4231, Test Acc: 0.5250\n",
      "Epoch: 350, Loss: 0.2780, Train Acc: 0.8875, Loss: 2.4536, Test Acc: 0.5500\n",
      "Epoch: 351, Loss: 0.2847, Train Acc: 0.8820, Loss: 2.1201, Test Acc: 0.5000\n",
      "Epoch: 352, Loss: 0.2480, Train Acc: 0.9250, Loss: 1.9028, Test Acc: 0.5750\n",
      "Epoch: 353, Loss: 0.2530, Train Acc: 0.9313, Loss: 1.9842, Test Acc: 0.5750\n",
      "Epoch: 354, Loss: 0.2548, Train Acc: 0.9062, Loss: 2.3006, Test Acc: 0.5500\n",
      "Epoch: 355, Loss: 0.2842, Train Acc: 0.8570, Loss: 2.5764, Test Acc: 0.5000\n",
      "Epoch: 356, Loss: 0.2735, Train Acc: 0.8750, Loss: 2.4793, Test Acc: 0.5500\n",
      "Epoch: 357, Loss: 0.2425, Train Acc: 0.9125, Loss: 2.1486, Test Acc: 0.5750\n",
      "Epoch: 358, Loss: 0.2317, Train Acc: 0.9313, Loss: 2.1257, Test Acc: 0.5500\n",
      "Epoch: 359, Loss: 0.2324, Train Acc: 0.9250, Loss: 1.9842, Test Acc: 0.5500\n",
      "Epoch: 360, Loss: 0.2455, Train Acc: 0.9062, Loss: 2.4839, Test Acc: 0.5500\n",
      "Epoch: 361, Loss: 0.2731, Train Acc: 0.8632, Loss: 2.7027, Test Acc: 0.4750\n",
      "Epoch: 362, Loss: 0.2626, Train Acc: 0.8813, Loss: 2.5906, Test Acc: 0.5250\n",
      "Epoch: 363, Loss: 0.2295, Train Acc: 0.9375, Loss: 2.2057, Test Acc: 0.5250\n",
      "Epoch: 364, Loss: 0.2110, Train Acc: 0.9313, Loss: 2.0557, Test Acc: 0.5750\n",
      "Epoch: 365, Loss: 0.2131, Train Acc: 0.9313, Loss: 2.0011, Test Acc: 0.6000\n",
      "Epoch: 366, Loss: 0.2171, Train Acc: 0.9438, Loss: 2.0960, Test Acc: 0.5250\n",
      "Epoch: 367, Loss: 0.2366, Train Acc: 0.9125, Loss: 2.6393, Test Acc: 0.5250\n",
      "Epoch: 368, Loss: 0.2582, Train Acc: 0.8507, Loss: 3.0985, Test Acc: 0.5750\n",
      "Epoch: 369, Loss: 0.3028, Train Acc: 0.8820, Loss: 2.7405, Test Acc: 0.5000\n",
      "Epoch: 370, Loss: 0.2338, Train Acc: 0.9125, Loss: 1.9653, Test Acc: 0.5500\n",
      "Epoch: 371, Loss: 0.2966, Train Acc: 0.8070, Loss: 0.8632, Test Acc: 0.5500\n",
      "Epoch: 372, Loss: 0.4000, Train Acc: 0.8882, Loss: 1.6581, Test Acc: 0.5500\n",
      "Epoch: 373, Loss: 0.4766, Train Acc: 0.8820, Loss: 1.5744, Test Acc: 0.5750\n",
      "Epoch: 374, Loss: 0.6333, Train Acc: 0.7765, Loss: 2.9028, Test Acc: 0.5500\n",
      "Epoch: 375, Loss: 0.5841, Train Acc: 0.7599, Loss: 1.0961, Test Acc: 0.5000\n",
      "Epoch: 376, Loss: 0.4012, Train Acc: 0.7967, Loss: 1.7638, Test Acc: 0.5250\n",
      "Epoch: 377, Loss: 0.3567, Train Acc: 0.8382, Loss: 1.4478, Test Acc: 0.5500\n",
      "Epoch: 378, Loss: 0.3424, Train Acc: 0.8875, Loss: 1.7007, Test Acc: 0.5250\n",
      "Epoch: 379, Loss: 0.3335, Train Acc: 0.8882, Loss: 1.9640, Test Acc: 0.5500\n",
      "Epoch: 380, Loss: 0.2773, Train Acc: 0.9125, Loss: 1.7341, Test Acc: 0.5750\n",
      "Epoch: 381, Loss: 0.2672, Train Acc: 0.8812, Loss: 2.3389, Test Acc: 0.5500\n",
      "Epoch: 382, Loss: 0.2606, Train Acc: 0.9187, Loss: 2.0225, Test Acc: 0.5500\n",
      "Epoch: 383, Loss: 0.2299, Train Acc: 0.9187, Loss: 1.8889, Test Acc: 0.5750\n",
      "Epoch: 384, Loss: 0.2414, Train Acc: 0.8938, Loss: 2.6159, Test Acc: 0.5250\n",
      "Epoch: 385, Loss: 0.2394, Train Acc: 0.9250, Loss: 2.2810, Test Acc: 0.5250\n",
      "Epoch: 386, Loss: 0.2103, Train Acc: 0.9313, Loss: 2.0898, Test Acc: 0.5750\n",
      "Epoch: 387, Loss: 0.1993, Train Acc: 0.9375, Loss: 2.2444, Test Acc: 0.5500\n",
      "Epoch: 388, Loss: 0.2151, Train Acc: 0.9250, Loss: 2.6355, Test Acc: 0.5000\n",
      "Epoch: 389, Loss: 0.2096, Train Acc: 0.9250, Loss: 2.2928, Test Acc: 0.5750\n",
      "Epoch: 390, Loss: 0.1936, Train Acc: 0.9375, Loss: 2.2508, Test Acc: 0.5500\n",
      "Epoch: 391, Loss: 0.1864, Train Acc: 0.9438, Loss: 2.3249, Test Acc: 0.5500\n",
      "Epoch: 392, Loss: 0.1924, Train Acc: 0.9438, Loss: 2.5944, Test Acc: 0.5500\n",
      "Epoch: 393, Loss: 0.1918, Train Acc: 0.9312, Loss: 2.6196, Test Acc: 0.5500\n",
      "Epoch: 394, Loss: 0.1893, Train Acc: 0.9500, Loss: 2.6066, Test Acc: 0.5500\n",
      "Epoch: 395, Loss: 0.1739, Train Acc: 0.9500, Loss: 2.4020, Test Acc: 0.5500\n",
      "Epoch: 396, Loss: 0.1757, Train Acc: 0.9563, Loss: 2.6161, Test Acc: 0.5750\n",
      "Epoch: 397, Loss: 0.1775, Train Acc: 0.9375, Loss: 2.5980, Test Acc: 0.5750\n",
      "Epoch: 398, Loss: 0.1773, Train Acc: 0.9375, Loss: 2.8186, Test Acc: 0.5500\n",
      "Epoch: 399, Loss: 0.1725, Train Acc: 0.9438, Loss: 2.7783, Test Acc: 0.5250\n",
      "Epoch: 400, Loss: 0.1657, Train Acc: 0.9375, Loss: 2.6300, Test Acc: 0.5750\n",
      "Epoch: 401, Loss: 0.1612, Train Acc: 0.9500, Loss: 2.4908, Test Acc: 0.5750\n",
      "Epoch: 402, Loss: 0.1679, Train Acc: 0.9500, Loss: 2.8064, Test Acc: 0.5250\n",
      "Epoch: 403, Loss: 0.1711, Train Acc: 0.9438, Loss: 2.9445, Test Acc: 0.5250\n",
      "Epoch: 404, Loss: 0.1781, Train Acc: 0.9438, Loss: 3.1736, Test Acc: 0.5250\n",
      "Epoch: 405, Loss: 0.1668, Train Acc: 0.9563, Loss: 2.9332, Test Acc: 0.5250\n",
      "Epoch: 406, Loss: 0.1508, Train Acc: 0.9438, Loss: 2.6325, Test Acc: 0.6000\n",
      "Epoch: 407, Loss: 0.1596, Train Acc: 0.9500, Loss: 2.3071, Test Acc: 0.6000\n",
      "Epoch: 408, Loss: 0.1711, Train Acc: 0.9563, Loss: 2.6449, Test Acc: 0.5750\n",
      "Epoch: 409, Loss: 0.1966, Train Acc: 0.9438, Loss: 2.9762, Test Acc: 0.5500\n",
      "Epoch: 410, Loss: 0.2106, Train Acc: 0.8757, Loss: 3.6822, Test Acc: 0.5000\n",
      "Epoch: 411, Loss: 0.2090, Train Acc: 0.9438, Loss: 3.1084, Test Acc: 0.5250\n",
      "Epoch: 412, Loss: 0.1617, Train Acc: 0.9500, Loss: 2.7477, Test Acc: 0.5250\n",
      "Epoch: 413, Loss: 0.1771, Train Acc: 0.8875, Loss: 1.7322, Test Acc: 0.5500\n",
      "Epoch: 414, Loss: 0.2271, Train Acc: 0.9438, Loss: 2.6671, Test Acc: 0.5250\n",
      "Epoch: 415, Loss: 0.1889, Train Acc: 0.9312, Loss: 2.8042, Test Acc: 0.5750\n",
      "Epoch: 416, Loss: 0.1757, Train Acc: 0.9500, Loss: 2.5761, Test Acc: 0.5500\n",
      "Epoch: 417, Loss: 0.1557, Train Acc: 0.9563, Loss: 2.7951, Test Acc: 0.5500\n",
      "Epoch: 418, Loss: 0.1707, Train Acc: 0.9500, Loss: 3.2836, Test Acc: 0.5250\n",
      "Epoch: 419, Loss: 0.1620, Train Acc: 0.9688, Loss: 3.2998, Test Acc: 0.5000\n",
      "Epoch: 420, Loss: 0.1370, Train Acc: 0.9563, Loss: 2.8668, Test Acc: 0.5500\n",
      "Epoch: 421, Loss: 0.1364, Train Acc: 0.9563, Loss: 2.9733, Test Acc: 0.5750\n",
      "Epoch: 422, Loss: 0.1372, Train Acc: 0.9688, Loss: 3.1145, Test Acc: 0.5750\n",
      "Epoch: 423, Loss: 0.1345, Train Acc: 0.9625, Loss: 3.3685, Test Acc: 0.5500\n",
      "Epoch: 424, Loss: 0.1314, Train Acc: 0.9688, Loss: 3.1896, Test Acc: 0.5500\n",
      "Epoch: 425, Loss: 0.1225, Train Acc: 0.9750, Loss: 2.9723, Test Acc: 0.5750\n",
      "Epoch: 426, Loss: 0.1191, Train Acc: 0.9625, Loss: 2.7319, Test Acc: 0.6000\n",
      "Epoch: 427, Loss: 0.1277, Train Acc: 0.9750, Loss: 3.1817, Test Acc: 0.5750\n",
      "Epoch: 428, Loss: 0.1235, Train Acc: 0.9688, Loss: 3.3254, Test Acc: 0.5750\n",
      "Epoch: 429, Loss: 0.1318, Train Acc: 0.9563, Loss: 3.5957, Test Acc: 0.5250\n",
      "Epoch: 430, Loss: 0.1192, Train Acc: 0.9812, Loss: 3.2330, Test Acc: 0.5500\n",
      "Epoch: 431, Loss: 0.1130, Train Acc: 0.9500, Loss: 2.6133, Test Acc: 0.6000\n",
      "Epoch: 432, Loss: 0.1840, Train Acc: 0.7688, Loss: 1.3027, Test Acc: 0.5000\n",
      "Epoch: 433, Loss: 0.3257, Train Acc: 0.7945, Loss: 1.1286, Test Acc: 0.5000\n",
      "Epoch: 434, Loss: 0.3847, Train Acc: 0.8500, Loss: 1.4633, Test Acc: 0.5750\n",
      "Epoch: 435, Loss: 0.5075, Train Acc: 0.9007, Loss: 2.4031, Test Acc: 0.5250\n",
      "Epoch: 436, Loss: 0.4211, Train Acc: 0.7897, Loss: 3.3687, Test Acc: 0.5000\n",
      "Epoch: 437, Loss: 0.4070, Train Acc: 0.8515, Loss: 3.2911, Test Acc: 0.4500\n",
      "Epoch: 438, Loss: 0.3105, Train Acc: 0.8938, Loss: 1.7627, Test Acc: 0.5000\n",
      "Epoch: 439, Loss: 0.2478, Train Acc: 0.9250, Loss: 2.3936, Test Acc: 0.5250\n",
      "Epoch: 440, Loss: 0.2638, Train Acc: 0.8632, Loss: 3.6935, Test Acc: 0.4250\n",
      "Epoch: 441, Loss: 0.2587, Train Acc: 0.9000, Loss: 1.4910, Test Acc: 0.4750\n",
      "Epoch: 442, Loss: 0.2155, Train Acc: 0.9312, Loss: 2.9429, Test Acc: 0.5000\n",
      "Epoch: 443, Loss: 0.1958, Train Acc: 0.9500, Loss: 3.0540, Test Acc: 0.4750\n",
      "Epoch: 444, Loss: 0.1585, Train Acc: 0.9500, Loss: 2.4594, Test Acc: 0.5250\n",
      "Epoch: 445, Loss: 0.1579, Train Acc: 0.9625, Loss: 3.1373, Test Acc: 0.4750\n",
      "Epoch: 446, Loss: 0.1580, Train Acc: 0.9500, Loss: 3.3858, Test Acc: 0.5500\n",
      "Epoch: 447, Loss: 0.1425, Train Acc: 0.9625, Loss: 2.8987, Test Acc: 0.5500\n",
      "Epoch: 448, Loss: 0.1257, Train Acc: 0.9688, Loss: 3.2323, Test Acc: 0.5500\n",
      "Epoch: 449, Loss: 0.1235, Train Acc: 0.9750, Loss: 3.3984, Test Acc: 0.5750\n",
      "Epoch: 450, Loss: 0.1182, Train Acc: 0.9750, Loss: 3.1463, Test Acc: 0.5750\n",
      "Epoch: 451, Loss: 0.1067, Train Acc: 0.9750, Loss: 3.1602, Test Acc: 0.5750\n",
      "Epoch: 452, Loss: 0.1097, Train Acc: 0.9750, Loss: 3.2889, Test Acc: 0.5500\n",
      "Epoch: 453, Loss: 0.1046, Train Acc: 0.9750, Loss: 3.1490, Test Acc: 0.5750\n",
      "Epoch: 454, Loss: 0.1015, Train Acc: 0.9750, Loss: 3.2328, Test Acc: 0.5750\n",
      "Epoch: 455, Loss: 0.0978, Train Acc: 0.9812, Loss: 3.2559, Test Acc: 0.5750\n",
      "Epoch: 456, Loss: 0.0973, Train Acc: 0.9750, Loss: 3.3160, Test Acc: 0.5750\n",
      "Epoch: 457, Loss: 0.0923, Train Acc: 0.9812, Loss: 3.2855, Test Acc: 0.5750\n",
      "Epoch: 458, Loss: 0.0915, Train Acc: 0.9750, Loss: 3.3447, Test Acc: 0.5750\n",
      "Epoch: 459, Loss: 0.0923, Train Acc: 0.9812, Loss: 3.4112, Test Acc: 0.5750\n",
      "Epoch: 460, Loss: 0.0897, Train Acc: 0.9812, Loss: 3.2718, Test Acc: 0.5750\n",
      "Epoch: 461, Loss: 0.0898, Train Acc: 0.9812, Loss: 3.4034, Test Acc: 0.5750\n",
      "Epoch: 462, Loss: 0.0857, Train Acc: 0.9812, Loss: 3.3588, Test Acc: 0.5750\n",
      "Epoch: 463, Loss: 0.0841, Train Acc: 0.9812, Loss: 3.4404, Test Acc: 0.5500\n",
      "Epoch: 464, Loss: 0.0822, Train Acc: 0.9812, Loss: 3.5069, Test Acc: 0.5750\n",
      "Epoch: 465, Loss: 0.0828, Train Acc: 0.9875, Loss: 3.5800, Test Acc: 0.5500\n",
      "Epoch: 466, Loss: 0.0791, Train Acc: 0.9812, Loss: 3.3899, Test Acc: 0.5750\n",
      "Epoch: 467, Loss: 0.0798, Train Acc: 0.9875, Loss: 3.5853, Test Acc: 0.5750\n",
      "Epoch: 468, Loss: 0.0793, Train Acc: 0.9812, Loss: 3.5274, Test Acc: 0.5750\n",
      "Epoch: 469, Loss: 0.0808, Train Acc: 0.9875, Loss: 3.6772, Test Acc: 0.5500\n",
      "Epoch: 470, Loss: 0.0762, Train Acc: 0.9812, Loss: 3.4813, Test Acc: 0.5750\n",
      "Epoch: 471, Loss: 0.0769, Train Acc: 0.9812, Loss: 3.7291, Test Acc: 0.5500\n",
      "Epoch: 472, Loss: 0.0723, Train Acc: 0.9875, Loss: 3.5868, Test Acc: 0.5750\n",
      "Epoch: 473, Loss: 0.0731, Train Acc: 0.9875, Loss: 3.6973, Test Acc: 0.5500\n",
      "Epoch: 474, Loss: 0.0688, Train Acc: 0.9875, Loss: 3.5730, Test Acc: 0.6000\n",
      "Epoch: 475, Loss: 0.0716, Train Acc: 0.9875, Loss: 3.8206, Test Acc: 0.5750\n",
      "Epoch: 476, Loss: 0.0682, Train Acc: 0.9875, Loss: 3.7086, Test Acc: 0.6000\n",
      "Epoch: 477, Loss: 0.0673, Train Acc: 0.9875, Loss: 3.7533, Test Acc: 0.5750\n",
      "Epoch: 478, Loss: 0.0632, Train Acc: 0.9937, Loss: 3.6968, Test Acc: 0.6000\n",
      "Epoch: 479, Loss: 0.0639, Train Acc: 0.9937, Loss: 3.9228, Test Acc: 0.5750\n",
      "Epoch: 480, Loss: 0.0620, Train Acc: 0.9875, Loss: 3.8192, Test Acc: 0.6250\n",
      "Epoch: 481, Loss: 0.0608, Train Acc: 0.9875, Loss: 3.8543, Test Acc: 0.5500\n",
      "Epoch: 482, Loss: 0.0577, Train Acc: 0.9937, Loss: 3.8005, Test Acc: 0.6000\n",
      "Epoch: 483, Loss: 0.0602, Train Acc: 1.0000, Loss: 4.0694, Test Acc: 0.5750\n",
      "Epoch: 484, Loss: 0.0601, Train Acc: 0.9875, Loss: 3.9090, Test Acc: 0.6000\n",
      "Epoch: 485, Loss: 0.0607, Train Acc: 0.9875, Loss: 3.9796, Test Acc: 0.5250\n",
      "Epoch: 486, Loss: 0.0538, Train Acc: 0.9937, Loss: 3.8112, Test Acc: 0.5750\n",
      "Epoch: 487, Loss: 0.0562, Train Acc: 0.9937, Loss: 4.2377, Test Acc: 0.5500\n",
      "Epoch: 488, Loss: 0.0577, Train Acc: 0.9937, Loss: 4.0828, Test Acc: 0.6000\n",
      "Epoch: 489, Loss: 0.0581, Train Acc: 1.0000, Loss: 4.2938, Test Acc: 0.5500\n",
      "Epoch: 490, Loss: 0.0516, Train Acc: 0.9937, Loss: 3.7182, Test Acc: 0.5750\n",
      "Epoch: 491, Loss: 0.0556, Train Acc: 1.0000, Loss: 4.2482, Test Acc: 0.5250\n",
      "Epoch: 492, Loss: 0.0539, Train Acc: 0.9937, Loss: 4.2093, Test Acc: 0.6250\n",
      "Epoch: 493, Loss: 0.0577, Train Acc: 1.0000, Loss: 4.4344, Test Acc: 0.5750\n",
      "Epoch: 494, Loss: 0.0497, Train Acc: 0.9937, Loss: 3.8067, Test Acc: 0.6000\n",
      "Epoch: 495, Loss: 0.0500, Train Acc: 0.9937, Loss: 4.1598, Test Acc: 0.5500\n",
      "Epoch: 496, Loss: 0.0496, Train Acc: 0.9937, Loss: 4.0236, Test Acc: 0.6250\n",
      "Epoch: 497, Loss: 0.0525, Train Acc: 0.9875, Loss: 4.4851, Test Acc: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 498, Loss: 0.0519, Train Acc: 0.9937, Loss: 4.2743, Test Acc: 0.6250\n",
      "Epoch: 499, Loss: 0.0549, Train Acc: 1.0000, Loss: 4.1483, Test Acc: 0.5500\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 108\n",
      " 109 140 142 144 145 146 147 148 149 150 151 154 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180] TEST: [107 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 141 143 152 153 155]\n",
      "145\n",
      "36\n",
      "Epoch: 001, Loss: 0.7897, Train Acc: 0.4967, Loss: 0.2203, Test Acc: 0.5250\n",
      "Epoch: 002, Loss: 0.8197, Train Acc: 0.5033, Loss: 1.5062, Test Acc: 0.4750\n",
      "Epoch: 003, Loss: 0.7378, Train Acc: 0.4967, Loss: 0.4086, Test Acc: 0.5250\n",
      "Epoch: 004, Loss: 0.7829, Train Acc: 0.5151, Loss: 0.8006, Test Acc: 0.4750\n",
      "Epoch: 005, Loss: 0.7016, Train Acc: 0.5033, Loss: 0.9657, Test Acc: 0.4750\n",
      "Epoch: 006, Loss: 0.7003, Train Acc: 0.4967, Loss: 0.6129, Test Acc: 0.5250\n",
      "Epoch: 007, Loss: 0.7198, Train Acc: 0.4801, Loss: 0.7174, Test Acc: 0.4000\n",
      "Epoch: 008, Loss: 0.6982, Train Acc: 0.5151, Loss: 0.8606, Test Acc: 0.4750\n",
      "Epoch: 009, Loss: 0.6961, Train Acc: 0.4801, Loss: 0.7164, Test Acc: 0.3750\n",
      "Epoch: 010, Loss: 0.7048, Train Acc: 0.4912, Loss: 0.7054, Test Acc: 0.4250\n",
      "Epoch: 011, Loss: 0.6999, Train Acc: 0.5151, Loss: 0.7935, Test Acc: 0.4750\n",
      "Epoch: 012, Loss: 0.6962, Train Acc: 0.5081, Loss: 0.7550, Test Acc: 0.5750\n",
      "Epoch: 013, Loss: 0.6998, Train Acc: 0.4934, Loss: 0.7236, Test Acc: 0.3500\n",
      "Epoch: 014, Loss: 0.6997, Train Acc: 0.5081, Loss: 0.7645, Test Acc: 0.5750\n",
      "Epoch: 015, Loss: 0.6970, Train Acc: 0.5081, Loss: 0.7627, Test Acc: 0.5750\n",
      "Epoch: 016, Loss: 0.6981, Train Acc: 0.5136, Loss: 0.7421, Test Acc: 0.5000\n",
      "Epoch: 017, Loss: 0.6990, Train Acc: 0.5011, Loss: 0.7540, Test Acc: 0.5500\n",
      "Epoch: 018, Loss: 0.6977, Train Acc: 0.5018, Loss: 0.7617, Test Acc: 0.5250\n",
      "Epoch: 019, Loss: 0.6975, Train Acc: 0.4893, Loss: 0.7518, Test Acc: 0.5500\n",
      "Epoch: 020, Loss: 0.6981, Train Acc: 0.4956, Loss: 0.7487, Test Acc: 0.5500\n",
      "Epoch: 021, Loss: 0.6976, Train Acc: 0.4956, Loss: 0.7525, Test Acc: 0.5500\n",
      "Epoch: 022, Loss: 0.6974, Train Acc: 0.5018, Loss: 0.7559, Test Acc: 0.5250\n",
      "Epoch: 023, Loss: 0.6972, Train Acc: 0.5136, Loss: 0.7572, Test Acc: 0.5500\n",
      "Epoch: 024, Loss: 0.6973, Train Acc: 0.5011, Loss: 0.7552, Test Acc: 0.5500\n",
      "Epoch: 025, Loss: 0.6973, Train Acc: 0.5136, Loss: 0.7553, Test Acc: 0.5500\n",
      "Epoch: 026, Loss: 0.6969, Train Acc: 0.5018, Loss: 0.7578, Test Acc: 0.5500\n",
      "Epoch: 027, Loss: 0.6968, Train Acc: 0.5191, Loss: 0.7607, Test Acc: 0.5500\n",
      "Epoch: 028, Loss: 0.6968, Train Acc: 0.5191, Loss: 0.7580, Test Acc: 0.5250\n",
      "Epoch: 029, Loss: 0.6968, Train Acc: 0.5136, Loss: 0.7566, Test Acc: 0.5500\n",
      "Epoch: 030, Loss: 0.6966, Train Acc: 0.5081, Loss: 0.7569, Test Acc: 0.5500\n",
      "Epoch: 031, Loss: 0.6963, Train Acc: 0.5254, Loss: 0.7599, Test Acc: 0.5500\n",
      "Epoch: 032, Loss: 0.6964, Train Acc: 0.5191, Loss: 0.7570, Test Acc: 0.5500\n",
      "Epoch: 033, Loss: 0.6961, Train Acc: 0.4956, Loss: 0.7568, Test Acc: 0.5500\n",
      "Epoch: 034, Loss: 0.6959, Train Acc: 0.5018, Loss: 0.7585, Test Acc: 0.5500\n",
      "Epoch: 035, Loss: 0.6960, Train Acc: 0.5081, Loss: 0.7628, Test Acc: 0.5000\n",
      "Epoch: 036, Loss: 0.6958, Train Acc: 0.5261, Loss: 0.7633, Test Acc: 0.5000\n",
      "Epoch: 037, Loss: 0.6957, Train Acc: 0.5136, Loss: 0.7574, Test Acc: 0.4500\n",
      "Epoch: 038, Loss: 0.6957, Train Acc: 0.5199, Loss: 0.7618, Test Acc: 0.5000\n",
      "Epoch: 039, Loss: 0.6953, Train Acc: 0.5081, Loss: 0.7614, Test Acc: 0.5000\n",
      "Epoch: 040, Loss: 0.6952, Train Acc: 0.5379, Loss: 0.7601, Test Acc: 0.5000\n",
      "Epoch: 041, Loss: 0.6952, Train Acc: 0.5143, Loss: 0.7600, Test Acc: 0.5000\n",
      "Epoch: 042, Loss: 0.6947, Train Acc: 0.5379, Loss: 0.7674, Test Acc: 0.5000\n",
      "Epoch: 043, Loss: 0.6949, Train Acc: 0.5496, Loss: 0.7657, Test Acc: 0.4500\n",
      "Epoch: 044, Loss: 0.6948, Train Acc: 0.5739, Loss: 0.7611, Test Acc: 0.4500\n",
      "Epoch: 045, Loss: 0.6946, Train Acc: 0.5143, Loss: 0.7642, Test Acc: 0.5000\n",
      "Epoch: 046, Loss: 0.6940, Train Acc: 0.5199, Loss: 0.7629, Test Acc: 0.4750\n",
      "Epoch: 047, Loss: 0.6940, Train Acc: 0.5614, Loss: 0.7662, Test Acc: 0.4750\n",
      "Epoch: 048, Loss: 0.6942, Train Acc: 0.5676, Loss: 0.7658, Test Acc: 0.4500\n",
      "Epoch: 049, Loss: 0.6939, Train Acc: 0.5621, Loss: 0.7651, Test Acc: 0.4750\n",
      "Epoch: 050, Loss: 0.6935, Train Acc: 0.5434, Loss: 0.7630, Test Acc: 0.4750\n",
      "Epoch: 051, Loss: 0.6932, Train Acc: 0.5434, Loss: 0.7617, Test Acc: 0.4500\n",
      "Epoch: 052, Loss: 0.6933, Train Acc: 0.5559, Loss: 0.7649, Test Acc: 0.4500\n",
      "Epoch: 053, Loss: 0.6931, Train Acc: 0.5434, Loss: 0.7629, Test Acc: 0.4750\n",
      "Epoch: 054, Loss: 0.6924, Train Acc: 0.5496, Loss: 0.7640, Test Acc: 0.4750\n",
      "Epoch: 055, Loss: 0.6926, Train Acc: 0.5434, Loss: 0.7549, Test Acc: 0.4750\n",
      "Epoch: 056, Loss: 0.6921, Train Acc: 0.5496, Loss: 0.7678, Test Acc: 0.4500\n",
      "Epoch: 057, Loss: 0.6918, Train Acc: 0.5434, Loss: 0.7687, Test Acc: 0.4500\n",
      "Epoch: 058, Loss: 0.6915, Train Acc: 0.5434, Loss: 0.7690, Test Acc: 0.4500\n",
      "Epoch: 059, Loss: 0.6919, Train Acc: 0.5496, Loss: 0.7678, Test Acc: 0.4500\n",
      "Epoch: 060, Loss: 0.6916, Train Acc: 0.5559, Loss: 0.7630, Test Acc: 0.4500\n",
      "Epoch: 061, Loss: 0.6911, Train Acc: 0.5809, Loss: 0.7600, Test Acc: 0.4750\n",
      "Epoch: 062, Loss: 0.6904, Train Acc: 0.5559, Loss: 0.7748, Test Acc: 0.4500\n",
      "Epoch: 063, Loss: 0.6905, Train Acc: 0.5614, Loss: 0.7877, Test Acc: 0.4500\n",
      "Epoch: 064, Loss: 0.6914, Train Acc: 0.5684, Loss: 0.7642, Test Acc: 0.3750\n",
      "Epoch: 065, Loss: 0.6916, Train Acc: 0.5746, Loss: 0.7721, Test Acc: 0.4250\n",
      "Epoch: 066, Loss: 0.6895, Train Acc: 0.5746, Loss: 0.7808, Test Acc: 0.4500\n",
      "Epoch: 067, Loss: 0.6887, Train Acc: 0.5684, Loss: 0.7714, Test Acc: 0.4250\n",
      "Epoch: 068, Loss: 0.6899, Train Acc: 0.5496, Loss: 0.7622, Test Acc: 0.4250\n",
      "Epoch: 069, Loss: 0.6895, Train Acc: 0.5934, Loss: 0.7728, Test Acc: 0.4500\n",
      "Epoch: 070, Loss: 0.6882, Train Acc: 0.5934, Loss: 0.7683, Test Acc: 0.4500\n",
      "Epoch: 071, Loss: 0.6879, Train Acc: 0.5559, Loss: 0.7783, Test Acc: 0.4500\n",
      "Epoch: 072, Loss: 0.6881, Train Acc: 0.5621, Loss: 0.7762, Test Acc: 0.4250\n",
      "Epoch: 073, Loss: 0.6882, Train Acc: 0.5496, Loss: 0.7726, Test Acc: 0.4250\n",
      "Epoch: 074, Loss: 0.6872, Train Acc: 0.5684, Loss: 0.7768, Test Acc: 0.4250\n",
      "Epoch: 075, Loss: 0.6870, Train Acc: 0.5684, Loss: 0.7718, Test Acc: 0.4500\n",
      "Epoch: 076, Loss: 0.6866, Train Acc: 0.5746, Loss: 0.7856, Test Acc: 0.4250\n",
      "Epoch: 077, Loss: 0.6864, Train Acc: 0.5684, Loss: 0.7858, Test Acc: 0.4500\n",
      "Epoch: 078, Loss: 0.6867, Train Acc: 0.5559, Loss: 0.7774, Test Acc: 0.4500\n",
      "Epoch: 079, Loss: 0.6861, Train Acc: 0.5934, Loss: 0.7885, Test Acc: 0.4250\n",
      "Epoch: 080, Loss: 0.6846, Train Acc: 0.6059, Loss: 0.7859, Test Acc: 0.4250\n",
      "Epoch: 081, Loss: 0.6847, Train Acc: 0.5684, Loss: 0.7803, Test Acc: 0.4250\n",
      "Epoch: 082, Loss: 0.6844, Train Acc: 0.5996, Loss: 0.7864, Test Acc: 0.4250\n",
      "Epoch: 083, Loss: 0.6834, Train Acc: 0.5934, Loss: 0.7878, Test Acc: 0.4250\n",
      "Epoch: 084, Loss: 0.6829, Train Acc: 0.5934, Loss: 0.7885, Test Acc: 0.3750\n",
      "Epoch: 085, Loss: 0.6821, Train Acc: 0.5871, Loss: 0.7903, Test Acc: 0.3750\n",
      "Epoch: 086, Loss: 0.6823, Train Acc: 0.6239, Loss: 0.7876, Test Acc: 0.3750\n",
      "Epoch: 087, Loss: 0.6815, Train Acc: 0.5871, Loss: 0.8008, Test Acc: 0.3750\n",
      "Epoch: 088, Loss: 0.6801, Train Acc: 0.6239, Loss: 0.7894, Test Acc: 0.4000\n",
      "Epoch: 089, Loss: 0.6807, Train Acc: 0.6239, Loss: 0.7927, Test Acc: 0.4000\n",
      "Epoch: 090, Loss: 0.6790, Train Acc: 0.6051, Loss: 0.8060, Test Acc: 0.3750\n",
      "Epoch: 091, Loss: 0.6783, Train Acc: 0.6239, Loss: 0.7893, Test Acc: 0.4000\n",
      "Epoch: 092, Loss: 0.6789, Train Acc: 0.6114, Loss: 0.8045, Test Acc: 0.4000\n",
      "Epoch: 093, Loss: 0.6768, Train Acc: 0.6176, Loss: 0.8036, Test Acc: 0.4000\n",
      "Epoch: 094, Loss: 0.6771, Train Acc: 0.6176, Loss: 0.8048, Test Acc: 0.4250\n",
      "Epoch: 095, Loss: 0.6760, Train Acc: 0.6176, Loss: 0.8099, Test Acc: 0.4250\n",
      "Epoch: 096, Loss: 0.6754, Train Acc: 0.6176, Loss: 0.8074, Test Acc: 0.3750\n",
      "Epoch: 097, Loss: 0.6750, Train Acc: 0.6114, Loss: 0.8160, Test Acc: 0.4250\n",
      "Epoch: 098, Loss: 0.6741, Train Acc: 0.6239, Loss: 0.8110, Test Acc: 0.3750\n",
      "Epoch: 099, Loss: 0.6737, Train Acc: 0.6176, Loss: 0.8211, Test Acc: 0.4000\n",
      "Epoch: 100, Loss: 0.6723, Train Acc: 0.6239, Loss: 0.8158, Test Acc: 0.3750\n",
      "Epoch: 101, Loss: 0.6725, Train Acc: 0.6239, Loss: 0.8264, Test Acc: 0.3750\n",
      "Epoch: 102, Loss: 0.6716, Train Acc: 0.6301, Loss: 0.8290, Test Acc: 0.3750\n",
      "Epoch: 103, Loss: 0.6713, Train Acc: 0.6239, Loss: 0.8254, Test Acc: 0.3750\n",
      "Epoch: 104, Loss: 0.6701, Train Acc: 0.6176, Loss: 0.8407, Test Acc: 0.4000\n",
      "Epoch: 105, Loss: 0.6689, Train Acc: 0.6301, Loss: 0.8290, Test Acc: 0.3500\n",
      "Epoch: 106, Loss: 0.6692, Train Acc: 0.6239, Loss: 0.8480, Test Acc: 0.4000\n",
      "Epoch: 107, Loss: 0.6668, Train Acc: 0.6176, Loss: 0.8444, Test Acc: 0.3500\n",
      "Epoch: 108, Loss: 0.6669, Train Acc: 0.6301, Loss: 0.8472, Test Acc: 0.3500\n",
      "Epoch: 109, Loss: 0.6658, Train Acc: 0.6364, Loss: 0.8511, Test Acc: 0.3500\n",
      "Epoch: 110, Loss: 0.6654, Train Acc: 0.6364, Loss: 0.8549, Test Acc: 0.3500\n",
      "Epoch: 111, Loss: 0.6640, Train Acc: 0.6364, Loss: 0.8567, Test Acc: 0.3500\n",
      "Epoch: 112, Loss: 0.6640, Train Acc: 0.6364, Loss: 0.8520, Test Acc: 0.3500\n",
      "Epoch: 113, Loss: 0.6631, Train Acc: 0.6239, Loss: 0.8699, Test Acc: 0.3500\n",
      "Epoch: 114, Loss: 0.6613, Train Acc: 0.6419, Loss: 0.8672, Test Acc: 0.3500\n",
      "Epoch: 115, Loss: 0.6615, Train Acc: 0.6419, Loss: 0.8767, Test Acc: 0.3500\n",
      "Epoch: 116, Loss: 0.6601, Train Acc: 0.6357, Loss: 0.8827, Test Acc: 0.3500\n",
      "Epoch: 117, Loss: 0.6593, Train Acc: 0.6419, Loss: 0.8809, Test Acc: 0.3500\n",
      "Epoch: 118, Loss: 0.6588, Train Acc: 0.6419, Loss: 0.8862, Test Acc: 0.3500\n",
      "Epoch: 119, Loss: 0.6588, Train Acc: 0.6482, Loss: 0.8825, Test Acc: 0.3500\n",
      "Epoch: 120, Loss: 0.6565, Train Acc: 0.6482, Loss: 0.8796, Test Acc: 0.3500\n",
      "Epoch: 121, Loss: 0.6569, Train Acc: 0.6544, Loss: 0.8966, Test Acc: 0.3500\n",
      "Epoch: 122, Loss: 0.6555, Train Acc: 0.6482, Loss: 0.8968, Test Acc: 0.3500\n",
      "Epoch: 123, Loss: 0.6550, Train Acc: 0.6482, Loss: 0.8965, Test Acc: 0.3500\n",
      "Epoch: 124, Loss: 0.6547, Train Acc: 0.6482, Loss: 0.8992, Test Acc: 0.3500\n",
      "Epoch: 125, Loss: 0.6527, Train Acc: 0.6482, Loss: 0.9044, Test Acc: 0.3500\n",
      "Epoch: 126, Loss: 0.6525, Train Acc: 0.6482, Loss: 0.9081, Test Acc: 0.3500\n",
      "Epoch: 127, Loss: 0.6524, Train Acc: 0.6482, Loss: 0.9122, Test Acc: 0.3500\n",
      "Epoch: 128, Loss: 0.6495, Train Acc: 0.6419, Loss: 0.9084, Test Acc: 0.3500\n",
      "Epoch: 129, Loss: 0.6511, Train Acc: 0.6419, Loss: 0.9163, Test Acc: 0.3500\n",
      "Epoch: 130, Loss: 0.6484, Train Acc: 0.6482, Loss: 0.9217, Test Acc: 0.3500\n",
      "Epoch: 131, Loss: 0.6489, Train Acc: 0.6482, Loss: 0.9181, Test Acc: 0.3500\n",
      "Epoch: 132, Loss: 0.6472, Train Acc: 0.6482, Loss: 0.9304, Test Acc: 0.3500\n",
      "Epoch: 133, Loss: 0.6474, Train Acc: 0.6544, Loss: 0.9284, Test Acc: 0.3500\n",
      "Epoch: 134, Loss: 0.6456, Train Acc: 0.6544, Loss: 0.9340, Test Acc: 0.3500\n",
      "Epoch: 135, Loss: 0.6464, Train Acc: 0.6544, Loss: 0.9348, Test Acc: 0.3000\n",
      "Epoch: 136, Loss: 0.6431, Train Acc: 0.6544, Loss: 0.9434, Test Acc: 0.3500\n",
      "Epoch: 137, Loss: 0.6440, Train Acc: 0.6607, Loss: 0.9364, Test Acc: 0.3000\n",
      "Epoch: 138, Loss: 0.6442, Train Acc: 0.6544, Loss: 0.9562, Test Acc: 0.3500\n",
      "Epoch: 139, Loss: 0.6427, Train Acc: 0.6607, Loss: 0.9502, Test Acc: 0.3250\n",
      "Epoch: 140, Loss: 0.6406, Train Acc: 0.6607, Loss: 0.9457, Test Acc: 0.3000\n",
      "Epoch: 141, Loss: 0.6409, Train Acc: 0.6607, Loss: 0.9655, Test Acc: 0.3000\n",
      "Epoch: 142, Loss: 0.6387, Train Acc: 0.6482, Loss: 0.9520, Test Acc: 0.3250\n",
      "Epoch: 143, Loss: 0.6388, Train Acc: 0.6607, Loss: 0.9687, Test Acc: 0.3250\n",
      "Epoch: 144, Loss: 0.6385, Train Acc: 0.6607, Loss: 0.9658, Test Acc: 0.3250\n",
      "Epoch: 145, Loss: 0.6360, Train Acc: 0.6482, Loss: 0.9755, Test Acc: 0.3250\n",
      "Epoch: 146, Loss: 0.6386, Train Acc: 0.6607, Loss: 0.9631, Test Acc: 0.3250\n",
      "Epoch: 147, Loss: 0.6329, Train Acc: 0.6607, Loss: 0.9623, Test Acc: 0.3250\n",
      "Epoch: 148, Loss: 0.6357, Train Acc: 0.6544, Loss: 0.9823, Test Acc: 0.3250\n",
      "Epoch: 149, Loss: 0.6331, Train Acc: 0.6607, Loss: 0.9646, Test Acc: 0.3250\n",
      "Epoch: 150, Loss: 0.6324, Train Acc: 0.6544, Loss: 0.9636, Test Acc: 0.3250\n",
      "Epoch: 151, Loss: 0.6306, Train Acc: 0.6544, Loss: 0.9821, Test Acc: 0.3250\n",
      "Epoch: 152, Loss: 0.6321, Train Acc: 0.6544, Loss: 0.9945, Test Acc: 0.3250\n",
      "Epoch: 153, Loss: 0.6291, Train Acc: 0.6544, Loss: 0.9732, Test Acc: 0.3250\n",
      "Epoch: 154, Loss: 0.6285, Train Acc: 0.6544, Loss: 0.9755, Test Acc: 0.3250\n",
      "Epoch: 155, Loss: 0.6301, Train Acc: 0.6544, Loss: 1.0268, Test Acc: 0.3500\n",
      "Epoch: 156, Loss: 0.6255, Train Acc: 0.6544, Loss: 0.9352, Test Acc: 0.3000\n",
      "Epoch: 157, Loss: 0.6274, Train Acc: 0.6669, Loss: 1.0131, Test Acc: 0.3500\n",
      "Epoch: 158, Loss: 0.6253, Train Acc: 0.6544, Loss: 0.9951, Test Acc: 0.3250\n",
      "Epoch: 159, Loss: 0.6239, Train Acc: 0.6607, Loss: 0.9837, Test Acc: 0.3250\n",
      "Epoch: 160, Loss: 0.6232, Train Acc: 0.6669, Loss: 0.9856, Test Acc: 0.3250\n",
      "Epoch: 161, Loss: 0.6231, Train Acc: 0.6544, Loss: 1.0263, Test Acc: 0.3500\n",
      "Epoch: 162, Loss: 0.6218, Train Acc: 0.6544, Loss: 0.9921, Test Acc: 0.3250\n",
      "Epoch: 163, Loss: 0.6196, Train Acc: 0.6669, Loss: 0.9800, Test Acc: 0.3250\n",
      "Epoch: 164, Loss: 0.6208, Train Acc: 0.6544, Loss: 1.0388, Test Acc: 0.3500\n",
      "Epoch: 165, Loss: 0.6171, Train Acc: 0.6849, Loss: 0.9761, Test Acc: 0.3250\n",
      "Epoch: 166, Loss: 0.6186, Train Acc: 0.6669, Loss: 1.0007, Test Acc: 0.3250\n",
      "Epoch: 167, Loss: 0.6220, Train Acc: 0.6607, Loss: 1.0608, Test Acc: 0.3500\n",
      "Epoch: 168, Loss: 0.6131, Train Acc: 0.6717, Loss: 0.9516, Test Acc: 0.3000\n",
      "Epoch: 169, Loss: 0.6227, Train Acc: 0.6489, Loss: 1.0438, Test Acc: 0.3750\n",
      "Epoch: 170, Loss: 0.6102, Train Acc: 0.6904, Loss: 0.9817, Test Acc: 0.3250\n",
      "Epoch: 171, Loss: 0.6186, Train Acc: 0.6607, Loss: 1.0651, Test Acc: 0.3500\n",
      "Epoch: 172, Loss: 0.6082, Train Acc: 0.7029, Loss: 0.9476, Test Acc: 0.3500\n",
      "Epoch: 173, Loss: 0.6126, Train Acc: 0.6544, Loss: 1.0660, Test Acc: 0.3500\n",
      "Epoch: 174, Loss: 0.6105, Train Acc: 0.6732, Loss: 1.0150, Test Acc: 0.3250\n",
      "Epoch: 175, Loss: 0.6056, Train Acc: 0.6904, Loss: 1.0043, Test Acc: 0.3500\n",
      "Epoch: 176, Loss: 0.6169, Train Acc: 0.6676, Loss: 1.0778, Test Acc: 0.3750\n",
      "Epoch: 177, Loss: 0.5999, Train Acc: 0.7022, Loss: 0.9814, Test Acc: 0.3250\n",
      "Epoch: 178, Loss: 0.6141, Train Acc: 0.6489, Loss: 1.1153, Test Acc: 0.3750\n",
      "Epoch: 179, Loss: 0.6008, Train Acc: 0.7210, Loss: 0.9288, Test Acc: 0.3000\n",
      "Epoch: 180, Loss: 0.6045, Train Acc: 0.6607, Loss: 1.0828, Test Acc: 0.3500\n",
      "Epoch: 181, Loss: 0.6039, Train Acc: 0.6607, Loss: 1.0245, Test Acc: 0.3250\n",
      "Epoch: 182, Loss: 0.5979, Train Acc: 0.7022, Loss: 0.9611, Test Acc: 0.3250\n",
      "Epoch: 183, Loss: 0.6038, Train Acc: 0.7092, Loss: 1.0451, Test Acc: 0.3750\n",
      "Epoch: 184, Loss: 0.5955, Train Acc: 0.7029, Loss: 1.0379, Test Acc: 0.3500\n",
      "Epoch: 185, Loss: 0.5988, Train Acc: 0.6724, Loss: 1.0574, Test Acc: 0.3750\n",
      "Epoch: 186, Loss: 0.5918, Train Acc: 0.7210, Loss: 0.9600, Test Acc: 0.3250\n",
      "Epoch: 187, Loss: 0.5996, Train Acc: 0.6974, Loss: 1.0939, Test Acc: 0.3750\n",
      "Epoch: 188, Loss: 0.5882, Train Acc: 0.7085, Loss: 1.0271, Test Acc: 0.3500\n",
      "Epoch: 189, Loss: 0.5888, Train Acc: 0.7085, Loss: 1.0366, Test Acc: 0.3750\n",
      "Epoch: 190, Loss: 0.5882, Train Acc: 0.7210, Loss: 1.0312, Test Acc: 0.3500\n",
      "Epoch: 191, Loss: 0.5903, Train Acc: 0.7092, Loss: 1.1036, Test Acc: 0.3750\n",
      "Epoch: 192, Loss: 0.5812, Train Acc: 0.7335, Loss: 0.9343, Test Acc: 0.4000\n",
      "Epoch: 193, Loss: 0.5923, Train Acc: 0.6864, Loss: 1.0649, Test Acc: 0.4000\n",
      "Epoch: 194, Loss: 0.5808, Train Acc: 0.7272, Loss: 1.0086, Test Acc: 0.3500\n",
      "Epoch: 195, Loss: 0.5784, Train Acc: 0.7397, Loss: 1.0552, Test Acc: 0.3750\n",
      "Epoch: 196, Loss: 0.5756, Train Acc: 0.7335, Loss: 1.0643, Test Acc: 0.3750\n",
      "Epoch: 197, Loss: 0.5746, Train Acc: 0.7335, Loss: 1.0218, Test Acc: 0.3750\n",
      "Epoch: 198, Loss: 0.5726, Train Acc: 0.7210, Loss: 1.0054, Test Acc: 0.3500\n",
      "Epoch: 199, Loss: 0.5907, Train Acc: 0.6919, Loss: 1.1391, Test Acc: 0.4750\n",
      "Epoch: 200, Loss: 0.5746, Train Acc: 0.6974, Loss: 0.9236, Test Acc: 0.4000\n",
      "Epoch: 201, Loss: 0.5798, Train Acc: 0.7529, Loss: 1.0640, Test Acc: 0.3750\n",
      "Epoch: 202, Loss: 0.5742, Train Acc: 0.7460, Loss: 1.0918, Test Acc: 0.3750\n",
      "Epoch: 203, Loss: 0.5618, Train Acc: 0.7335, Loss: 0.9335, Test Acc: 0.4000\n",
      "Epoch: 204, Loss: 0.5776, Train Acc: 0.7404, Loss: 0.9714, Test Acc: 0.4750\n",
      "Epoch: 205, Loss: 0.5611, Train Acc: 0.7272, Loss: 1.0120, Test Acc: 0.3500\n",
      "Epoch: 206, Loss: 0.5600, Train Acc: 0.7404, Loss: 1.0236, Test Acc: 0.4250\n",
      "Epoch: 207, Loss: 0.5534, Train Acc: 0.7397, Loss: 0.9627, Test Acc: 0.4000\n",
      "Epoch: 208, Loss: 0.5595, Train Acc: 0.7044, Loss: 1.1594, Test Acc: 0.4000\n",
      "Epoch: 209, Loss: 0.5578, Train Acc: 0.7460, Loss: 0.9938, Test Acc: 0.4000\n",
      "Epoch: 210, Loss: 0.5426, Train Acc: 0.7279, Loss: 0.9172, Test Acc: 0.4000\n",
      "Epoch: 211, Loss: 0.5482, Train Acc: 0.7522, Loss: 1.0784, Test Acc: 0.3500\n",
      "Epoch: 212, Loss: 0.5553, Train Acc: 0.7404, Loss: 0.9546, Test Acc: 0.4000\n",
      "Epoch: 213, Loss: 0.5381, Train Acc: 0.7772, Loss: 0.9474, Test Acc: 0.4000\n",
      "Epoch: 214, Loss: 0.5390, Train Acc: 0.7585, Loss: 0.9588, Test Acc: 0.4500\n",
      "Epoch: 215, Loss: 0.5429, Train Acc: 0.7585, Loss: 1.0820, Test Acc: 0.3500\n",
      "Epoch: 216, Loss: 0.5462, Train Acc: 0.7342, Loss: 0.9982, Test Acc: 0.4000\n",
      "Epoch: 217, Loss: 0.5346, Train Acc: 0.7710, Loss: 0.9695, Test Acc: 0.3500\n",
      "Epoch: 218, Loss: 0.5469, Train Acc: 0.7224, Loss: 0.9329, Test Acc: 0.4500\n",
      "Epoch: 219, Loss: 0.5382, Train Acc: 0.7772, Loss: 1.0385, Test Acc: 0.4000\n",
      "Epoch: 220, Loss: 0.5224, Train Acc: 0.7647, Loss: 0.9082, Test Acc: 0.4250\n",
      "Epoch: 221, Loss: 0.5230, Train Acc: 0.7460, Loss: 1.0156, Test Acc: 0.3750\n",
      "Epoch: 222, Loss: 0.5203, Train Acc: 0.7585, Loss: 0.9795, Test Acc: 0.4250\n",
      "Epoch: 223, Loss: 0.5153, Train Acc: 0.8195, Loss: 0.9075, Test Acc: 0.3750\n",
      "Epoch: 224, Loss: 0.5320, Train Acc: 0.7592, Loss: 0.9811, Test Acc: 0.4250\n",
      "Epoch: 225, Loss: 0.5326, Train Acc: 0.7765, Loss: 1.0686, Test Acc: 0.3750\n",
      "Epoch: 226, Loss: 0.5218, Train Acc: 0.7772, Loss: 0.7323, Test Acc: 0.4500\n",
      "Epoch: 227, Loss: 0.5185, Train Acc: 0.7710, Loss: 1.1822, Test Acc: 0.4000\n",
      "Epoch: 228, Loss: 0.5194, Train Acc: 0.7952, Loss: 0.9489, Test Acc: 0.4250\n",
      "Epoch: 229, Loss: 0.5036, Train Acc: 0.7952, Loss: 0.8510, Test Acc: 0.4500\n",
      "Epoch: 230, Loss: 0.5010, Train Acc: 0.8070, Loss: 0.9930, Test Acc: 0.3500\n",
      "Epoch: 231, Loss: 0.5275, Train Acc: 0.7585, Loss: 0.8625, Test Acc: 0.4500\n",
      "Epoch: 232, Loss: 0.5173, Train Acc: 0.7585, Loss: 1.1786, Test Acc: 0.4000\n",
      "Epoch: 233, Loss: 0.4997, Train Acc: 0.8312, Loss: 0.7627, Test Acc: 0.4000\n",
      "Epoch: 234, Loss: 0.5037, Train Acc: 0.7765, Loss: 0.8076, Test Acc: 0.4250\n",
      "Epoch: 235, Loss: 0.5150, Train Acc: 0.7890, Loss: 1.1062, Test Acc: 0.3750\n",
      "Epoch: 236, Loss: 0.5087, Train Acc: 0.7772, Loss: 0.7327, Test Acc: 0.4750\n",
      "Epoch: 237, Loss: 0.4938, Train Acc: 0.7640, Loss: 1.0903, Test Acc: 0.4000\n",
      "Epoch: 238, Loss: 0.4848, Train Acc: 0.8140, Loss: 0.8426, Test Acc: 0.4250\n",
      "Epoch: 239, Loss: 0.4630, Train Acc: 0.8500, Loss: 0.8866, Test Acc: 0.4750\n",
      "Epoch: 240, Loss: 0.4723, Train Acc: 0.7765, Loss: 0.9634, Test Acc: 0.4750\n",
      "Epoch: 241, Loss: 0.4666, Train Acc: 0.8257, Loss: 0.8949, Test Acc: 0.4500\n",
      "Epoch: 242, Loss: 0.4612, Train Acc: 0.8438, Loss: 0.8947, Test Acc: 0.4750\n",
      "Epoch: 243, Loss: 0.4605, Train Acc: 0.8132, Loss: 0.9884, Test Acc: 0.4750\n",
      "Epoch: 244, Loss: 0.4515, Train Acc: 0.8438, Loss: 0.9236, Test Acc: 0.5000\n",
      "Epoch: 245, Loss: 0.4497, Train Acc: 0.8132, Loss: 1.0397, Test Acc: 0.4500\n",
      "Epoch: 246, Loss: 0.4513, Train Acc: 0.8375, Loss: 0.9369, Test Acc: 0.5000\n",
      "Epoch: 247, Loss: 0.4390, Train Acc: 0.8562, Loss: 0.7587, Test Acc: 0.4500\n",
      "Epoch: 248, Loss: 0.4426, Train Acc: 0.8562, Loss: 0.8046, Test Acc: 0.5000\n",
      "Epoch: 249, Loss: 0.4586, Train Acc: 0.8500, Loss: 0.8680, Test Acc: 0.4500\n",
      "Epoch: 250, Loss: 0.4975, Train Acc: 0.8022, Loss: 0.8008, Test Acc: 0.4750\n",
      "Epoch: 251, Loss: 0.5254, Train Acc: 0.7710, Loss: 1.2378, Test Acc: 0.4250\n",
      "Epoch: 252, Loss: 0.5247, Train Acc: 0.7224, Loss: 0.8151, Test Acc: 0.5000\n",
      "Epoch: 253, Loss: 0.5250, Train Acc: 0.7952, Loss: 0.7402, Test Acc: 0.5000\n",
      "Epoch: 254, Loss: 0.4873, Train Acc: 0.7827, Loss: 0.7423, Test Acc: 0.4500\n",
      "Epoch: 255, Loss: 0.4849, Train Acc: 0.8022, Loss: 0.7598, Test Acc: 0.5250\n",
      "Epoch: 256, Loss: 0.4515, Train Acc: 0.8257, Loss: 0.9130, Test Acc: 0.4500\n",
      "Epoch: 257, Loss: 0.4390, Train Acc: 0.8438, Loss: 0.8206, Test Acc: 0.4250\n",
      "Epoch: 258, Loss: 0.4310, Train Acc: 0.8375, Loss: 0.8740, Test Acc: 0.4750\n",
      "Epoch: 259, Loss: 0.4197, Train Acc: 0.8625, Loss: 0.8252, Test Acc: 0.4500\n",
      "Epoch: 260, Loss: 0.4272, Train Acc: 0.8375, Loss: 0.8343, Test Acc: 0.4750\n",
      "Epoch: 261, Loss: 0.4269, Train Acc: 0.8438, Loss: 0.9528, Test Acc: 0.5000\n",
      "Epoch: 262, Loss: 0.4248, Train Acc: 0.8625, Loss: 0.7933, Test Acc: 0.4250\n",
      "Epoch: 263, Loss: 0.4300, Train Acc: 0.8375, Loss: 0.8232, Test Acc: 0.4750\n",
      "Epoch: 264, Loss: 0.4330, Train Acc: 0.8452, Loss: 0.6864, Test Acc: 0.4750\n",
      "Epoch: 265, Loss: 0.4308, Train Acc: 0.8382, Loss: 1.0565, Test Acc: 0.4500\n",
      "Epoch: 266, Loss: 0.4347, Train Acc: 0.8147, Loss: 0.8974, Test Acc: 0.4500\n",
      "Epoch: 267, Loss: 0.4206, Train Acc: 0.8382, Loss: 0.9333, Test Acc: 0.4750\n",
      "Epoch: 268, Loss: 0.4038, Train Acc: 0.8687, Loss: 0.6964, Test Acc: 0.4750\n",
      "Epoch: 269, Loss: 0.3959, Train Acc: 0.8500, Loss: 0.7724, Test Acc: 0.4750\n",
      "Epoch: 270, Loss: 0.4194, Train Acc: 0.8695, Loss: 0.8421, Test Acc: 0.4750\n",
      "Epoch: 271, Loss: 0.4241, Train Acc: 0.8202, Loss: 1.1433, Test Acc: 0.4750\n",
      "Epoch: 272, Loss: 0.4257, Train Acc: 0.8452, Loss: 0.8154, Test Acc: 0.5000\n",
      "Epoch: 273, Loss: 0.4163, Train Acc: 0.8140, Loss: 0.8589, Test Acc: 0.4500\n",
      "Epoch: 274, Loss: 0.4159, Train Acc: 0.8577, Loss: 0.7136, Test Acc: 0.4750\n",
      "Epoch: 275, Loss: 0.4072, Train Acc: 0.8570, Loss: 0.9068, Test Acc: 0.4750\n",
      "Epoch: 276, Loss: 0.4062, Train Acc: 0.8272, Loss: 1.1110, Test Acc: 0.4750\n",
      "Epoch: 277, Loss: 0.3935, Train Acc: 0.8625, Loss: 1.0800, Test Acc: 0.5000\n",
      "Epoch: 278, Loss: 0.3830, Train Acc: 0.8687, Loss: 0.6582, Test Acc: 0.4500\n",
      "Epoch: 279, Loss: 0.3786, Train Acc: 0.8257, Loss: 0.5653, Test Acc: 0.4750\n",
      "Epoch: 280, Loss: 0.4004, Train Acc: 0.8687, Loss: 0.6835, Test Acc: 0.4500\n",
      "Epoch: 281, Loss: 0.4230, Train Acc: 0.7835, Loss: 1.2467, Test Acc: 0.5250\n",
      "Epoch: 282, Loss: 0.4302, Train Acc: 0.8202, Loss: 1.3775, Test Acc: 0.4500\n",
      "Epoch: 283, Loss: 0.3982, Train Acc: 0.8688, Loss: 0.6254, Test Acc: 0.4250\n",
      "Epoch: 284, Loss: 0.3704, Train Acc: 0.8195, Loss: 0.5625, Test Acc: 0.4500\n",
      "Epoch: 285, Loss: 0.4043, Train Acc: 0.8750, Loss: 0.7941, Test Acc: 0.4500\n",
      "Epoch: 286, Loss: 0.3907, Train Acc: 0.8687, Loss: 0.9850, Test Acc: 0.4750\n",
      "Epoch: 287, Loss: 0.4144, Train Acc: 0.8452, Loss: 0.7630, Test Acc: 0.4750\n",
      "Epoch: 288, Loss: 0.3875, Train Acc: 0.8327, Loss: 1.2318, Test Acc: 0.4750\n",
      "Epoch: 289, Loss: 0.4044, Train Acc: 0.8452, Loss: 0.6764, Test Acc: 0.4750\n",
      "Epoch: 290, Loss: 0.3705, Train Acc: 0.8507, Loss: 1.0302, Test Acc: 0.4500\n",
      "Epoch: 291, Loss: 0.3788, Train Acc: 0.8882, Loss: 0.7365, Test Acc: 0.4250\n",
      "Epoch: 292, Loss: 0.3543, Train Acc: 0.8570, Loss: 1.1358, Test Acc: 0.4750\n",
      "Epoch: 293, Loss: 0.3468, Train Acc: 0.8875, Loss: 0.9084, Test Acc: 0.4500\n",
      "Epoch: 294, Loss: 0.3279, Train Acc: 0.8938, Loss: 0.7425, Test Acc: 0.4750\n",
      "Epoch: 295, Loss: 0.3238, Train Acc: 0.8875, Loss: 0.4864, Test Acc: 0.4500\n",
      "Epoch: 296, Loss: 0.3300, Train Acc: 0.9062, Loss: 0.6399, Test Acc: 0.4500\n",
      "Epoch: 297, Loss: 0.3495, Train Acc: 0.8875, Loss: 0.8640, Test Acc: 0.4500\n",
      "Epoch: 298, Loss: 0.3613, Train Acc: 0.8687, Loss: 1.3136, Test Acc: 0.5000\n",
      "Epoch: 299, Loss: 0.3856, Train Acc: 0.8452, Loss: 1.0156, Test Acc: 0.4250\n",
      "Epoch: 300, Loss: 0.3729, Train Acc: 0.8452, Loss: 1.4168, Test Acc: 0.4750\n",
      "Epoch: 301, Loss: 0.3938, Train Acc: 0.8335, Loss: 0.8673, Test Acc: 0.4750\n",
      "Epoch: 302, Loss: 0.3516, Train Acc: 0.8632, Loss: 1.2831, Test Acc: 0.4500\n",
      "Epoch: 303, Loss: 0.3733, Train Acc: 0.8452, Loss: 1.0511, Test Acc: 0.5000\n",
      "Epoch: 304, Loss: 0.3270, Train Acc: 0.8813, Loss: 1.2219, Test Acc: 0.4750\n",
      "Epoch: 305, Loss: 0.3179, Train Acc: 0.8875, Loss: 0.9187, Test Acc: 0.4750\n",
      "Epoch: 306, Loss: 0.2934, Train Acc: 0.9062, Loss: 0.5473, Test Acc: 0.4250\n",
      "Epoch: 307, Loss: 0.2936, Train Acc: 0.9000, Loss: 0.6496, Test Acc: 0.4500\n",
      "Epoch: 308, Loss: 0.3024, Train Acc: 0.9062, Loss: 0.6522, Test Acc: 0.4500\n",
      "Epoch: 309, Loss: 0.3092, Train Acc: 0.9125, Loss: 0.9261, Test Acc: 0.4750\n",
      "Epoch: 310, Loss: 0.3436, Train Acc: 0.8938, Loss: 1.3281, Test Acc: 0.5000\n",
      "Epoch: 311, Loss: 0.3450, Train Acc: 0.8335, Loss: 1.7853, Test Acc: 0.4750\n",
      "Epoch: 312, Loss: 0.3784, Train Acc: 0.8452, Loss: 1.2071, Test Acc: 0.4250\n",
      "Epoch: 313, Loss: 0.3558, Train Acc: 0.8577, Loss: 1.4456, Test Acc: 0.4750\n",
      "Epoch: 314, Loss: 0.3411, Train Acc: 0.8640, Loss: 0.7164, Test Acc: 0.4750\n",
      "Epoch: 315, Loss: 0.3139, Train Acc: 0.8820, Loss: 1.1533, Test Acc: 0.4500\n",
      "Epoch: 316, Loss: 0.3331, Train Acc: 0.8452, Loss: 1.1215, Test Acc: 0.5250\n",
      "Epoch: 317, Loss: 0.3281, Train Acc: 0.8632, Loss: 1.7252, Test Acc: 0.4750\n",
      "Epoch: 318, Loss: 0.3234, Train Acc: 0.9000, Loss: 1.1348, Test Acc: 0.4750\n",
      "Epoch: 319, Loss: 0.2800, Train Acc: 0.9250, Loss: 0.5355, Test Acc: 0.4500\n",
      "Epoch: 320, Loss: 0.2791, Train Acc: 0.9062, Loss: 0.3981, Test Acc: 0.4500\n",
      "Epoch: 321, Loss: 0.2870, Train Acc: 0.9187, Loss: 0.5588, Test Acc: 0.4250\n",
      "Epoch: 322, Loss: 0.2916, Train Acc: 0.9250, Loss: 0.9954, Test Acc: 0.5000\n",
      "Epoch: 323, Loss: 0.2950, Train Acc: 0.8812, Loss: 1.8060, Test Acc: 0.5000\n",
      "Epoch: 324, Loss: 0.3267, Train Acc: 0.8140, Loss: 1.9492, Test Acc: 0.5250\n",
      "Epoch: 325, Loss: 0.3324, Train Acc: 0.9125, Loss: 0.6162, Test Acc: 0.4500\n",
      "Epoch: 326, Loss: 0.2748, Train Acc: 0.8632, Loss: 0.2252, Test Acc: 0.5500\n",
      "Epoch: 327, Loss: 0.3151, Train Acc: 0.9187, Loss: 0.5120, Test Acc: 0.4250\n",
      "Epoch: 328, Loss: 0.2991, Train Acc: 0.9250, Loss: 1.1890, Test Acc: 0.5000\n",
      "Epoch: 329, Loss: 0.2918, Train Acc: 0.8452, Loss: 1.7428, Test Acc: 0.5250\n",
      "Epoch: 330, Loss: 0.3209, Train Acc: 0.8702, Loss: 1.3702, Test Acc: 0.4750\n",
      "Epoch: 331, Loss: 0.2702, Train Acc: 0.9375, Loss: 0.3823, Test Acc: 0.4500\n",
      "Epoch: 332, Loss: 0.2469, Train Acc: 0.9437, Loss: 0.3765, Test Acc: 0.5000\n",
      "Epoch: 333, Loss: 0.2666, Train Acc: 0.9250, Loss: 0.9224, Test Acc: 0.4250\n",
      "Epoch: 334, Loss: 0.2498, Train Acc: 0.9062, Loss: 1.6115, Test Acc: 0.5000\n",
      "Epoch: 335, Loss: 0.2752, Train Acc: 0.8577, Loss: 1.5130, Test Acc: 0.5000\n",
      "Epoch: 336, Loss: 0.2701, Train Acc: 0.9250, Loss: 0.8879, Test Acc: 0.5000\n",
      "Epoch: 337, Loss: 0.2273, Train Acc: 0.9375, Loss: 0.2330, Test Acc: 0.5250\n",
      "Epoch: 338, Loss: 0.2445, Train Acc: 0.9375, Loss: 0.4176, Test Acc: 0.5000\n",
      "Epoch: 339, Loss: 0.2590, Train Acc: 0.9500, Loss: 0.9811, Test Acc: 0.4250\n",
      "Epoch: 340, Loss: 0.2408, Train Acc: 0.9250, Loss: 1.4013, Test Acc: 0.4750\n",
      "Epoch: 341, Loss: 0.2626, Train Acc: 0.8757, Loss: 1.6243, Test Acc: 0.5500\n",
      "Epoch: 342, Loss: 0.2633, Train Acc: 0.9250, Loss: 1.3188, Test Acc: 0.4500\n",
      "Epoch: 343, Loss: 0.2265, Train Acc: 0.9375, Loss: 0.3830, Test Acc: 0.4500\n",
      "Epoch: 344, Loss: 0.2191, Train Acc: 0.9437, Loss: 0.6369, Test Acc: 0.5500\n",
      "Epoch: 345, Loss: 0.2356, Train Acc: 0.9437, Loss: 0.7402, Test Acc: 0.4000\n",
      "Epoch: 346, Loss: 0.2154, Train Acc: 0.9375, Loss: 1.3400, Test Acc: 0.4750\n",
      "Epoch: 347, Loss: 0.2407, Train Acc: 0.8820, Loss: 1.6239, Test Acc: 0.5750\n",
      "Epoch: 348, Loss: 0.2521, Train Acc: 0.9257, Loss: 1.5610, Test Acc: 0.4750\n",
      "Epoch: 349, Loss: 0.2233, Train Acc: 0.9500, Loss: 0.5964, Test Acc: 0.4250\n",
      "Epoch: 350, Loss: 0.1962, Train Acc: 0.9563, Loss: 0.4875, Test Acc: 0.5250\n",
      "Epoch: 351, Loss: 0.2079, Train Acc: 0.9437, Loss: 0.4507, Test Acc: 0.4000\n",
      "Epoch: 352, Loss: 0.1839, Train Acc: 0.9500, Loss: 1.3288, Test Acc: 0.4750\n",
      "Epoch: 353, Loss: 0.2041, Train Acc: 0.9375, Loss: 1.2898, Test Acc: 0.4750\n",
      "Epoch: 354, Loss: 0.2027, Train Acc: 0.9320, Loss: 1.4299, Test Acc: 0.4500\n",
      "Epoch: 355, Loss: 0.1996, Train Acc: 0.9375, Loss: 1.0005, Test Acc: 0.4250\n",
      "Epoch: 356, Loss: 0.1824, Train Acc: 0.9500, Loss: 0.9493, Test Acc: 0.5000\n",
      "Epoch: 357, Loss: 0.1745, Train Acc: 0.9625, Loss: 0.4690, Test Acc: 0.4500\n",
      "Epoch: 358, Loss: 0.1612, Train Acc: 0.9625, Loss: 0.6712, Test Acc: 0.5500\n",
      "Epoch: 359, Loss: 0.1685, Train Acc: 0.9688, Loss: 0.6265, Test Acc: 0.5250\n",
      "Epoch: 360, Loss: 0.1539, Train Acc: 0.9625, Loss: 0.9794, Test Acc: 0.5000\n",
      "Epoch: 361, Loss: 0.1760, Train Acc: 0.9500, Loss: 1.2668, Test Acc: 0.5000\n",
      "Epoch: 362, Loss: 0.1752, Train Acc: 0.9250, Loss: 1.5456, Test Acc: 0.5000\n",
      "Epoch: 363, Loss: 0.1905, Train Acc: 0.8938, Loss: 1.4924, Test Acc: 0.5250\n",
      "Epoch: 364, Loss: 0.1991, Train Acc: 0.9438, Loss: 1.2416, Test Acc: 0.4500\n",
      "Epoch: 365, Loss: 0.1729, Train Acc: 0.9625, Loss: 0.5196, Test Acc: 0.5250\n",
      "Epoch: 366, Loss: 0.1923, Train Acc: 0.8132, Loss: 0.0254, Test Acc: 0.5250\n",
      "Epoch: 367, Loss: 0.4865, Train Acc: 0.6272, Loss: 0.0104, Test Acc: 0.4750\n",
      "Epoch: 368, Loss: 0.8926, Train Acc: 0.8702, Loss: 0.4215, Test Acc: 0.5250\n",
      "Epoch: 369, Loss: 0.7385, Train Acc: 0.6739, Loss: 2.0487, Test Acc: 0.5250\n",
      "Epoch: 370, Loss: 0.6969, Train Acc: 0.7015, Loss: 0.2188, Test Acc: 0.4500\n",
      "Epoch: 371, Loss: 0.5033, Train Acc: 0.8452, Loss: 0.5457, Test Acc: 0.4500\n",
      "Epoch: 372, Loss: 0.4367, Train Acc: 0.8390, Loss: 0.7326, Test Acc: 0.4500\n",
      "Epoch: 373, Loss: 0.3914, Train Acc: 0.8640, Loss: 0.4641, Test Acc: 0.4250\n",
      "Epoch: 374, Loss: 0.3608, Train Acc: 0.8390, Loss: 0.6824, Test Acc: 0.4250\n",
      "Epoch: 375, Loss: 0.3336, Train Acc: 0.9062, Loss: 0.4422, Test Acc: 0.4750\n",
      "Epoch: 376, Loss: 0.2992, Train Acc: 0.9250, Loss: 0.6130, Test Acc: 0.4750\n",
      "Epoch: 377, Loss: 0.2911, Train Acc: 0.9375, Loss: 0.6638, Test Acc: 0.4500\n",
      "Epoch: 378, Loss: 0.2646, Train Acc: 0.9312, Loss: 0.5009, Test Acc: 0.4750\n",
      "Epoch: 379, Loss: 0.2556, Train Acc: 0.9375, Loss: 0.6963, Test Acc: 0.4500\n",
      "Epoch: 380, Loss: 0.2496, Train Acc: 0.9563, Loss: 0.7701, Test Acc: 0.4250\n",
      "Epoch: 381, Loss: 0.2287, Train Acc: 0.9438, Loss: 0.6716, Test Acc: 0.5000\n",
      "Epoch: 382, Loss: 0.2191, Train Acc: 0.9563, Loss: 0.7543, Test Acc: 0.4750\n",
      "Epoch: 383, Loss: 0.2103, Train Acc: 0.9500, Loss: 0.9010, Test Acc: 0.5000\n",
      "Epoch: 384, Loss: 0.2058, Train Acc: 0.9500, Loss: 0.7558, Test Acc: 0.5000\n",
      "Epoch: 385, Loss: 0.1980, Train Acc: 0.9563, Loss: 0.8633, Test Acc: 0.5000\n",
      "Epoch: 386, Loss: 0.1944, Train Acc: 0.9563, Loss: 1.0902, Test Acc: 0.5250\n",
      "Epoch: 387, Loss: 0.1918, Train Acc: 0.9625, Loss: 0.8304, Test Acc: 0.4750\n",
      "Epoch: 388, Loss: 0.1822, Train Acc: 0.9625, Loss: 0.8806, Test Acc: 0.5250\n",
      "Epoch: 389, Loss: 0.1796, Train Acc: 0.9563, Loss: 1.1135, Test Acc: 0.4750\n",
      "Epoch: 390, Loss: 0.1787, Train Acc: 0.9625, Loss: 1.1377, Test Acc: 0.4750\n",
      "Epoch: 391, Loss: 0.1724, Train Acc: 0.9688, Loss: 0.9968, Test Acc: 0.5500\n",
      "Epoch: 392, Loss: 0.1677, Train Acc: 0.9688, Loss: 0.9220, Test Acc: 0.4750\n",
      "Epoch: 393, Loss: 0.1664, Train Acc: 0.9625, Loss: 1.2636, Test Acc: 0.4750\n",
      "Epoch: 394, Loss: 0.1661, Train Acc: 0.9688, Loss: 1.1415, Test Acc: 0.4750\n",
      "Epoch: 395, Loss: 0.1576, Train Acc: 0.9688, Loss: 1.1088, Test Acc: 0.5500\n",
      "Epoch: 396, Loss: 0.1536, Train Acc: 0.9688, Loss: 1.0904, Test Acc: 0.5250\n",
      "Epoch: 397, Loss: 0.1520, Train Acc: 0.9750, Loss: 1.3511, Test Acc: 0.4750\n",
      "Epoch: 398, Loss: 0.1493, Train Acc: 0.9750, Loss: 1.2916, Test Acc: 0.4750\n",
      "Epoch: 399, Loss: 0.1416, Train Acc: 0.9688, Loss: 1.1765, Test Acc: 0.5500\n",
      "Epoch: 400, Loss: 0.1406, Train Acc: 0.9688, Loss: 1.2894, Test Acc: 0.5000\n",
      "Epoch: 401, Loss: 0.1385, Train Acc: 0.9750, Loss: 1.4804, Test Acc: 0.4750\n",
      "Epoch: 402, Loss: 0.1369, Train Acc: 0.9750, Loss: 1.4286, Test Acc: 0.4750\n",
      "Epoch: 403, Loss: 0.1320, Train Acc: 0.9688, Loss: 1.2660, Test Acc: 0.5000\n",
      "Epoch: 404, Loss: 0.1282, Train Acc: 0.9688, Loss: 1.1763, Test Acc: 0.5500\n",
      "Epoch: 405, Loss: 0.1287, Train Acc: 0.9750, Loss: 1.3789, Test Acc: 0.5000\n",
      "Epoch: 406, Loss: 0.1278, Train Acc: 0.9750, Loss: 1.6377, Test Acc: 0.5000\n",
      "Epoch: 407, Loss: 0.1289, Train Acc: 0.9750, Loss: 1.5592, Test Acc: 0.5250\n",
      "Epoch: 408, Loss: 0.1197, Train Acc: 0.9750, Loss: 1.2093, Test Acc: 0.5500\n",
      "Epoch: 409, Loss: 0.1211, Train Acc: 0.9750, Loss: 1.0927, Test Acc: 0.5000\n",
      "Epoch: 410, Loss: 0.1197, Train Acc: 0.9750, Loss: 1.3648, Test Acc: 0.5000\n",
      "Epoch: 411, Loss: 0.1189, Train Acc: 0.9750, Loss: 1.7401, Test Acc: 0.5000\n",
      "Epoch: 412, Loss: 0.1215, Train Acc: 0.9750, Loss: 1.7452, Test Acc: 0.5000\n",
      "Epoch: 413, Loss: 0.1138, Train Acc: 0.9750, Loss: 1.2224, Test Acc: 0.5500\n",
      "Epoch: 414, Loss: 0.1115, Train Acc: 0.9750, Loss: 0.9586, Test Acc: 0.5000\n",
      "Epoch: 415, Loss: 0.1308, Train Acc: 0.9812, Loss: 1.1993, Test Acc: 0.5250\n",
      "Epoch: 416, Loss: 0.1192, Train Acc: 0.9750, Loss: 1.9895, Test Acc: 0.5000\n",
      "Epoch: 417, Loss: 0.1310, Train Acc: 0.9438, Loss: 2.4590, Test Acc: 0.5000\n",
      "Epoch: 418, Loss: 0.1376, Train Acc: 0.9688, Loss: 2.1707, Test Acc: 0.5000\n",
      "Epoch: 419, Loss: 0.1114, Train Acc: 0.9500, Loss: 0.8644, Test Acc: 0.5000\n",
      "Epoch: 420, Loss: 0.1362, Train Acc: 0.9250, Loss: 0.9765, Test Acc: 0.5000\n",
      "Epoch: 421, Loss: 0.1613, Train Acc: 0.9750, Loss: 1.2894, Test Acc: 0.5250\n",
      "Epoch: 422, Loss: 0.1386, Train Acc: 0.9563, Loss: 2.7453, Test Acc: 0.5000\n",
      "Epoch: 423, Loss: 0.1772, Train Acc: 0.8640, Loss: 3.6100, Test Acc: 0.5500\n",
      "Epoch: 424, Loss: 0.2383, Train Acc: 0.8640, Loss: 3.2495, Test Acc: 0.5250\n",
      "Epoch: 425, Loss: 0.2108, Train Acc: 0.9312, Loss: 0.8935, Test Acc: 0.4750\n",
      "Epoch: 426, Loss: 0.2372, Train Acc: 0.8382, Loss: 0.1089, Test Acc: 0.5500\n",
      "Epoch: 427, Loss: 0.4365, Train Acc: 0.9250, Loss: 1.9662, Test Acc: 0.4500\n",
      "Epoch: 428, Loss: 0.4162, Train Acc: 0.9187, Loss: 3.3944, Test Acc: 0.4750\n",
      "Epoch: 429, Loss: 0.3669, Train Acc: 0.7897, Loss: 2.7614, Test Acc: 0.4500\n",
      "Epoch: 430, Loss: 0.3537, Train Acc: 0.9438, Loss: 1.7089, Test Acc: 0.5000\n",
      "Epoch: 431, Loss: 0.2043, Train Acc: 0.9500, Loss: 1.2179, Test Acc: 0.4750\n",
      "Epoch: 432, Loss: 0.1963, Train Acc: 0.9500, Loss: 2.0156, Test Acc: 0.4500\n",
      "Epoch: 433, Loss: 0.1455, Train Acc: 0.9375, Loss: 1.3987, Test Acc: 0.5000\n",
      "Epoch: 434, Loss: 0.1995, Train Acc: 0.9625, Loss: 1.9532, Test Acc: 0.4750\n",
      "Epoch: 435, Loss: 0.1347, Train Acc: 0.9625, Loss: 1.2674, Test Acc: 0.5250\n",
      "Epoch: 436, Loss: 0.1380, Train Acc: 0.9688, Loss: 2.0179, Test Acc: 0.5500\n",
      "Epoch: 437, Loss: 0.1260, Train Acc: 0.9688, Loss: 2.3038, Test Acc: 0.4750\n",
      "Epoch: 438, Loss: 0.1048, Train Acc: 0.9688, Loss: 1.3279, Test Acc: 0.5250\n",
      "Epoch: 439, Loss: 0.1252, Train Acc: 0.9750, Loss: 2.1514, Test Acc: 0.5000\n",
      "Epoch: 440, Loss: 0.0981, Train Acc: 0.9812, Loss: 1.8833, Test Acc: 0.5750\n",
      "Epoch: 441, Loss: 0.0933, Train Acc: 0.9812, Loss: 1.6788, Test Acc: 0.5250\n",
      "Epoch: 442, Loss: 0.0942, Train Acc: 0.9750, Loss: 1.8811, Test Acc: 0.4750\n",
      "Epoch: 443, Loss: 0.0800, Train Acc: 0.9812, Loss: 1.7088, Test Acc: 0.5500\n",
      "Epoch: 444, Loss: 0.0960, Train Acc: 0.9750, Loss: 2.1500, Test Acc: 0.4750\n",
      "Epoch: 445, Loss: 0.0861, Train Acc: 0.9750, Loss: 1.9764, Test Acc: 0.5750\n",
      "Epoch: 446, Loss: 0.0879, Train Acc: 0.9812, Loss: 1.9004, Test Acc: 0.4750\n",
      "Epoch: 447, Loss: 0.0832, Train Acc: 0.9812, Loss: 1.9048, Test Acc: 0.4750\n",
      "Epoch: 448, Loss: 0.0712, Train Acc: 0.9750, Loss: 1.6192, Test Acc: 0.5500\n",
      "Epoch: 449, Loss: 0.0881, Train Acc: 0.9750, Loss: 2.4808, Test Acc: 0.4750\n",
      "Epoch: 450, Loss: 0.0808, Train Acc: 0.9875, Loss: 2.0869, Test Acc: 0.5000\n",
      "Epoch: 451, Loss: 0.0779, Train Acc: 0.9812, Loss: 1.5924, Test Acc: 0.5250\n",
      "Epoch: 452, Loss: 0.0760, Train Acc: 0.9875, Loss: 1.9127, Test Acc: 0.4750\n",
      "Epoch: 453, Loss: 0.0653, Train Acc: 0.9875, Loss: 1.6759, Test Acc: 0.5500\n",
      "Epoch: 454, Loss: 0.0731, Train Acc: 0.9750, Loss: 2.2958, Test Acc: 0.4750\n",
      "Epoch: 455, Loss: 0.0725, Train Acc: 0.9875, Loss: 2.0151, Test Acc: 0.4750\n",
      "Epoch: 456, Loss: 0.0678, Train Acc: 0.9875, Loss: 1.8069, Test Acc: 0.4750\n",
      "Epoch: 457, Loss: 0.0687, Train Acc: 0.9750, Loss: 2.1155, Test Acc: 0.4750\n",
      "Epoch: 458, Loss: 0.0602, Train Acc: 0.9875, Loss: 1.8031, Test Acc: 0.5500\n",
      "Epoch: 459, Loss: 0.0714, Train Acc: 0.9750, Loss: 2.3417, Test Acc: 0.4750\n",
      "Epoch: 460, Loss: 0.0717, Train Acc: 1.0000, Loss: 2.3792, Test Acc: 0.4750\n",
      "Epoch: 461, Loss: 0.0613, Train Acc: 0.9812, Loss: 1.5671, Test Acc: 0.5500\n",
      "Epoch: 462, Loss: 0.0738, Train Acc: 0.9875, Loss: 2.5175, Test Acc: 0.4750\n",
      "Epoch: 463, Loss: 0.0708, Train Acc: 1.0000, Loss: 2.4492, Test Acc: 0.4750\n",
      "Epoch: 464, Loss: 0.0587, Train Acc: 0.9875, Loss: 1.4564, Test Acc: 0.5500\n",
      "Epoch: 465, Loss: 0.0754, Train Acc: 0.9875, Loss: 2.4303, Test Acc: 0.4750\n",
      "Epoch: 466, Loss: 0.0712, Train Acc: 1.0000, Loss: 2.9671, Test Acc: 0.4750\n",
      "Epoch: 467, Loss: 0.0610, Train Acc: 0.9563, Loss: 1.2384, Test Acc: 0.5250\n",
      "Epoch: 468, Loss: 0.0909, Train Acc: 0.9812, Loss: 1.9269, Test Acc: 0.4750\n",
      "Epoch: 469, Loss: 0.0756, Train Acc: 0.9875, Loss: 2.9950, Test Acc: 0.4750\n",
      "Epoch: 470, Loss: 0.0625, Train Acc: 0.9688, Loss: 1.7474, Test Acc: 0.5500\n",
      "Epoch: 471, Loss: 0.0813, Train Acc: 0.9812, Loss: 2.0097, Test Acc: 0.4750\n",
      "Epoch: 472, Loss: 0.0689, Train Acc: 0.9875, Loss: 2.9409, Test Acc: 0.4750\n",
      "Epoch: 473, Loss: 0.0530, Train Acc: 0.9750, Loss: 1.7228, Test Acc: 0.5500\n",
      "Epoch: 474, Loss: 0.0765, Train Acc: 0.9750, Loss: 2.0970, Test Acc: 0.4750\n",
      "Epoch: 475, Loss: 0.0642, Train Acc: 1.0000, Loss: 2.3473, Test Acc: 0.4750\n",
      "Epoch: 476, Loss: 0.0482, Train Acc: 0.9937, Loss: 1.9415, Test Acc: 0.5000\n",
      "Epoch: 477, Loss: 0.0553, Train Acc: 0.9875, Loss: 2.9962, Test Acc: 0.4750\n",
      "Epoch: 478, Loss: 0.0529, Train Acc: 1.0000, Loss: 2.2150, Test Acc: 0.4750\n",
      "Epoch: 479, Loss: 0.0485, Train Acc: 1.0000, Loss: 2.2458, Test Acc: 0.4750\n",
      "Epoch: 480, Loss: 0.0494, Train Acc: 0.9938, Loss: 2.6473, Test Acc: 0.4750\n",
      "Epoch: 481, Loss: 0.0402, Train Acc: 1.0000, Loss: 2.2117, Test Acc: 0.5000\n",
      "Epoch: 482, Loss: 0.0484, Train Acc: 0.9875, Loss: 2.7282, Test Acc: 0.4750\n",
      "Epoch: 483, Loss: 0.0478, Train Acc: 1.0000, Loss: 2.4874, Test Acc: 0.4750\n",
      "Epoch: 484, Loss: 0.0416, Train Acc: 1.0000, Loss: 2.2033, Test Acc: 0.4750\n",
      "Epoch: 485, Loss: 0.0442, Train Acc: 1.0000, Loss: 2.8462, Test Acc: 0.4750\n",
      "Epoch: 486, Loss: 0.0372, Train Acc: 1.0000, Loss: 2.2569, Test Acc: 0.5000\n",
      "Epoch: 487, Loss: 0.0402, Train Acc: 1.0000, Loss: 2.5311, Test Acc: 0.4750\n",
      "Epoch: 488, Loss: 0.0390, Train Acc: 1.0000, Loss: 2.7097, Test Acc: 0.4750\n",
      "Epoch: 489, Loss: 0.0332, Train Acc: 1.0000, Loss: 2.2374, Test Acc: 0.4750\n",
      "Epoch: 490, Loss: 0.0373, Train Acc: 1.0000, Loss: 2.7202, Test Acc: 0.4750\n",
      "Epoch: 491, Loss: 0.0338, Train Acc: 1.0000, Loss: 2.6326, Test Acc: 0.4750\n",
      "Epoch: 492, Loss: 0.0331, Train Acc: 1.0000, Loss: 2.5326, Test Acc: 0.4750\n",
      "Epoch: 493, Loss: 0.0321, Train Acc: 1.0000, Loss: 2.7195, Test Acc: 0.4750\n",
      "Epoch: 494, Loss: 0.0302, Train Acc: 1.0000, Loss: 2.4750, Test Acc: 0.4750\n",
      "Epoch: 495, Loss: 0.0307, Train Acc: 1.0000, Loss: 2.5315, Test Acc: 0.4750\n",
      "Epoch: 496, Loss: 0.0296, Train Acc: 1.0000, Loss: 2.6845, Test Acc: 0.4750\n",
      "Epoch: 497, Loss: 0.0286, Train Acc: 1.0000, Loss: 2.5143, Test Acc: 0.4750\n",
      "Epoch: 498, Loss: 0.0293, Train Acc: 1.0000, Loss: 2.6695, Test Acc: 0.4750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 499, Loss: 0.0280, Train Acc: 1.0000, Loss: 2.7034, Test Acc: 0.4750\n",
      "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 141 143 152 153\n",
      " 155] TEST: [140 142 144 145 146 147 148 149 150 151 154 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180]\n",
      "145\n",
      "36\n",
      "Epoch: 001, Loss: 0.8504, Train Acc: 0.5309, Loss: 0.8761, Test Acc: 0.5000\n",
      "Epoch: 002, Loss: 0.7352, Train Acc: 0.5309, Loss: 0.8046, Test Acc: 0.5000\n",
      "Epoch: 003, Loss: 0.6976, Train Acc: 0.4886, Loss: 0.7115, Test Acc: 0.4750\n",
      "Epoch: 004, Loss: 0.7043, Train Acc: 0.5309, Loss: 0.7696, Test Acc: 0.5000\n",
      "Epoch: 005, Loss: 0.6951, Train Acc: 0.5309, Loss: 0.7106, Test Acc: 0.5000\n",
      "Epoch: 006, Loss: 0.6999, Train Acc: 0.5309, Loss: 0.7573, Test Acc: 0.5000\n",
      "Epoch: 007, Loss: 0.6943, Train Acc: 0.5309, Loss: 0.7282, Test Acc: 0.5000\n",
      "Epoch: 008, Loss: 0.6973, Train Acc: 0.5309, Loss: 0.7559, Test Acc: 0.5000\n",
      "Epoch: 009, Loss: 0.6938, Train Acc: 0.5309, Loss: 0.7446, Test Acc: 0.5000\n",
      "Epoch: 010, Loss: 0.6959, Train Acc: 0.5309, Loss: 0.7620, Test Acc: 0.5000\n",
      "Epoch: 011, Loss: 0.6936, Train Acc: 0.5309, Loss: 0.7580, Test Acc: 0.5000\n",
      "Epoch: 012, Loss: 0.6950, Train Acc: 0.5309, Loss: 0.7695, Test Acc: 0.5000\n",
      "Epoch: 013, Loss: 0.6936, Train Acc: 0.5309, Loss: 0.7689, Test Acc: 0.5000\n",
      "Epoch: 014, Loss: 0.6945, Train Acc: 0.5309, Loss: 0.7777, Test Acc: 0.5000\n",
      "Epoch: 015, Loss: 0.6935, Train Acc: 0.5309, Loss: 0.7778, Test Acc: 0.5000\n",
      "Epoch: 016, Loss: 0.6940, Train Acc: 0.5309, Loss: 0.7862, Test Acc: 0.5000\n",
      "Epoch: 017, Loss: 0.6934, Train Acc: 0.5309, Loss: 0.7854, Test Acc: 0.5000\n",
      "Epoch: 018, Loss: 0.6937, Train Acc: 0.5309, Loss: 0.7928, Test Acc: 0.5000\n",
      "Epoch: 019, Loss: 0.6933, Train Acc: 0.5309, Loss: 0.7935, Test Acc: 0.5000\n",
      "Epoch: 020, Loss: 0.6934, Train Acc: 0.5309, Loss: 0.8001, Test Acc: 0.5000\n",
      "Epoch: 021, Loss: 0.6933, Train Acc: 0.5309, Loss: 0.8029, Test Acc: 0.5000\n",
      "Epoch: 022, Loss: 0.6933, Train Acc: 0.5309, Loss: 0.8064, Test Acc: 0.5000\n",
      "Epoch: 023, Loss: 0.6931, Train Acc: 0.5309, Loss: 0.8071, Test Acc: 0.5000\n",
      "Epoch: 024, Loss: 0.6930, Train Acc: 0.5309, Loss: 0.8117, Test Acc: 0.5000\n",
      "Epoch: 025, Loss: 0.6929, Train Acc: 0.5489, Loss: 0.8138, Test Acc: 0.5000\n",
      "Epoch: 026, Loss: 0.6928, Train Acc: 0.5489, Loss: 0.8174, Test Acc: 0.5000\n",
      "Epoch: 027, Loss: 0.6927, Train Acc: 0.5489, Loss: 0.8197, Test Acc: 0.5000\n",
      "Epoch: 028, Loss: 0.6926, Train Acc: 0.5364, Loss: 0.8220, Test Acc: 0.5000\n",
      "Epoch: 029, Loss: 0.6925, Train Acc: 0.5364, Loss: 0.8252, Test Acc: 0.5000\n",
      "Epoch: 030, Loss: 0.6924, Train Acc: 0.5364, Loss: 0.8278, Test Acc: 0.5000\n",
      "Epoch: 031, Loss: 0.6922, Train Acc: 0.5482, Loss: 0.8298, Test Acc: 0.5000\n",
      "Epoch: 032, Loss: 0.6922, Train Acc: 0.5482, Loss: 0.8316, Test Acc: 0.5000\n",
      "Epoch: 033, Loss: 0.6921, Train Acc: 0.5482, Loss: 0.8345, Test Acc: 0.4750\n",
      "Epoch: 034, Loss: 0.6920, Train Acc: 0.5482, Loss: 0.8370, Test Acc: 0.4750\n",
      "Epoch: 035, Loss: 0.6919, Train Acc: 0.5482, Loss: 0.8383, Test Acc: 0.4750\n",
      "Epoch: 036, Loss: 0.6917, Train Acc: 0.5482, Loss: 0.8404, Test Acc: 0.4750\n",
      "Epoch: 037, Loss: 0.6916, Train Acc: 0.5482, Loss: 0.8417, Test Acc: 0.4750\n",
      "Epoch: 038, Loss: 0.6914, Train Acc: 0.5482, Loss: 0.8438, Test Acc: 0.4750\n",
      "Epoch: 039, Loss: 0.6913, Train Acc: 0.5599, Loss: 0.8443, Test Acc: 0.4750\n",
      "Epoch: 040, Loss: 0.6912, Train Acc: 0.5599, Loss: 0.8470, Test Acc: 0.4750\n",
      "Epoch: 041, Loss: 0.6910, Train Acc: 0.5544, Loss: 0.8475, Test Acc: 0.4750\n",
      "Epoch: 042, Loss: 0.6909, Train Acc: 0.5662, Loss: 0.8513, Test Acc: 0.4750\n",
      "Epoch: 043, Loss: 0.6908, Train Acc: 0.5544, Loss: 0.8529, Test Acc: 0.4750\n",
      "Epoch: 044, Loss: 0.6907, Train Acc: 0.5544, Loss: 0.8544, Test Acc: 0.4750\n",
      "Epoch: 045, Loss: 0.6905, Train Acc: 0.5482, Loss: 0.8572, Test Acc: 0.4750\n",
      "Epoch: 046, Loss: 0.6904, Train Acc: 0.5482, Loss: 0.8588, Test Acc: 0.4750\n",
      "Epoch: 047, Loss: 0.6902, Train Acc: 0.5599, Loss: 0.8583, Test Acc: 0.4500\n",
      "Epoch: 048, Loss: 0.6900, Train Acc: 0.5482, Loss: 0.8611, Test Acc: 0.4500\n",
      "Epoch: 049, Loss: 0.6898, Train Acc: 0.5599, Loss: 0.8609, Test Acc: 0.4500\n",
      "Epoch: 050, Loss: 0.6896, Train Acc: 0.5717, Loss: 0.8624, Test Acc: 0.4500\n",
      "Epoch: 051, Loss: 0.6895, Train Acc: 0.5654, Loss: 0.8642, Test Acc: 0.4500\n",
      "Epoch: 052, Loss: 0.6892, Train Acc: 0.5592, Loss: 0.8653, Test Acc: 0.4500\n",
      "Epoch: 053, Loss: 0.6891, Train Acc: 0.5592, Loss: 0.8690, Test Acc: 0.4500\n",
      "Epoch: 054, Loss: 0.6889, Train Acc: 0.5529, Loss: 0.8685, Test Acc: 0.4500\n",
      "Epoch: 055, Loss: 0.6887, Train Acc: 0.5592, Loss: 0.8729, Test Acc: 0.4500\n",
      "Epoch: 056, Loss: 0.6885, Train Acc: 0.5529, Loss: 0.8727, Test Acc: 0.4500\n",
      "Epoch: 057, Loss: 0.6883, Train Acc: 0.5529, Loss: 0.8760, Test Acc: 0.4500\n",
      "Epoch: 058, Loss: 0.6880, Train Acc: 0.5529, Loss: 0.8762, Test Acc: 0.4500\n",
      "Epoch: 059, Loss: 0.6877, Train Acc: 0.5529, Loss: 0.8778, Test Acc: 0.4500\n",
      "Epoch: 060, Loss: 0.6875, Train Acc: 0.5529, Loss: 0.8801, Test Acc: 0.4500\n",
      "Epoch: 061, Loss: 0.6872, Train Acc: 0.5412, Loss: 0.8801, Test Acc: 0.4500\n",
      "Epoch: 062, Loss: 0.6870, Train Acc: 0.5412, Loss: 0.8825, Test Acc: 0.4500\n",
      "Epoch: 063, Loss: 0.6867, Train Acc: 0.5412, Loss: 0.8845, Test Acc: 0.4500\n",
      "Epoch: 064, Loss: 0.6864, Train Acc: 0.5474, Loss: 0.8849, Test Acc: 0.4500\n",
      "Epoch: 065, Loss: 0.6860, Train Acc: 0.5474, Loss: 0.8908, Test Acc: 0.4500\n",
      "Epoch: 066, Loss: 0.6859, Train Acc: 0.5474, Loss: 0.8964, Test Acc: 0.4500\n",
      "Epoch: 067, Loss: 0.6855, Train Acc: 0.5537, Loss: 0.8926, Test Acc: 0.4500\n",
      "Epoch: 068, Loss: 0.6850, Train Acc: 0.5474, Loss: 0.8908, Test Acc: 0.4500\n",
      "Epoch: 069, Loss: 0.6846, Train Acc: 0.5537, Loss: 0.8939, Test Acc: 0.4500\n",
      "Epoch: 070, Loss: 0.6845, Train Acc: 0.5537, Loss: 0.8992, Test Acc: 0.4500\n",
      "Epoch: 071, Loss: 0.6842, Train Acc: 0.5474, Loss: 0.8969, Test Acc: 0.4500\n",
      "Epoch: 072, Loss: 0.6839, Train Acc: 0.5474, Loss: 0.8971, Test Acc: 0.4500\n",
      "Epoch: 073, Loss: 0.6832, Train Acc: 0.5474, Loss: 0.8921, Test Acc: 0.4500\n",
      "Epoch: 074, Loss: 0.6827, Train Acc: 0.5474, Loss: 0.8926, Test Acc: 0.4500\n",
      "Epoch: 075, Loss: 0.6824, Train Acc: 0.5474, Loss: 0.9012, Test Acc: 0.4500\n",
      "Epoch: 076, Loss: 0.6822, Train Acc: 0.5474, Loss: 0.9040, Test Acc: 0.4500\n",
      "Epoch: 077, Loss: 0.6815, Train Acc: 0.5537, Loss: 0.8953, Test Acc: 0.4500\n",
      "Epoch: 078, Loss: 0.6811, Train Acc: 0.5654, Loss: 0.9010, Test Acc: 0.4500\n",
      "Epoch: 079, Loss: 0.6807, Train Acc: 0.5654, Loss: 0.9024, Test Acc: 0.4500\n",
      "Epoch: 080, Loss: 0.6799, Train Acc: 0.5717, Loss: 0.8988, Test Acc: 0.4500\n",
      "Epoch: 081, Loss: 0.6798, Train Acc: 0.5654, Loss: 0.9023, Test Acc: 0.4750\n",
      "Epoch: 082, Loss: 0.6794, Train Acc: 0.5717, Loss: 0.8977, Test Acc: 0.4500\n",
      "Epoch: 083, Loss: 0.6784, Train Acc: 0.5779, Loss: 0.8975, Test Acc: 0.4750\n",
      "Epoch: 084, Loss: 0.6783, Train Acc: 0.5717, Loss: 0.9058, Test Acc: 0.4750\n",
      "Epoch: 085, Loss: 0.6776, Train Acc: 0.5779, Loss: 0.9035, Test Acc: 0.4500\n",
      "Epoch: 086, Loss: 0.6771, Train Acc: 0.5779, Loss: 0.9131, Test Acc: 0.4750\n",
      "Epoch: 087, Loss: 0.6765, Train Acc: 0.5779, Loss: 0.9096, Test Acc: 0.4750\n",
      "Epoch: 088, Loss: 0.6756, Train Acc: 0.5842, Loss: 0.9163, Test Acc: 0.4750\n",
      "Epoch: 089, Loss: 0.6750, Train Acc: 0.5842, Loss: 0.9213, Test Acc: 0.4750\n",
      "Epoch: 090, Loss: 0.6743, Train Acc: 0.5842, Loss: 0.9285, Test Acc: 0.4750\n",
      "Epoch: 091, Loss: 0.6733, Train Acc: 0.5842, Loss: 0.9341, Test Acc: 0.4750\n",
      "Epoch: 092, Loss: 0.6728, Train Acc: 0.5849, Loss: 0.9290, Test Acc: 0.4500\n",
      "Epoch: 093, Loss: 0.6718, Train Acc: 0.5974, Loss: 0.9194, Test Acc: 0.4500\n",
      "Epoch: 094, Loss: 0.6710, Train Acc: 0.5974, Loss: 0.9168, Test Acc: 0.4500\n",
      "Epoch: 095, Loss: 0.6702, Train Acc: 0.5974, Loss: 0.9132, Test Acc: 0.4750\n",
      "Epoch: 096, Loss: 0.6691, Train Acc: 0.5974, Loss: 0.9011, Test Acc: 0.4750\n",
      "Epoch: 097, Loss: 0.6686, Train Acc: 0.6037, Loss: 0.9019, Test Acc: 0.4750\n",
      "Epoch: 098, Loss: 0.6676, Train Acc: 0.6037, Loss: 0.8961, Test Acc: 0.4750\n",
      "Epoch: 099, Loss: 0.6668, Train Acc: 0.6099, Loss: 0.8900, Test Acc: 0.4750\n",
      "Epoch: 100, Loss: 0.6656, Train Acc: 0.6099, Loss: 0.8838, Test Acc: 0.4750\n",
      "Epoch: 101, Loss: 0.6648, Train Acc: 0.6162, Loss: 0.8849, Test Acc: 0.4750\n",
      "Epoch: 102, Loss: 0.6638, Train Acc: 0.6162, Loss: 0.8731, Test Acc: 0.4750\n",
      "Epoch: 103, Loss: 0.6621, Train Acc: 0.6224, Loss: 0.8753, Test Acc: 0.4750\n",
      "Epoch: 104, Loss: 0.6620, Train Acc: 0.6224, Loss: 0.8823, Test Acc: 0.4750\n",
      "Epoch: 105, Loss: 0.6610, Train Acc: 0.6224, Loss: 0.8752, Test Acc: 0.4500\n",
      "Epoch: 106, Loss: 0.6583, Train Acc: 0.6224, Loss: 0.8718, Test Acc: 0.4250\n",
      "Epoch: 107, Loss: 0.6572, Train Acc: 0.6404, Loss: 0.8594, Test Acc: 0.4250\n",
      "Epoch: 108, Loss: 0.6568, Train Acc: 0.6342, Loss: 0.8632, Test Acc: 0.4250\n",
      "Epoch: 109, Loss: 0.6561, Train Acc: 0.6404, Loss: 0.8559, Test Acc: 0.4250\n",
      "Epoch: 110, Loss: 0.6544, Train Acc: 0.6404, Loss: 0.8576, Test Acc: 0.4250\n",
      "Epoch: 111, Loss: 0.6523, Train Acc: 0.6404, Loss: 0.8511, Test Acc: 0.4750\n",
      "Epoch: 112, Loss: 0.6502, Train Acc: 0.6592, Loss: 0.8320, Test Acc: 0.4750\n",
      "Epoch: 113, Loss: 0.6490, Train Acc: 0.6529, Loss: 0.8443, Test Acc: 0.4750\n",
      "Epoch: 114, Loss: 0.6483, Train Acc: 0.6592, Loss: 0.8486, Test Acc: 0.4750\n",
      "Epoch: 115, Loss: 0.6455, Train Acc: 0.6592, Loss: 0.8736, Test Acc: 0.4750\n",
      "Epoch: 116, Loss: 0.6447, Train Acc: 0.6592, Loss: 0.8548, Test Acc: 0.4500\n",
      "Epoch: 117, Loss: 0.6420, Train Acc: 0.6537, Loss: 0.8606, Test Acc: 0.4750\n",
      "Epoch: 118, Loss: 0.6396, Train Acc: 0.6647, Loss: 0.8489, Test Acc: 0.4500\n",
      "Epoch: 119, Loss: 0.6360, Train Acc: 0.6890, Loss: 0.8112, Test Acc: 0.4750\n",
      "Epoch: 120, Loss: 0.6343, Train Acc: 0.6890, Loss: 0.8137, Test Acc: 0.4500\n",
      "Epoch: 121, Loss: 0.6342, Train Acc: 0.6890, Loss: 0.8085, Test Acc: 0.4500\n",
      "Epoch: 122, Loss: 0.6346, Train Acc: 0.6772, Loss: 0.8396, Test Acc: 0.4750\n",
      "Epoch: 123, Loss: 0.6314, Train Acc: 0.6412, Loss: 0.8868, Test Acc: 0.4750\n",
      "Epoch: 124, Loss: 0.6268, Train Acc: 0.6349, Loss: 0.8633, Test Acc: 0.4500\n",
      "Epoch: 125, Loss: 0.6175, Train Acc: 0.6654, Loss: 0.8713, Test Acc: 0.4750\n",
      "Epoch: 126, Loss: 0.6106, Train Acc: 0.6960, Loss: 0.8085, Test Acc: 0.4750\n",
      "Epoch: 127, Loss: 0.6060, Train Acc: 0.7507, Loss: 0.7318, Test Acc: 0.4500\n",
      "Epoch: 128, Loss: 0.6100, Train Acc: 0.7327, Loss: 0.6900, Test Acc: 0.5500\n",
      "Epoch: 129, Loss: 0.6152, Train Acc: 0.7327, Loss: 0.7396, Test Acc: 0.4500\n",
      "Epoch: 130, Loss: 0.6201, Train Acc: 0.6897, Loss: 0.8469, Test Acc: 0.5000\n",
      "Epoch: 131, Loss: 0.6161, Train Acc: 0.6412, Loss: 0.9795, Test Acc: 0.5250\n",
      "Epoch: 132, Loss: 0.6075, Train Acc: 0.6232, Loss: 0.9877, Test Acc: 0.5000\n",
      "Epoch: 133, Loss: 0.5933, Train Acc: 0.6529, Loss: 0.8807, Test Acc: 0.5000\n",
      "Epoch: 134, Loss: 0.5779, Train Acc: 0.6960, Loss: 0.8014, Test Acc: 0.5000\n",
      "Epoch: 135, Loss: 0.5669, Train Acc: 0.7460, Loss: 0.6640, Test Acc: 0.5500\n",
      "Epoch: 136, Loss: 0.5670, Train Acc: 0.7390, Loss: 0.5891, Test Acc: 0.5750\n",
      "Epoch: 137, Loss: 0.5686, Train Acc: 0.7272, Loss: 0.5447, Test Acc: 0.5000\n",
      "Epoch: 138, Loss: 0.5728, Train Acc: 0.7272, Loss: 0.4963, Test Acc: 0.4750\n",
      "Epoch: 139, Loss: 0.5793, Train Acc: 0.7335, Loss: 0.5497, Test Acc: 0.5000\n",
      "Epoch: 140, Loss: 0.5862, Train Acc: 0.7827, Loss: 0.6364, Test Acc: 0.5250\n",
      "Epoch: 141, Loss: 0.5931, Train Acc: 0.7022, Loss: 0.9029, Test Acc: 0.5500\n",
      "Epoch: 142, Loss: 0.5905, Train Acc: 0.5926, Loss: 1.3047, Test Acc: 0.5000\n",
      "Epoch: 143, Loss: 0.5920, Train Acc: 0.5989, Loss: 0.9789, Test Acc: 0.4750\n",
      "Epoch: 144, Loss: 0.5620, Train Acc: 0.7710, Loss: 0.7027, Test Acc: 0.5250\n",
      "Epoch: 145, Loss: 0.5366, Train Acc: 0.7404, Loss: 0.6599, Test Acc: 0.5250\n",
      "Epoch: 146, Loss: 0.5355, Train Acc: 0.7342, Loss: 0.6021, Test Acc: 0.5250\n",
      "Epoch: 147, Loss: 0.5383, Train Acc: 0.7827, Loss: 0.6728, Test Acc: 0.5500\n",
      "Epoch: 148, Loss: 0.5389, Train Acc: 0.7522, Loss: 0.8337, Test Acc: 0.4500\n",
      "Epoch: 149, Loss: 0.5396, Train Acc: 0.7147, Loss: 0.8848, Test Acc: 0.5750\n",
      "Epoch: 150, Loss: 0.5376, Train Acc: 0.7210, Loss: 1.0617, Test Acc: 0.6000\n",
      "Epoch: 151, Loss: 0.5353, Train Acc: 0.6897, Loss: 1.1513, Test Acc: 0.5750\n",
      "Epoch: 152, Loss: 0.5348, Train Acc: 0.6960, Loss: 1.1758, Test Acc: 0.5500\n",
      "Epoch: 153, Loss: 0.5266, Train Acc: 0.6960, Loss: 1.2635, Test Acc: 0.5500\n",
      "Epoch: 154, Loss: 0.5219, Train Acc: 0.7022, Loss: 1.2509, Test Acc: 0.5750\n",
      "Epoch: 155, Loss: 0.5141, Train Acc: 0.7022, Loss: 1.2814, Test Acc: 0.5750\n",
      "Epoch: 156, Loss: 0.5104, Train Acc: 0.7210, Loss: 1.1789, Test Acc: 0.5750\n",
      "Epoch: 157, Loss: 0.5005, Train Acc: 0.7397, Loss: 1.1265, Test Acc: 0.5500\n",
      "Epoch: 158, Loss: 0.4930, Train Acc: 0.7460, Loss: 1.1520, Test Acc: 0.5500\n",
      "Epoch: 159, Loss: 0.4879, Train Acc: 0.7335, Loss: 1.1925, Test Acc: 0.5500\n",
      "Epoch: 160, Loss: 0.4831, Train Acc: 0.7460, Loss: 1.0386, Test Acc: 0.5500\n",
      "Epoch: 161, Loss: 0.4764, Train Acc: 0.7397, Loss: 1.2604, Test Acc: 0.5250\n",
      "Epoch: 162, Loss: 0.4720, Train Acc: 0.7522, Loss: 1.1287, Test Acc: 0.5250\n",
      "Epoch: 163, Loss: 0.4670, Train Acc: 0.7522, Loss: 1.2464, Test Acc: 0.5250\n",
      "Epoch: 164, Loss: 0.4608, Train Acc: 0.7647, Loss: 1.1328, Test Acc: 0.5250\n",
      "Epoch: 165, Loss: 0.4553, Train Acc: 0.7772, Loss: 1.2022, Test Acc: 0.5250\n",
      "Epoch: 166, Loss: 0.4464, Train Acc: 0.7835, Loss: 1.2037, Test Acc: 0.5250\n",
      "Epoch: 167, Loss: 0.4443, Train Acc: 0.7772, Loss: 1.1912, Test Acc: 0.5250\n",
      "Epoch: 168, Loss: 0.4382, Train Acc: 0.8022, Loss: 1.2631, Test Acc: 0.5250\n",
      "Epoch: 169, Loss: 0.4353, Train Acc: 0.7772, Loss: 1.1592, Test Acc: 0.5000\n",
      "Epoch: 170, Loss: 0.4316, Train Acc: 0.8022, Loss: 1.3891, Test Acc: 0.5250\n",
      "Epoch: 171, Loss: 0.4243, Train Acc: 0.7772, Loss: 1.3023, Test Acc: 0.5000\n",
      "Epoch: 172, Loss: 0.4253, Train Acc: 0.8085, Loss: 1.3208, Test Acc: 0.5000\n",
      "Epoch: 173, Loss: 0.4168, Train Acc: 0.7897, Loss: 1.5637, Test Acc: 0.5250\n",
      "Epoch: 174, Loss: 0.4215, Train Acc: 0.8085, Loss: 1.3411, Test Acc: 0.5000\n",
      "Epoch: 175, Loss: 0.4151, Train Acc: 0.8085, Loss: 1.6950, Test Acc: 0.5250\n",
      "Epoch: 176, Loss: 0.4139, Train Acc: 0.7897, Loss: 1.4725, Test Acc: 0.4750\n",
      "Epoch: 177, Loss: 0.4050, Train Acc: 0.8210, Loss: 1.8400, Test Acc: 0.5000\n",
      "Epoch: 178, Loss: 0.3999, Train Acc: 0.7835, Loss: 1.5247, Test Acc: 0.4750\n",
      "Epoch: 179, Loss: 0.4013, Train Acc: 0.8335, Loss: 1.8023, Test Acc: 0.4750\n",
      "Epoch: 180, Loss: 0.3900, Train Acc: 0.8147, Loss: 1.0488, Test Acc: 0.5000\n",
      "Epoch: 181, Loss: 0.4091, Train Acc: 0.7460, Loss: 2.9853, Test Acc: 0.5250\n",
      "Epoch: 182, Loss: 0.4592, Train Acc: 0.7897, Loss: 0.7199, Test Acc: 0.5500\n",
      "Epoch: 183, Loss: 0.4418, Train Acc: 0.8335, Loss: 1.4872, Test Acc: 0.4750\n",
      "Epoch: 184, Loss: 0.3876, Train Acc: 0.8022, Loss: 1.9013, Test Acc: 0.5000\n",
      "Epoch: 185, Loss: 0.3842, Train Acc: 0.8022, Loss: 1.8635, Test Acc: 0.5000\n",
      "Epoch: 186, Loss: 0.3779, Train Acc: 0.7897, Loss: 2.4454, Test Acc: 0.5250\n",
      "Epoch: 187, Loss: 0.3738, Train Acc: 0.8397, Loss: 1.4409, Test Acc: 0.4750\n",
      "Epoch: 188, Loss: 0.3650, Train Acc: 0.8022, Loss: 1.9617, Test Acc: 0.5000\n",
      "Epoch: 189, Loss: 0.3524, Train Acc: 0.8210, Loss: 2.1665, Test Acc: 0.4750\n",
      "Epoch: 190, Loss: 0.3509, Train Acc: 0.8085, Loss: 2.2841, Test Acc: 0.4750\n",
      "Epoch: 191, Loss: 0.3451, Train Acc: 0.8272, Loss: 2.1894, Test Acc: 0.4750\n",
      "Epoch: 192, Loss: 0.3506, Train Acc: 0.8272, Loss: 2.0633, Test Acc: 0.5250\n",
      "Epoch: 193, Loss: 0.3351, Train Acc: 0.8085, Loss: 2.3867, Test Acc: 0.5250\n",
      "Epoch: 194, Loss: 0.3428, Train Acc: 0.8397, Loss: 2.3226, Test Acc: 0.5250\n",
      "Epoch: 195, Loss: 0.3202, Train Acc: 0.8147, Loss: 2.0509, Test Acc: 0.5000\n",
      "Epoch: 196, Loss: 0.3367, Train Acc: 0.8015, Loss: 2.5605, Test Acc: 0.5000\n",
      "Epoch: 197, Loss: 0.3362, Train Acc: 0.8452, Loss: 2.8336, Test Acc: 0.5000\n",
      "Epoch: 198, Loss: 0.3617, Train Acc: 0.8390, Loss: 1.5180, Test Acc: 0.5250\n",
      "Epoch: 199, Loss: 0.3925, Train Acc: 0.7702, Loss: 3.5218, Test Acc: 0.5500\n",
      "Epoch: 200, Loss: 0.3643, Train Acc: 0.8140, Loss: 3.0080, Test Acc: 0.5250\n",
      "Epoch: 201, Loss: 0.4081, Train Acc: 0.8272, Loss: 0.9168, Test Acc: 0.5000\n",
      "Epoch: 202, Loss: 0.3765, Train Acc: 0.8390, Loss: 3.0853, Test Acc: 0.5750\n",
      "Epoch: 203, Loss: 0.3722, Train Acc: 0.7452, Loss: 3.3279, Test Acc: 0.5500\n",
      "Epoch: 204, Loss: 0.4018, Train Acc: 0.7890, Loss: 4.5511, Test Acc: 0.5750\n",
      "Epoch: 205, Loss: 0.3534, Train Acc: 0.7577, Loss: 3.6465, Test Acc: 0.5250\n",
      "Epoch: 206, Loss: 0.3826, Train Acc: 0.6537, Loss: 5.0712, Test Acc: 0.5750\n",
      "Epoch: 207, Loss: 0.4387, Train Acc: 0.6717, Loss: 4.9505, Test Acc: 0.5750\n",
      "Epoch: 208, Loss: 0.4913, Train Acc: 0.6294, Loss: 4.8438, Test Acc: 0.5500\n",
      "Epoch: 209, Loss: 0.6056, Train Acc: 0.8085, Loss: 2.1515, Test Acc: 0.5250\n",
      "Epoch: 210, Loss: 0.6113, Train Acc: 0.5794, Loss: 0.1146, Test Acc: 0.5750\n",
      "Epoch: 211, Loss: 0.7526, Train Acc: 0.7287, Loss: 1.9149, Test Acc: 0.5250\n",
      "Epoch: 212, Loss: 0.5801, Train Acc: 0.7092, Loss: 4.2609, Test Acc: 0.5500\n",
      "Epoch: 213, Loss: 0.4943, Train Acc: 0.8022, Loss: 0.5030, Test Acc: 0.5500\n",
      "Epoch: 214, Loss: 0.5109, Train Acc: 0.6662, Loss: 0.5144, Test Acc: 0.5000\n",
      "Epoch: 215, Loss: 0.5189, Train Acc: 0.7779, Loss: 4.0428, Test Acc: 0.6000\n",
      "Epoch: 216, Loss: 0.4275, Train Acc: 0.8015, Loss: 1.9048, Test Acc: 0.5750\n",
      "Epoch: 217, Loss: 0.4250, Train Acc: 0.7945, Loss: 0.6117, Test Acc: 0.5500\n",
      "Epoch: 218, Loss: 0.4200, Train Acc: 0.8335, Loss: 2.4047, Test Acc: 0.6000\n",
      "Epoch: 219, Loss: 0.3619, Train Acc: 0.8515, Loss: 2.2873, Test Acc: 0.5750\n",
      "Epoch: 220, Loss: 0.3572, Train Acc: 0.8515, Loss: 1.0756, Test Acc: 0.5000\n",
      "Epoch: 221, Loss: 0.3581, Train Acc: 0.8702, Loss: 1.6065, Test Acc: 0.5250\n",
      "Epoch: 222, Loss: 0.3260, Train Acc: 0.8452, Loss: 2.2488, Test Acc: 0.5250\n",
      "Epoch: 223, Loss: 0.3342, Train Acc: 0.8452, Loss: 1.1987, Test Acc: 0.5250\n",
      "Epoch: 224, Loss: 0.3396, Train Acc: 0.8507, Loss: 0.6755, Test Acc: 0.5000\n",
      "Epoch: 225, Loss: 0.3455, Train Acc: 0.8640, Loss: 2.1156, Test Acc: 0.5500\n",
      "Epoch: 226, Loss: 0.3065, Train Acc: 0.8577, Loss: 2.4783, Test Acc: 0.5500\n",
      "Epoch: 227, Loss: 0.3125, Train Acc: 0.8827, Loss: 1.5503, Test Acc: 0.5000\n",
      "Epoch: 228, Loss: 0.3188, Train Acc: 0.8632, Loss: 1.2907, Test Acc: 0.5000\n",
      "Epoch: 229, Loss: 0.3268, Train Acc: 0.8202, Loss: 0.6952, Test Acc: 0.5000\n",
      "Epoch: 230, Loss: 0.3476, Train Acc: 0.8577, Loss: 1.7787, Test Acc: 0.5250\n",
      "Epoch: 231, Loss: 0.2992, Train Acc: 0.8397, Loss: 3.2239, Test Acc: 0.5500\n",
      "Epoch: 232, Loss: 0.2964, Train Acc: 0.8577, Loss: 2.5745, Test Acc: 0.5250\n",
      "Epoch: 233, Loss: 0.3184, Train Acc: 0.8702, Loss: 1.6234, Test Acc: 0.5500\n",
      "Epoch: 234, Loss: 0.3216, Train Acc: 0.8515, Loss: 1.2752, Test Acc: 0.5000\n",
      "Epoch: 235, Loss: 0.3123, Train Acc: 0.8390, Loss: 1.4157, Test Acc: 0.5000\n",
      "Epoch: 236, Loss: 0.2961, Train Acc: 0.8577, Loss: 3.0199, Test Acc: 0.5500\n",
      "Epoch: 237, Loss: 0.2703, Train Acc: 0.8577, Loss: 2.6741, Test Acc: 0.5250\n",
      "Epoch: 238, Loss: 0.2823, Train Acc: 0.9007, Loss: 2.3350, Test Acc: 0.5500\n",
      "Epoch: 239, Loss: 0.2865, Train Acc: 0.8882, Loss: 1.7339, Test Acc: 0.5000\n",
      "Epoch: 240, Loss: 0.2871, Train Acc: 0.8570, Loss: 1.5481, Test Acc: 0.5000\n",
      "Epoch: 241, Loss: 0.2915, Train Acc: 0.9007, Loss: 1.5812, Test Acc: 0.5000\n",
      "Epoch: 242, Loss: 0.2706, Train Acc: 0.8765, Loss: 3.0831, Test Acc: 0.5250\n",
      "Epoch: 243, Loss: 0.2542, Train Acc: 0.8577, Loss: 3.4190, Test Acc: 0.5500\n",
      "Epoch: 244, Loss: 0.2841, Train Acc: 0.9007, Loss: 2.7770, Test Acc: 0.5250\n",
      "Epoch: 245, Loss: 0.2808, Train Acc: 0.8882, Loss: 1.5948, Test Acc: 0.5000\n",
      "Epoch: 246, Loss: 0.2860, Train Acc: 0.8015, Loss: 0.5598, Test Acc: 0.5500\n",
      "Epoch: 247, Loss: 0.3278, Train Acc: 0.8382, Loss: 1.0225, Test Acc: 0.5500\n",
      "Epoch: 248, Loss: 0.3169, Train Acc: 0.9062, Loss: 2.8837, Test Acc: 0.5750\n",
      "Epoch: 249, Loss: 0.2576, Train Acc: 0.8507, Loss: 4.6105, Test Acc: 0.5750\n",
      "Epoch: 250, Loss: 0.2896, Train Acc: 0.8640, Loss: 3.4576, Test Acc: 0.5000\n",
      "Epoch: 251, Loss: 0.3107, Train Acc: 0.9195, Loss: 1.9434, Test Acc: 0.5500\n",
      "Epoch: 252, Loss: 0.2707, Train Acc: 0.8320, Loss: 0.9941, Test Acc: 0.5500\n",
      "Epoch: 253, Loss: 0.2948, Train Acc: 0.8632, Loss: 1.6095, Test Acc: 0.5500\n",
      "Epoch: 254, Loss: 0.2961, Train Acc: 0.8890, Loss: 3.5627, Test Acc: 0.6250\n",
      "Epoch: 255, Loss: 0.2620, Train Acc: 0.8140, Loss: 4.6507, Test Acc: 0.5250\n",
      "Epoch: 256, Loss: 0.3093, Train Acc: 0.8938, Loss: 3.4173, Test Acc: 0.5250\n",
      "Epoch: 257, Loss: 0.2884, Train Acc: 0.8265, Loss: 1.4554, Test Acc: 0.5000\n",
      "Epoch: 258, Loss: 0.3096, Train Acc: 0.8327, Loss: 1.5815, Test Acc: 0.5500\n",
      "Epoch: 259, Loss: 0.3251, Train Acc: 0.9015, Loss: 3.5070, Test Acc: 0.6000\n",
      "Epoch: 260, Loss: 0.2587, Train Acc: 0.7952, Loss: 4.9335, Test Acc: 0.5250\n",
      "Epoch: 261, Loss: 0.3181, Train Acc: 0.8695, Loss: 4.6208, Test Acc: 0.5750\n",
      "Epoch: 262, Loss: 0.3094, Train Acc: 0.8382, Loss: 1.5261, Test Acc: 0.5250\n",
      "Epoch: 263, Loss: 0.3616, Train Acc: 0.7827, Loss: 2.2544, Test Acc: 0.5000\n",
      "Epoch: 264, Loss: 0.4013, Train Acc: 0.9070, Loss: 2.5375, Test Acc: 0.5750\n",
      "Epoch: 265, Loss: 0.3222, Train Acc: 0.7702, Loss: 6.2218, Test Acc: 0.5250\n",
      "Epoch: 266, Loss: 0.4155, Train Acc: 0.7515, Loss: 4.7006, Test Acc: 0.5250\n",
      "Epoch: 267, Loss: 0.5113, Train Acc: 0.6967, Loss: 0.1892, Test Acc: 0.5500\n",
      "Epoch: 268, Loss: 0.5068, Train Acc: 0.8320, Loss: 2.4464, Test Acc: 0.5000\n",
      "Epoch: 269, Loss: 0.4739, Train Acc: 0.7154, Loss: 4.8493, Test Acc: 0.5500\n",
      "Epoch: 270, Loss: 0.4835, Train Acc: 0.8750, Loss: 0.6031, Test Acc: 0.5500\n",
      "Epoch: 271, Loss: 0.4009, Train Acc: 0.7820, Loss: 0.5758, Test Acc: 0.5250\n",
      "Epoch: 272, Loss: 0.3911, Train Acc: 0.8210, Loss: 4.7535, Test Acc: 0.6250\n",
      "Epoch: 273, Loss: 0.3256, Train Acc: 0.8827, Loss: 1.5171, Test Acc: 0.5500\n",
      "Epoch: 274, Loss: 0.2997, Train Acc: 0.8695, Loss: 1.0597, Test Acc: 0.5000\n",
      "Epoch: 275, Loss: 0.2787, Train Acc: 0.8882, Loss: 3.7378, Test Acc: 0.5750\n",
      "Epoch: 276, Loss: 0.2466, Train Acc: 0.9375, Loss: 2.0543, Test Acc: 0.5250\n",
      "Epoch: 277, Loss: 0.2366, Train Acc: 0.9187, Loss: 1.5040, Test Acc: 0.4750\n",
      "Epoch: 278, Loss: 0.2372, Train Acc: 0.9195, Loss: 3.4095, Test Acc: 0.6000\n",
      "Epoch: 279, Loss: 0.2179, Train Acc: 0.9438, Loss: 2.1678, Test Acc: 0.5750\n",
      "Epoch: 280, Loss: 0.2092, Train Acc: 0.9375, Loss: 1.7218, Test Acc: 0.5500\n",
      "Epoch: 281, Loss: 0.2039, Train Acc: 0.9500, Loss: 2.4307, Test Acc: 0.5250\n",
      "Epoch: 282, Loss: 0.1995, Train Acc: 0.9438, Loss: 2.5791, Test Acc: 0.5250\n",
      "Epoch: 283, Loss: 0.1943, Train Acc: 0.9438, Loss: 2.3833, Test Acc: 0.5250\n",
      "Epoch: 284, Loss: 0.1903, Train Acc: 0.9500, Loss: 2.6136, Test Acc: 0.5250\n",
      "Epoch: 285, Loss: 0.1873, Train Acc: 0.9563, Loss: 2.7211, Test Acc: 0.5250\n",
      "Epoch: 286, Loss: 0.1843, Train Acc: 0.9438, Loss: 2.5915, Test Acc: 0.5250\n",
      "Epoch: 287, Loss: 0.1808, Train Acc: 0.9500, Loss: 2.8324, Test Acc: 0.5250\n",
      "Epoch: 288, Loss: 0.1752, Train Acc: 0.9563, Loss: 2.8214, Test Acc: 0.5250\n",
      "Epoch: 289, Loss: 0.1749, Train Acc: 0.9438, Loss: 2.7105, Test Acc: 0.5250\n",
      "Epoch: 290, Loss: 0.1713, Train Acc: 0.9625, Loss: 2.9726, Test Acc: 0.5250\n",
      "Epoch: 291, Loss: 0.1641, Train Acc: 0.9625, Loss: 2.9290, Test Acc: 0.5500\n",
      "Epoch: 292, Loss: 0.1633, Train Acc: 0.9625, Loss: 3.1105, Test Acc: 0.5500\n",
      "Epoch: 293, Loss: 0.1597, Train Acc: 0.9563, Loss: 3.0060, Test Acc: 0.5500\n",
      "Epoch: 294, Loss: 0.1571, Train Acc: 0.9688, Loss: 3.1798, Test Acc: 0.5500\n",
      "Epoch: 295, Loss: 0.1517, Train Acc: 0.9688, Loss: 3.1691, Test Acc: 0.5500\n",
      "Epoch: 296, Loss: 0.1511, Train Acc: 0.9563, Loss: 2.9261, Test Acc: 0.5750\n",
      "Epoch: 297, Loss: 0.1567, Train Acc: 0.9812, Loss: 3.4357, Test Acc: 0.5750\n",
      "Epoch: 298, Loss: 0.1454, Train Acc: 0.9625, Loss: 3.2609, Test Acc: 0.5250\n",
      "Epoch: 299, Loss: 0.1467, Train Acc: 0.9563, Loss: 3.2118, Test Acc: 0.5750\n",
      "Epoch: 300, Loss: 0.1485, Train Acc: 0.9688, Loss: 3.4431, Test Acc: 0.5500\n",
      "Epoch: 301, Loss: 0.1373, Train Acc: 0.9750, Loss: 3.1090, Test Acc: 0.5750\n",
      "Epoch: 302, Loss: 0.1380, Train Acc: 0.9563, Loss: 3.3829, Test Acc: 0.5750\n",
      "Epoch: 303, Loss: 0.1373, Train Acc: 0.9750, Loss: 3.6037, Test Acc: 0.5500\n",
      "Epoch: 304, Loss: 0.1314, Train Acc: 0.9688, Loss: 3.2169, Test Acc: 0.5500\n",
      "Epoch: 305, Loss: 0.1295, Train Acc: 0.9812, Loss: 3.5872, Test Acc: 0.6000\n",
      "Epoch: 306, Loss: 0.1260, Train Acc: 0.9812, Loss: 3.6845, Test Acc: 0.5500\n",
      "Epoch: 307, Loss: 0.1193, Train Acc: 0.9625, Loss: 3.2876, Test Acc: 0.5750\n",
      "Epoch: 308, Loss: 0.1226, Train Acc: 0.9750, Loss: 3.1172, Test Acc: 0.5500\n",
      "Epoch: 309, Loss: 0.1181, Train Acc: 0.9812, Loss: 3.6022, Test Acc: 0.6000\n",
      "Epoch: 310, Loss: 0.1139, Train Acc: 0.9750, Loss: 3.8589, Test Acc: 0.5750\n",
      "Epoch: 311, Loss: 0.1112, Train Acc: 0.9812, Loss: 3.3742, Test Acc: 0.5500\n",
      "Epoch: 312, Loss: 0.1104, Train Acc: 0.9750, Loss: 3.2980, Test Acc: 0.6000\n",
      "Epoch: 313, Loss: 0.1145, Train Acc: 0.9875, Loss: 3.7741, Test Acc: 0.6000\n",
      "Epoch: 314, Loss: 0.1082, Train Acc: 0.9812, Loss: 4.1193, Test Acc: 0.5750\n",
      "Epoch: 315, Loss: 0.1096, Train Acc: 0.9563, Loss: 4.0036, Test Acc: 0.5500\n",
      "Epoch: 316, Loss: 0.1166, Train Acc: 0.9563, Loss: 4.0855, Test Acc: 0.5500\n",
      "Epoch: 317, Loss: 0.1309, Train Acc: 0.9500, Loss: 4.2529, Test Acc: 0.5250\n",
      "Epoch: 318, Loss: 0.1291, Train Acc: 0.9688, Loss: 3.3991, Test Acc: 0.5250\n",
      "Epoch: 319, Loss: 0.1213, Train Acc: 0.9875, Loss: 3.2947, Test Acc: 0.6000\n",
      "Epoch: 320, Loss: 0.1125, Train Acc: 0.9750, Loss: 3.4487, Test Acc: 0.5500\n",
      "Epoch: 321, Loss: 0.1152, Train Acc: 0.9187, Loss: 1.6381, Test Acc: 0.5250\n",
      "Epoch: 322, Loss: 0.1421, Train Acc: 0.8438, Loss: 1.3555, Test Acc: 0.5250\n",
      "Epoch: 323, Loss: 0.2106, Train Acc: 0.9000, Loss: 1.7808, Test Acc: 0.5250\n",
      "Epoch: 324, Loss: 0.1787, Train Acc: 0.9750, Loss: 4.5545, Test Acc: 0.5750\n",
      "Epoch: 325, Loss: 0.1583, Train Acc: 0.8812, Loss: 4.9788, Test Acc: 0.5000\n",
      "Epoch: 326, Loss: 0.3164, Train Acc: 0.6779, Loss: 8.0439, Test Acc: 0.5250\n",
      "Epoch: 327, Loss: 0.5152, Train Acc: 0.7882, Loss: 6.7341, Test Acc: 0.5250\n",
      "Epoch: 328, Loss: 0.5640, Train Acc: 0.8382, Loss: 2.7695, Test Acc: 0.4250\n",
      "Epoch: 329, Loss: 0.5560, Train Acc: 0.8625, Loss: 4.0908, Test Acc: 0.6000\n",
      "Epoch: 330, Loss: 0.4473, Train Acc: 0.9007, Loss: 8.5400, Test Acc: 0.6000\n",
      "Epoch: 331, Loss: 0.2692, Train Acc: 0.8640, Loss: 4.5750, Test Acc: 0.5250\n",
      "Epoch: 332, Loss: 0.2870, Train Acc: 0.8875, Loss: 1.9943, Test Acc: 0.5250\n",
      "Epoch: 333, Loss: 0.2951, Train Acc: 0.8265, Loss: 1.3648, Test Acc: 0.5500\n",
      "Epoch: 334, Loss: 0.3086, Train Acc: 0.8882, Loss: 2.3654, Test Acc: 0.5750\n",
      "Epoch: 335, Loss: 0.2922, Train Acc: 0.8577, Loss: 4.4489, Test Acc: 0.5500\n",
      "Epoch: 336, Loss: 0.2796, Train Acc: 0.8445, Loss: 6.2140, Test Acc: 0.5500\n",
      "Epoch: 337, Loss: 0.3270, Train Acc: 0.9500, Loss: 2.6169, Test Acc: 0.5500\n",
      "Epoch: 338, Loss: 0.3229, Train Acc: 0.7342, Loss: 2.2797, Test Acc: 0.4500\n",
      "Epoch: 339, Loss: 0.4225, Train Acc: 0.8515, Loss: 6.2779, Test Acc: 0.6000\n",
      "Epoch: 340, Loss: 0.2970, Train Acc: 0.8577, Loss: 3.9984, Test Acc: 0.5250\n",
      "Epoch: 341, Loss: 0.2575, Train Acc: 0.8570, Loss: 2.6279, Test Acc: 0.5000\n",
      "Epoch: 342, Loss: 0.3247, Train Acc: 0.9187, Loss: 4.0564, Test Acc: 0.5000\n",
      "Epoch: 343, Loss: 0.2263, Train Acc: 0.8460, Loss: 5.2407, Test Acc: 0.6000\n",
      "Epoch: 344, Loss: 0.2334, Train Acc: 0.9437, Loss: 3.9969, Test Acc: 0.5500\n",
      "Epoch: 345, Loss: 0.1441, Train Acc: 0.9438, Loss: 3.7267, Test Acc: 0.5500\n",
      "Epoch: 346, Loss: 0.1588, Train Acc: 0.9375, Loss: 5.3278, Test Acc: 0.5750\n",
      "Epoch: 347, Loss: 0.1478, Train Acc: 0.9375, Loss: 3.0800, Test Acc: 0.5250\n",
      "Epoch: 348, Loss: 0.1494, Train Acc: 0.9812, Loss: 5.6589, Test Acc: 0.5750\n",
      "Epoch: 349, Loss: 0.1350, Train Acc: 0.9875, Loss: 3.6256, Test Acc: 0.5750\n",
      "Epoch: 350, Loss: 0.1158, Train Acc: 0.9812, Loss: 3.7570, Test Acc: 0.5750\n",
      "Epoch: 351, Loss: 0.1067, Train Acc: 0.9812, Loss: 4.1672, Test Acc: 0.5750\n",
      "Epoch: 352, Loss: 0.1017, Train Acc: 0.9875, Loss: 4.4907, Test Acc: 0.5750\n",
      "Epoch: 353, Loss: 0.0942, Train Acc: 0.9812, Loss: 3.5318, Test Acc: 0.6000\n",
      "Epoch: 354, Loss: 0.0952, Train Acc: 0.9812, Loss: 5.0167, Test Acc: 0.5500\n",
      "Epoch: 355, Loss: 0.0907, Train Acc: 0.9875, Loss: 4.3826, Test Acc: 0.6000\n",
      "Epoch: 356, Loss: 0.0856, Train Acc: 0.9937, Loss: 4.4019, Test Acc: 0.5750\n",
      "Epoch: 357, Loss: 0.0816, Train Acc: 0.9875, Loss: 4.4061, Test Acc: 0.5750\n",
      "Epoch: 358, Loss: 0.0812, Train Acc: 0.9875, Loss: 5.0158, Test Acc: 0.5750\n",
      "Epoch: 359, Loss: 0.0784, Train Acc: 0.9937, Loss: 4.8715, Test Acc: 0.5750\n",
      "Epoch: 360, Loss: 0.0753, Train Acc: 0.9937, Loss: 4.7146, Test Acc: 0.5750\n",
      "Epoch: 361, Loss: 0.0746, Train Acc: 0.9875, Loss: 4.9629, Test Acc: 0.5750\n",
      "Epoch: 362, Loss: 0.0748, Train Acc: 0.9937, Loss: 4.7833, Test Acc: 0.5750\n",
      "Epoch: 363, Loss: 0.0727, Train Acc: 0.9937, Loss: 4.7256, Test Acc: 0.5750\n",
      "Epoch: 364, Loss: 0.0685, Train Acc: 0.9937, Loss: 4.8418, Test Acc: 0.5750\n",
      "Epoch: 365, Loss: 0.0687, Train Acc: 0.9875, Loss: 5.1669, Test Acc: 0.5750\n",
      "Epoch: 366, Loss: 0.0707, Train Acc: 1.0000, Loss: 5.0002, Test Acc: 0.5750\n",
      "Epoch: 367, Loss: 0.0674, Train Acc: 0.9937, Loss: 4.9474, Test Acc: 0.5750\n",
      "Epoch: 368, Loss: 0.0635, Train Acc: 0.9937, Loss: 4.9216, Test Acc: 0.5750\n",
      "Epoch: 369, Loss: 0.0666, Train Acc: 0.9875, Loss: 5.5964, Test Acc: 0.5750\n",
      "Epoch: 370, Loss: 0.0664, Train Acc: 1.0000, Loss: 4.8678, Test Acc: 0.5750\n",
      "Epoch: 371, Loss: 0.0612, Train Acc: 0.9937, Loss: 4.8499, Test Acc: 0.5750\n",
      "Epoch: 372, Loss: 0.0588, Train Acc: 0.9937, Loss: 5.0791, Test Acc: 0.5750\n",
      "Epoch: 373, Loss: 0.0621, Train Acc: 0.9937, Loss: 5.5038, Test Acc: 0.5750\n",
      "Epoch: 374, Loss: 0.0589, Train Acc: 1.0000, Loss: 5.0072, Test Acc: 0.5750\n",
      "Epoch: 375, Loss: 0.0554, Train Acc: 0.9937, Loss: 5.2079, Test Acc: 0.5750\n",
      "Epoch: 376, Loss: 0.0548, Train Acc: 0.9937, Loss: 5.2451, Test Acc: 0.6000\n",
      "Epoch: 377, Loss: 0.0589, Train Acc: 0.9937, Loss: 5.7866, Test Acc: 0.5750\n",
      "Epoch: 378, Loss: 0.0547, Train Acc: 1.0000, Loss: 5.1825, Test Acc: 0.6000\n",
      "Epoch: 379, Loss: 0.0510, Train Acc: 0.9937, Loss: 5.0822, Test Acc: 0.5750\n",
      "Epoch: 380, Loss: 0.0501, Train Acc: 1.0000, Loss: 5.0549, Test Acc: 0.5750\n",
      "Epoch: 381, Loss: 0.0494, Train Acc: 0.9937, Loss: 5.4960, Test Acc: 0.5750\n",
      "Epoch: 382, Loss: 0.0477, Train Acc: 0.9937, Loss: 5.7418, Test Acc: 0.5750\n",
      "Epoch: 383, Loss: 0.0476, Train Acc: 1.0000, Loss: 5.2834, Test Acc: 0.5750\n",
      "Epoch: 384, Loss: 0.0474, Train Acc: 0.9937, Loss: 5.5055, Test Acc: 0.5750\n",
      "Epoch: 385, Loss: 0.0444, Train Acc: 1.0000, Loss: 5.0914, Test Acc: 0.6000\n",
      "Epoch: 386, Loss: 0.0451, Train Acc: 0.9937, Loss: 5.7066, Test Acc: 0.5750\n",
      "Epoch: 387, Loss: 0.0431, Train Acc: 1.0000, Loss: 5.6110, Test Acc: 0.5750\n",
      "Epoch: 388, Loss: 0.0428, Train Acc: 1.0000, Loss: 5.4054, Test Acc: 0.5750\n",
      "Epoch: 389, Loss: 0.0415, Train Acc: 1.0000, Loss: 5.3213, Test Acc: 0.5750\n",
      "Epoch: 390, Loss: 0.0402, Train Acc: 1.0000, Loss: 5.5218, Test Acc: 0.5750\n",
      "Epoch: 391, Loss: 0.0398, Train Acc: 1.0000, Loss: 5.5364, Test Acc: 0.5750\n",
      "Epoch: 392, Loss: 0.0395, Train Acc: 1.0000, Loss: 5.7219, Test Acc: 0.5750\n",
      "Epoch: 393, Loss: 0.0377, Train Acc: 1.0000, Loss: 5.4052, Test Acc: 0.5750\n",
      "Epoch: 394, Loss: 0.0372, Train Acc: 1.0000, Loss: 5.5075, Test Acc: 0.5750\n",
      "Epoch: 395, Loss: 0.0372, Train Acc: 1.0000, Loss: 5.8027, Test Acc: 0.5750\n",
      "Epoch: 396, Loss: 0.0361, Train Acc: 1.0000, Loss: 5.2189, Test Acc: 0.5750\n",
      "Epoch: 397, Loss: 0.0358, Train Acc: 1.0000, Loss: 5.6481, Test Acc: 0.5750\n",
      "Epoch: 398, Loss: 0.0334, Train Acc: 1.0000, Loss: 5.8224, Test Acc: 0.5750\n",
      "Epoch: 399, Loss: 0.0352, Train Acc: 1.0000, Loss: 5.6630, Test Acc: 0.5750\n",
      "Epoch: 400, Loss: 0.0356, Train Acc: 1.0000, Loss: 5.4658, Test Acc: 0.5750\n",
      "Epoch: 401, Loss: 0.0316, Train Acc: 1.0000, Loss: 5.2717, Test Acc: 0.6000\n",
      "Epoch: 402, Loss: 0.0324, Train Acc: 1.0000, Loss: 5.8242, Test Acc: 0.5750\n",
      "Epoch: 403, Loss: 0.0364, Train Acc: 1.0000, Loss: 6.2527, Test Acc: 0.6000\n",
      "Epoch: 404, Loss: 0.0398, Train Acc: 1.0000, Loss: 5.3544, Test Acc: 0.5750\n",
      "Epoch: 405, Loss: 0.0354, Train Acc: 1.0000, Loss: 4.8239, Test Acc: 0.5750\n",
      "Epoch: 406, Loss: 0.0352, Train Acc: 1.0000, Loss: 5.0281, Test Acc: 0.6000\n",
      "Epoch: 407, Loss: 0.0392, Train Acc: 0.9812, Loss: 6.6097, Test Acc: 0.5500\n",
      "Epoch: 408, Loss: 0.0495, Train Acc: 0.9688, Loss: 6.8131, Test Acc: 0.5500\n",
      "Epoch: 409, Loss: 0.0806, Train Acc: 0.9625, Loss: 6.6547, Test Acc: 0.5250\n",
      "Epoch: 410, Loss: 0.1165, Train Acc: 0.9750, Loss: 7.6722, Test Acc: 0.5500\n",
      "Epoch: 411, Loss: 0.2067, Train Acc: 0.8445, Loss: 7.1274, Test Acc: 0.5500\n",
      "Epoch: 412, Loss: 0.3482, Train Acc: 0.9070, Loss: 5.6286, Test Acc: 0.5250\n",
      "Epoch: 413, Loss: 0.3847, Train Acc: 0.8445, Loss: 4.9079, Test Acc: 0.5500\n",
      "Epoch: 414, Loss: 0.4299, Train Acc: 0.9062, Loss: 4.5906, Test Acc: 0.5750\n",
      "Epoch: 415, Loss: 0.3368, Train Acc: 0.9257, Loss: 7.2569, Test Acc: 0.5250\n",
      "Epoch: 416, Loss: 0.2949, Train Acc: 0.9000, Loss: 7.1153, Test Acc: 0.5000\n",
      "Epoch: 417, Loss: 0.2394, Train Acc: 0.8875, Loss: 4.9851, Test Acc: 0.4750\n",
      "Epoch: 418, Loss: 0.2311, Train Acc: 0.9062, Loss: 8.1257, Test Acc: 0.5500\n",
      "Epoch: 419, Loss: 0.1783, Train Acc: 0.9438, Loss: 3.6904, Test Acc: 0.5500\n",
      "Epoch: 420, Loss: 0.1618, Train Acc: 0.9062, Loss: 2.0691, Test Acc: 0.5250\n",
      "Epoch: 421, Loss: 0.2503, Train Acc: 0.8070, Loss: 4.1803, Test Acc: 0.5000\n",
      "Epoch: 422, Loss: 0.2511, Train Acc: 0.9382, Loss: 6.8754, Test Acc: 0.6250\n",
      "Epoch: 423, Loss: 0.2672, Train Acc: 0.8640, Loss: 11.7244, Test Acc: 0.6000\n",
      "Epoch: 424, Loss: 0.2912, Train Acc: 0.9750, Loss: 3.4163, Test Acc: 0.6000\n",
      "Epoch: 425, Loss: 0.1711, Train Acc: 0.8820, Loss: 1.4052, Test Acc: 0.5500\n",
      "Epoch: 426, Loss: 0.2188, Train Acc: 0.9688, Loss: 7.4697, Test Acc: 0.6000\n",
      "Epoch: 427, Loss: 0.1030, Train Acc: 0.9563, Loss: 6.2409, Test Acc: 0.5250\n",
      "Epoch: 428, Loss: 0.1831, Train Acc: 0.9500, Loss: 7.6035, Test Acc: 0.5500\n",
      "Epoch: 429, Loss: 0.1393, Train Acc: 0.9750, Loss: 6.2401, Test Acc: 0.5500\n",
      "Epoch: 430, Loss: 0.0894, Train Acc: 0.9875, Loss: 6.3698, Test Acc: 0.6000\n",
      "Epoch: 431, Loss: 0.0893, Train Acc: 0.9875, Loss: 5.3448, Test Acc: 0.5750\n",
      "Epoch: 432, Loss: 0.0778, Train Acc: 0.9812, Loss: 5.0121, Test Acc: 0.5500\n",
      "Epoch: 433, Loss: 0.0782, Train Acc: 0.9750, Loss: 6.4991, Test Acc: 0.6000\n",
      "Epoch: 434, Loss: 0.0758, Train Acc: 0.9875, Loss: 8.0360, Test Acc: 0.5750\n",
      "Epoch: 435, Loss: 0.0693, Train Acc: 0.9688, Loss: 6.7203, Test Acc: 0.5500\n",
      "Epoch: 436, Loss: 0.0634, Train Acc: 0.9688, Loss: 7.6332, Test Acc: 0.5250\n",
      "Epoch: 437, Loss: 0.1197, Train Acc: 0.9937, Loss: 6.9697, Test Acc: 0.6000\n",
      "Epoch: 438, Loss: 0.0891, Train Acc: 0.9007, Loss: 2.8838, Test Acc: 0.5750\n",
      "Epoch: 439, Loss: 0.1221, Train Acc: 0.9750, Loss: 8.7413, Test Acc: 0.6500\n",
      "Epoch: 440, Loss: 0.1031, Train Acc: 0.9563, Loss: 8.5150, Test Acc: 0.6250\n",
      "Epoch: 441, Loss: 0.1026, Train Acc: 0.9812, Loss: 7.9075, Test Acc: 0.6000\n",
      "Epoch: 442, Loss: 0.0855, Train Acc: 0.9375, Loss: 3.8294, Test Acc: 0.5500\n",
      "Epoch: 443, Loss: 0.0922, Train Acc: 0.9688, Loss: 9.2664, Test Acc: 0.6250\n",
      "Epoch: 444, Loss: 0.0972, Train Acc: 0.9875, Loss: 5.5117, Test Acc: 0.5750\n",
      "Epoch: 445, Loss: 0.0735, Train Acc: 0.9875, Loss: 5.8929, Test Acc: 0.5750\n",
      "Epoch: 446, Loss: 0.0774, Train Acc: 0.9937, Loss: 6.7548, Test Acc: 0.5750\n",
      "Epoch: 447, Loss: 0.0431, Train Acc: 0.9937, Loss: 5.2838, Test Acc: 0.5500\n",
      "Epoch: 448, Loss: 0.0542, Train Acc: 0.9750, Loss: 5.9803, Test Acc: 0.5500\n",
      "Epoch: 449, Loss: 0.0996, Train Acc: 0.9875, Loss: 6.7452, Test Acc: 0.5250\n",
      "Epoch: 450, Loss: 0.0625, Train Acc: 0.9375, Loss: 4.2862, Test Acc: 0.5750\n",
      "Epoch: 451, Loss: 0.1010, Train Acc: 0.9875, Loss: 6.6288, Test Acc: 0.6250\n",
      "Epoch: 452, Loss: 0.1146, Train Acc: 0.9625, Loss: 7.3555, Test Acc: 0.5500\n",
      "Epoch: 453, Loss: 0.1180, Train Acc: 0.9812, Loss: 6.2458, Test Acc: 0.6000\n",
      "Epoch: 454, Loss: 0.0831, Train Acc: 0.9438, Loss: 3.5994, Test Acc: 0.5500\n",
      "Epoch: 455, Loss: 0.0952, Train Acc: 0.9750, Loss: 10.3048, Test Acc: 0.6500\n",
      "Epoch: 456, Loss: 0.0895, Train Acc: 0.9875, Loss: 5.0908, Test Acc: 0.5500\n",
      "Epoch: 457, Loss: 0.0611, Train Acc: 0.9875, Loss: 5.8708, Test Acc: 0.5500\n",
      "Epoch: 458, Loss: 0.0730, Train Acc: 1.0000, Loss: 6.0675, Test Acc: 0.5750\n",
      "Epoch: 459, Loss: 0.0528, Train Acc: 0.9688, Loss: 3.1248, Test Acc: 0.6250\n",
      "Epoch: 460, Loss: 0.0591, Train Acc: 0.9875, Loss: 6.2331, Test Acc: 0.5500\n",
      "Epoch: 461, Loss: 0.0566, Train Acc: 0.9937, Loss: 7.1569, Test Acc: 0.5750\n",
      "Epoch: 462, Loss: 0.0371, Train Acc: 1.0000, Loss: 6.8304, Test Acc: 0.5750\n",
      "Epoch: 463, Loss: 0.0381, Train Acc: 0.9937, Loss: 8.5353, Test Acc: 0.6000\n",
      "Epoch: 464, Loss: 0.0411, Train Acc: 1.0000, Loss: 5.9510, Test Acc: 0.5750\n",
      "Epoch: 465, Loss: 0.0336, Train Acc: 1.0000, Loss: 6.2953, Test Acc: 0.5500\n",
      "Epoch: 466, Loss: 0.0364, Train Acc: 0.9875, Loss: 5.6418, Test Acc: 0.5750\n",
      "Epoch: 467, Loss: 0.0435, Train Acc: 0.9688, Loss: 5.5697, Test Acc: 0.5500\n",
      "Epoch: 468, Loss: 0.0860, Train Acc: 1.0000, Loss: 8.7569, Test Acc: 0.5750\n",
      "Epoch: 469, Loss: 0.0641, Train Acc: 0.8812, Loss: 3.8803, Test Acc: 0.5750\n",
      "Epoch: 470, Loss: 0.1041, Train Acc: 0.9812, Loss: 7.6439, Test Acc: 0.6500\n",
      "Epoch: 471, Loss: 0.0931, Train Acc: 0.9320, Loss: 8.2359, Test Acc: 0.5250\n",
      "Epoch: 472, Loss: 0.1250, Train Acc: 0.9875, Loss: 5.9958, Test Acc: 0.6000\n",
      "Epoch: 473, Loss: 0.0691, Train Acc: 0.9132, Loss: 2.6859, Test Acc: 0.6000\n",
      "Epoch: 474, Loss: 0.0829, Train Acc: 0.9875, Loss: 9.4819, Test Acc: 0.6500\n",
      "Epoch: 475, Loss: 0.0515, Train Acc: 0.9875, Loss: 6.0330, Test Acc: 0.5500\n",
      "Epoch: 476, Loss: 0.0465, Train Acc: 0.9937, Loss: 5.9315, Test Acc: 0.5750\n",
      "Epoch: 477, Loss: 0.0499, Train Acc: 0.9937, Loss: 6.4073, Test Acc: 0.5750\n",
      "Epoch: 478, Loss: 0.0526, Train Acc: 1.0000, Loss: 10.0368, Test Acc: 0.6000\n",
      "Epoch: 479, Loss: 0.0541, Train Acc: 0.9812, Loss: 4.0942, Test Acc: 0.6000\n",
      "Epoch: 480, Loss: 0.0533, Train Acc: 0.9688, Loss: 5.6850, Test Acc: 0.5500\n",
      "Epoch: 481, Loss: 0.1009, Train Acc: 0.9812, Loss: 8.2008, Test Acc: 0.5750\n",
      "Epoch: 482, Loss: 0.0839, Train Acc: 0.9375, Loss: 4.1055, Test Acc: 0.5750\n",
      "Epoch: 483, Loss: 0.0704, Train Acc: 0.9625, Loss: 5.6651, Test Acc: 0.5250\n",
      "Epoch: 484, Loss: 0.1194, Train Acc: 0.9750, Loss: 11.5495, Test Acc: 0.6500\n",
      "Epoch: 485, Loss: 0.1407, Train Acc: 0.9563, Loss: 2.8773, Test Acc: 0.6250\n",
      "Epoch: 486, Loss: 0.1699, Train Acc: 0.7397, Loss: 1.0272, Test Acc: 0.5250\n",
      "Epoch: 487, Loss: 0.3955, Train Acc: 0.8687, Loss: 5.1856, Test Acc: 0.5750\n",
      "Epoch: 488, Loss: 0.3929, Train Acc: 0.8562, Loss: 10.4885, Test Acc: 0.5250\n",
      "Epoch: 489, Loss: 0.4738, Train Acc: 0.6897, Loss: 13.6881, Test Acc: 0.5500\n",
      "Epoch: 490, Loss: 1.0920, Train Acc: 0.8577, Loss: 5.0888, Test Acc: 0.5500\n",
      "Epoch: 491, Loss: 0.7526, Train Acc: 0.7537, Loss: 5.9047, Test Acc: 0.5750\n",
      "Epoch: 492, Loss: 0.4621, Train Acc: 0.7585, Loss: 6.6602, Test Acc: 0.5000\n",
      "Epoch: 493, Loss: 0.4263, Train Acc: 0.8015, Loss: 2.8042, Test Acc: 0.5500\n",
      "Epoch: 494, Loss: 0.2905, Train Acc: 0.8882, Loss: 6.8174, Test Acc: 0.5500\n",
      "Epoch: 495, Loss: 0.2352, Train Acc: 0.8820, Loss: 3.1877, Test Acc: 0.5500\n",
      "Epoch: 496, Loss: 0.2101, Train Acc: 0.9312, Loss: 5.3843, Test Acc: 0.5250\n",
      "Epoch: 497, Loss: 0.1608, Train Acc: 0.9437, Loss: 4.2686, Test Acc: 0.5500\n",
      "Epoch: 498, Loss: 0.1318, Train Acc: 0.9688, Loss: 5.9654, Test Acc: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 499, Loss: 0.1334, Train Acc: 0.9688, Loss: 4.8861, Test Acc: 0.5750\n",
      "Test accuracy: 0.5330000001192093\n",
      "Test stv: 0.08127730293179398\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "test_avg = []\n",
    "for train_index, test_index in kf.split(total_data, Y):\n",
    "    train_dataset=[]\n",
    "    test_dataset=[]\n",
    "    print(\"TRAIN: \", train_index, \"TEST:\", test_index)\n",
    "    for i in train_index:\n",
    "        train_dataset.append(total_data[i])\n",
    "    for i in test_index:\n",
    "        test_dataset.append(total_data[i])\n",
    "\n",
    "    print(len(train_dataset))\n",
    "    print(len(test_dataset))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = Net(dim_h=32)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.6)\n",
    "    # optimizer = torch.optim.Adadelta(model.parameters(), lr=0.7)\n",
    "    train_epoch=[]\n",
    "    test_epoch=[]\n",
    "    epoch = 1\n",
    "    train_acc=0\n",
    "    while epoch < 500:\n",
    "        loss = train(epoch, model)\n",
    "        loss2, train_acc = test(train_loader, model)\n",
    "        loss3, test_acc = test(test_loader, model)\n",
    "        train_epoch.append(train_acc)\n",
    "        test_epoch.append(test_acc)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "            f'Train Acc: {train_acc:.4f}, Loss: {loss3:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        epoch +=1\n",
    "\n",
    "    plt.plot(train_epoch, color=\"red\")\n",
    "    plt.plot(test_epoch, color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    test_avg.append(test_acc)\n",
    "\n",
    "print('Test accuracy: '+ str(np.array(test_avg).mean()))\n",
    "print('Test stv: '+ str(np.array(test_avg).std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb15f1e0f376981e7b6e1fc44ae8b8146823f10f258bcd6e448b0230b889fc06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
