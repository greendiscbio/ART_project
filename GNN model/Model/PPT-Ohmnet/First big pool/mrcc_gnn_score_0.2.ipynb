{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cpu.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gene matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APAF1</th>\n",
       "      <th>ARID1A</th>\n",
       "      <th>ATM</th>\n",
       "      <th>BAP1</th>\n",
       "      <th>CASP2</th>\n",
       "      <th>CRADD</th>\n",
       "      <th>CRYAB</th>\n",
       "      <th>DNMT1</th>\n",
       "      <th>DNMT3A</th>\n",
       "      <th>EPAS1</th>\n",
       "      <th>...</th>\n",
       "      <th>RNF139</th>\n",
       "      <th>SETD2</th>\n",
       "      <th>SLC2A1</th>\n",
       "      <th>SOD2</th>\n",
       "      <th>TGM2</th>\n",
       "      <th>TP53</th>\n",
       "      <th>TSC1</th>\n",
       "      <th>TSC2</th>\n",
       "      <th>VEGFA</th>\n",
       "      <th>VHL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.668769</td>\n",
       "      <td>33.848026</td>\n",
       "      <td>35.942429</td>\n",
       "      <td>33.677294</td>\n",
       "      <td>33.689015</td>\n",
       "      <td>34.20040</td>\n",
       "      <td>39.95791</td>\n",
       "      <td>35.15140</td>\n",
       "      <td>31.75476</td>\n",
       "      <td>37.95811</td>\n",
       "      <td>...</td>\n",
       "      <td>32.46554</td>\n",
       "      <td>32.58565</td>\n",
       "      <td>33.38586</td>\n",
       "      <td>38.67433</td>\n",
       "      <td>38.50142</td>\n",
       "      <td>33.83518</td>\n",
       "      <td>32.93402</td>\n",
       "      <td>34.93520</td>\n",
       "      <td>37.79678</td>\n",
       "      <td>32.30615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.337493</td>\n",
       "      <td>33.843513</td>\n",
       "      <td>35.988225</td>\n",
       "      <td>32.643149</td>\n",
       "      <td>33.946812</td>\n",
       "      <td>33.33414</td>\n",
       "      <td>39.76850</td>\n",
       "      <td>34.78386</td>\n",
       "      <td>32.83981</td>\n",
       "      <td>38.83281</td>\n",
       "      <td>...</td>\n",
       "      <td>32.27190</td>\n",
       "      <td>33.19915</td>\n",
       "      <td>33.69538</td>\n",
       "      <td>38.64559</td>\n",
       "      <td>34.33752</td>\n",
       "      <td>34.44810</td>\n",
       "      <td>33.16630</td>\n",
       "      <td>35.08304</td>\n",
       "      <td>40.09193</td>\n",
       "      <td>32.19988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.818198</td>\n",
       "      <td>33.516005</td>\n",
       "      <td>36.193587</td>\n",
       "      <td>32.368866</td>\n",
       "      <td>33.752815</td>\n",
       "      <td>31.15063</td>\n",
       "      <td>40.93124</td>\n",
       "      <td>34.97395</td>\n",
       "      <td>32.26242</td>\n",
       "      <td>37.19345</td>\n",
       "      <td>...</td>\n",
       "      <td>32.55514</td>\n",
       "      <td>32.84628</td>\n",
       "      <td>36.23588</td>\n",
       "      <td>40.50559</td>\n",
       "      <td>35.50178</td>\n",
       "      <td>35.41980</td>\n",
       "      <td>33.63282</td>\n",
       "      <td>34.79244</td>\n",
       "      <td>38.22308</td>\n",
       "      <td>31.49147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.601293</td>\n",
       "      <td>34.197698</td>\n",
       "      <td>36.578348</td>\n",
       "      <td>31.895400</td>\n",
       "      <td>34.064332</td>\n",
       "      <td>32.93107</td>\n",
       "      <td>40.02236</td>\n",
       "      <td>35.07183</td>\n",
       "      <td>32.64948</td>\n",
       "      <td>39.46713</td>\n",
       "      <td>...</td>\n",
       "      <td>33.19823</td>\n",
       "      <td>33.68316</td>\n",
       "      <td>34.41938</td>\n",
       "      <td>38.99231</td>\n",
       "      <td>35.77236</td>\n",
       "      <td>34.18862</td>\n",
       "      <td>32.88250</td>\n",
       "      <td>35.02014</td>\n",
       "      <td>39.94908</td>\n",
       "      <td>32.11538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.593121</td>\n",
       "      <td>33.351460</td>\n",
       "      <td>36.807497</td>\n",
       "      <td>33.968348</td>\n",
       "      <td>33.501184</td>\n",
       "      <td>33.49363</td>\n",
       "      <td>38.83921</td>\n",
       "      <td>35.23627</td>\n",
       "      <td>33.75096</td>\n",
       "      <td>38.49884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.89813</td>\n",
       "      <td>34.63036</td>\n",
       "      <td>34.59911</td>\n",
       "      <td>38.41437</td>\n",
       "      <td>33.47112</td>\n",
       "      <td>34.91241</td>\n",
       "      <td>33.44515</td>\n",
       "      <td>35.01310</td>\n",
       "      <td>39.31564</td>\n",
       "      <td>33.33646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>32.603769</td>\n",
       "      <td>34.133940</td>\n",
       "      <td>35.318612</td>\n",
       "      <td>33.843872</td>\n",
       "      <td>33.840555</td>\n",
       "      <td>31.12858</td>\n",
       "      <td>37.79607</td>\n",
       "      <td>35.72355</td>\n",
       "      <td>32.20131</td>\n",
       "      <td>39.13826</td>\n",
       "      <td>...</td>\n",
       "      <td>32.12573</td>\n",
       "      <td>33.34867</td>\n",
       "      <td>36.50807</td>\n",
       "      <td>35.15898</td>\n",
       "      <td>34.57504</td>\n",
       "      <td>35.39631</td>\n",
       "      <td>32.93248</td>\n",
       "      <td>35.12781</td>\n",
       "      <td>40.48054</td>\n",
       "      <td>31.79913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>33.619701</td>\n",
       "      <td>32.373330</td>\n",
       "      <td>35.771711</td>\n",
       "      <td>32.519967</td>\n",
       "      <td>31.854546</td>\n",
       "      <td>34.93690</td>\n",
       "      <td>40.18790</td>\n",
       "      <td>35.48691</td>\n",
       "      <td>31.85427</td>\n",
       "      <td>35.86338</td>\n",
       "      <td>...</td>\n",
       "      <td>34.27276</td>\n",
       "      <td>32.16275</td>\n",
       "      <td>33.97705</td>\n",
       "      <td>38.85295</td>\n",
       "      <td>32.38354</td>\n",
       "      <td>32.04003</td>\n",
       "      <td>32.62658</td>\n",
       "      <td>33.78873</td>\n",
       "      <td>37.41392</td>\n",
       "      <td>31.66344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>33.316811</td>\n",
       "      <td>34.118843</td>\n",
       "      <td>36.008091</td>\n",
       "      <td>33.115209</td>\n",
       "      <td>33.551305</td>\n",
       "      <td>32.89828</td>\n",
       "      <td>38.70298</td>\n",
       "      <td>35.74254</td>\n",
       "      <td>32.41718</td>\n",
       "      <td>37.91340</td>\n",
       "      <td>...</td>\n",
       "      <td>32.92305</td>\n",
       "      <td>34.01015</td>\n",
       "      <td>34.85694</td>\n",
       "      <td>37.96021</td>\n",
       "      <td>36.65499</td>\n",
       "      <td>33.34126</td>\n",
       "      <td>32.81059</td>\n",
       "      <td>35.24316</td>\n",
       "      <td>38.72091</td>\n",
       "      <td>32.39461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>33.046782</td>\n",
       "      <td>33.833796</td>\n",
       "      <td>37.008936</td>\n",
       "      <td>32.895151</td>\n",
       "      <td>33.903126</td>\n",
       "      <td>31.87461</td>\n",
       "      <td>38.81342</td>\n",
       "      <td>35.37826</td>\n",
       "      <td>32.71217</td>\n",
       "      <td>37.96870</td>\n",
       "      <td>...</td>\n",
       "      <td>31.87160</td>\n",
       "      <td>33.23246</td>\n",
       "      <td>34.24055</td>\n",
       "      <td>37.24924</td>\n",
       "      <td>36.84744</td>\n",
       "      <td>34.98283</td>\n",
       "      <td>34.04810</td>\n",
       "      <td>35.60526</td>\n",
       "      <td>40.53108</td>\n",
       "      <td>32.34561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>32.170042</td>\n",
       "      <td>33.739764</td>\n",
       "      <td>35.937812</td>\n",
       "      <td>33.404526</td>\n",
       "      <td>34.798860</td>\n",
       "      <td>31.85580</td>\n",
       "      <td>34.50354</td>\n",
       "      <td>36.39602</td>\n",
       "      <td>33.60303</td>\n",
       "      <td>38.75226</td>\n",
       "      <td>...</td>\n",
       "      <td>32.47268</td>\n",
       "      <td>32.81781</td>\n",
       "      <td>35.99620</td>\n",
       "      <td>38.54211</td>\n",
       "      <td>37.23935</td>\n",
       "      <td>33.82151</td>\n",
       "      <td>33.82576</td>\n",
       "      <td>35.13995</td>\n",
       "      <td>40.81516</td>\n",
       "      <td>30.34566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         APAF1     ARID1A        ATM       BAP1      CASP2     CRADD  \\\n",
       "0    32.668769  33.848026  35.942429  33.677294  33.689015  34.20040   \n",
       "1    32.337493  33.843513  35.988225  32.643149  33.946812  33.33414   \n",
       "2    31.818198  33.516005  36.193587  32.368866  33.752815  31.15063   \n",
       "3    32.601293  34.197698  36.578348  31.895400  34.064332  32.93107   \n",
       "4    33.593121  33.351460  36.807497  33.968348  33.501184  33.49363   \n",
       "..         ...        ...        ...        ...        ...       ...   \n",
       "176  32.603769  34.133940  35.318612  33.843872  33.840555  31.12858   \n",
       "177  33.619701  32.373330  35.771711  32.519967  31.854546  34.93690   \n",
       "178  33.316811  34.118843  36.008091  33.115209  33.551305  32.89828   \n",
       "179  33.046782  33.833796  37.008936  32.895151  33.903126  31.87461   \n",
       "180  32.170042  33.739764  35.937812  33.404526  34.798860  31.85580   \n",
       "\n",
       "        CRYAB     DNMT1    DNMT3A     EPAS1  ...    RNF139     SETD2  \\\n",
       "0    39.95791  35.15140  31.75476  37.95811  ...  32.46554  32.58565   \n",
       "1    39.76850  34.78386  32.83981  38.83281  ...  32.27190  33.19915   \n",
       "2    40.93124  34.97395  32.26242  37.19345  ...  32.55514  32.84628   \n",
       "3    40.02236  35.07183  32.64948  39.46713  ...  33.19823  33.68316   \n",
       "4    38.83921  35.23627  33.75096  38.49884  ...  30.89813  34.63036   \n",
       "..        ...       ...       ...       ...  ...       ...       ...   \n",
       "176  37.79607  35.72355  32.20131  39.13826  ...  32.12573  33.34867   \n",
       "177  40.18790  35.48691  31.85427  35.86338  ...  34.27276  32.16275   \n",
       "178  38.70298  35.74254  32.41718  37.91340  ...  32.92305  34.01015   \n",
       "179  38.81342  35.37826  32.71217  37.96870  ...  31.87160  33.23246   \n",
       "180  34.50354  36.39602  33.60303  38.75226  ...  32.47268  32.81781   \n",
       "\n",
       "       SLC2A1      SOD2      TGM2      TP53      TSC1      TSC2     VEGFA  \\\n",
       "0    33.38586  38.67433  38.50142  33.83518  32.93402  34.93520  37.79678   \n",
       "1    33.69538  38.64559  34.33752  34.44810  33.16630  35.08304  40.09193   \n",
       "2    36.23588  40.50559  35.50178  35.41980  33.63282  34.79244  38.22308   \n",
       "3    34.41938  38.99231  35.77236  34.18862  32.88250  35.02014  39.94908   \n",
       "4    34.59911  38.41437  33.47112  34.91241  33.44515  35.01310  39.31564   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "176  36.50807  35.15898  34.57504  35.39631  32.93248  35.12781  40.48054   \n",
       "177  33.97705  38.85295  32.38354  32.04003  32.62658  33.78873  37.41392   \n",
       "178  34.85694  37.96021  36.65499  33.34126  32.81059  35.24316  38.72091   \n",
       "179  34.24055  37.24924  36.84744  34.98283  34.04810  35.60526  40.53108   \n",
       "180  35.99620  38.54211  37.23935  33.82151  33.82576  35.13995  40.81516   \n",
       "\n",
       "          VHL  \n",
       "0    32.30615  \n",
       "1    32.19988  \n",
       "2    31.49147  \n",
       "3    32.11538  \n",
       "4    33.33646  \n",
       "..        ...  \n",
       "176  31.79913  \n",
       "177  31.66344  \n",
       "178  32.39461  \n",
       "179  32.34561  \n",
       "180  30.34566  \n",
       "\n",
       "[181 rows x 41 columns]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes = pd.read_csv('..\\..\\Data\\PPT-Ohmnet\\mRCC_big_pool\\mrcc_protein_matrix_151_genes_41_nodes.csv')\n",
    "Y = genes.Y\n",
    "\n",
    "genes = genes.iloc[:,1:42] \n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APAF1</th>\n",
       "      <th>ARID1A</th>\n",
       "      <th>ATM</th>\n",
       "      <th>BAP1</th>\n",
       "      <th>CASP2</th>\n",
       "      <th>CRADD</th>\n",
       "      <th>CRYAB</th>\n",
       "      <th>DNMT1</th>\n",
       "      <th>DNMT3A</th>\n",
       "      <th>EPAS1</th>\n",
       "      <th>...</th>\n",
       "      <th>RNF139</th>\n",
       "      <th>SETD2</th>\n",
       "      <th>SLC2A1</th>\n",
       "      <th>SOD2</th>\n",
       "      <th>TGM2</th>\n",
       "      <th>TP53</th>\n",
       "      <th>TSC1</th>\n",
       "      <th>TSC2</th>\n",
       "      <th>VEGFA</th>\n",
       "      <th>VHL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.610274</td>\n",
       "      <td>0.474298</td>\n",
       "      <td>0.551095</td>\n",
       "      <td>0.703386</td>\n",
       "      <td>0.622048</td>\n",
       "      <td>0.846161</td>\n",
       "      <td>0.847331</td>\n",
       "      <td>0.386131</td>\n",
       "      <td>0.367530</td>\n",
       "      <td>0.614968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547741</td>\n",
       "      <td>0.361620</td>\n",
       "      <td>0.420160</td>\n",
       "      <td>0.542412</td>\n",
       "      <td>0.945549</td>\n",
       "      <td>0.403803</td>\n",
       "      <td>0.411780</td>\n",
       "      <td>0.408244</td>\n",
       "      <td>0.439826</td>\n",
       "      <td>0.681580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.536117</td>\n",
       "      <td>0.472846</td>\n",
       "      <td>0.561963</td>\n",
       "      <td>0.465055</td>\n",
       "      <td>0.669459</td>\n",
       "      <td>0.704600</td>\n",
       "      <td>0.824106</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.617540</td>\n",
       "      <td>0.796869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504091</td>\n",
       "      <td>0.518369</td>\n",
       "      <td>0.458930</td>\n",
       "      <td>0.538144</td>\n",
       "      <td>0.301997</td>\n",
       "      <td>0.538341</td>\n",
       "      <td>0.474109</td>\n",
       "      <td>0.451980</td>\n",
       "      <td>0.760074</td>\n",
       "      <td>0.664154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.419872</td>\n",
       "      <td>0.367512</td>\n",
       "      <td>0.610698</td>\n",
       "      <td>0.401843</td>\n",
       "      <td>0.633782</td>\n",
       "      <td>0.347778</td>\n",
       "      <td>0.966677</td>\n",
       "      <td>0.331608</td>\n",
       "      <td>0.484501</td>\n",
       "      <td>0.455951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567938</td>\n",
       "      <td>0.428211</td>\n",
       "      <td>0.777154</td>\n",
       "      <td>0.814317</td>\n",
       "      <td>0.481939</td>\n",
       "      <td>0.751632</td>\n",
       "      <td>0.599295</td>\n",
       "      <td>0.366011</td>\n",
       "      <td>0.499309</td>\n",
       "      <td>0.547991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.595169</td>\n",
       "      <td>0.586761</td>\n",
       "      <td>0.702007</td>\n",
       "      <td>0.292727</td>\n",
       "      <td>0.691071</td>\n",
       "      <td>0.638732</td>\n",
       "      <td>0.855233</td>\n",
       "      <td>0.361682</td>\n",
       "      <td>0.573685</td>\n",
       "      <td>0.928781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712900</td>\n",
       "      <td>0.642034</td>\n",
       "      <td>0.549619</td>\n",
       "      <td>0.589625</td>\n",
       "      <td>0.523759</td>\n",
       "      <td>0.481384</td>\n",
       "      <td>0.397955</td>\n",
       "      <td>0.433372</td>\n",
       "      <td>0.740142</td>\n",
       "      <td>0.650298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.817191</td>\n",
       "      <td>0.314590</td>\n",
       "      <td>0.756387</td>\n",
       "      <td>0.770463</td>\n",
       "      <td>0.587505</td>\n",
       "      <td>0.730663</td>\n",
       "      <td>0.710160</td>\n",
       "      <td>0.412209</td>\n",
       "      <td>0.827480</td>\n",
       "      <td>0.727417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194423</td>\n",
       "      <td>0.884044</td>\n",
       "      <td>0.572132</td>\n",
       "      <td>0.503813</td>\n",
       "      <td>0.168091</td>\n",
       "      <td>0.640258</td>\n",
       "      <td>0.548936</td>\n",
       "      <td>0.431290</td>\n",
       "      <td>0.651756</td>\n",
       "      <td>0.850528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.595723</td>\n",
       "      <td>0.566255</td>\n",
       "      <td>0.403055</td>\n",
       "      <td>0.741776</td>\n",
       "      <td>0.649917</td>\n",
       "      <td>0.344175</td>\n",
       "      <td>0.582254</td>\n",
       "      <td>0.561932</td>\n",
       "      <td>0.470421</td>\n",
       "      <td>0.860390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471142</td>\n",
       "      <td>0.556572</td>\n",
       "      <td>0.811248</td>\n",
       "      <td>0.020453</td>\n",
       "      <td>0.338707</td>\n",
       "      <td>0.746476</td>\n",
       "      <td>0.411366</td>\n",
       "      <td>0.465225</td>\n",
       "      <td>0.814298</td>\n",
       "      <td>0.598440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.823141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510581</td>\n",
       "      <td>0.436667</td>\n",
       "      <td>0.284680</td>\n",
       "      <td>0.966518</td>\n",
       "      <td>0.875531</td>\n",
       "      <td>0.489221</td>\n",
       "      <td>0.390458</td>\n",
       "      <td>0.179353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955115</td>\n",
       "      <td>0.253569</td>\n",
       "      <td>0.494212</td>\n",
       "      <td>0.568933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.329281</td>\n",
       "      <td>0.069080</td>\n",
       "      <td>0.386405</td>\n",
       "      <td>0.576190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.755339</td>\n",
       "      <td>0.561399</td>\n",
       "      <td>0.566677</td>\n",
       "      <td>0.573847</td>\n",
       "      <td>0.596723</td>\n",
       "      <td>0.633373</td>\n",
       "      <td>0.693456</td>\n",
       "      <td>0.567767</td>\n",
       "      <td>0.520160</td>\n",
       "      <td>0.605671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650870</td>\n",
       "      <td>0.725580</td>\n",
       "      <td>0.604427</td>\n",
       "      <td>0.436379</td>\n",
       "      <td>0.660174</td>\n",
       "      <td>0.295386</td>\n",
       "      <td>0.378658</td>\n",
       "      <td>0.499349</td>\n",
       "      <td>0.568772</td>\n",
       "      <td>0.696085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.694893</td>\n",
       "      <td>0.469721</td>\n",
       "      <td>0.804191</td>\n",
       "      <td>0.523132</td>\n",
       "      <td>0.661425</td>\n",
       "      <td>0.466089</td>\n",
       "      <td>0.706998</td>\n",
       "      <td>0.455837</td>\n",
       "      <td>0.588130</td>\n",
       "      <td>0.617171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413858</td>\n",
       "      <td>0.526880</td>\n",
       "      <td>0.527218</td>\n",
       "      <td>0.330815</td>\n",
       "      <td>0.689918</td>\n",
       "      <td>0.655716</td>\n",
       "      <td>0.710731</td>\n",
       "      <td>0.606470</td>\n",
       "      <td>0.821350</td>\n",
       "      <td>0.688050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.498633</td>\n",
       "      <td>0.439478</td>\n",
       "      <td>0.549999</td>\n",
       "      <td>0.640524</td>\n",
       "      <td>0.826155</td>\n",
       "      <td>0.463015</td>\n",
       "      <td>0.178537</td>\n",
       "      <td>0.768557</td>\n",
       "      <td>0.793395</td>\n",
       "      <td>0.780118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549350</td>\n",
       "      <td>0.420937</td>\n",
       "      <td>0.747131</td>\n",
       "      <td>0.522780</td>\n",
       "      <td>0.750490</td>\n",
       "      <td>0.400802</td>\n",
       "      <td>0.651068</td>\n",
       "      <td>0.468816</td>\n",
       "      <td>0.860988</td>\n",
       "      <td>0.360103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        APAF1    ARID1A       ATM      BAP1     CASP2     CRADD     CRYAB  \\\n",
       "0    0.610274  0.474298  0.551095  0.703386  0.622048  0.846161  0.847331   \n",
       "1    0.536117  0.472846  0.561963  0.465055  0.669459  0.704600  0.824106   \n",
       "2    0.419872  0.367512  0.610698  0.401843  0.633782  0.347778  0.966677   \n",
       "3    0.595169  0.586761  0.702007  0.292727  0.691071  0.638732  0.855233   \n",
       "4    0.817191  0.314590  0.756387  0.770463  0.587505  0.730663  0.710160   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "176  0.595723  0.566255  0.403055  0.741776  0.649917  0.344175  0.582254   \n",
       "177  0.823141  0.000000  0.510581  0.436667  0.284680  0.966518  0.875531   \n",
       "178  0.755339  0.561399  0.566677  0.573847  0.596723  0.633373  0.693456   \n",
       "179  0.694893  0.469721  0.804191  0.523132  0.661425  0.466089  0.706998   \n",
       "180  0.498633  0.439478  0.549999  0.640524  0.826155  0.463015  0.178537   \n",
       "\n",
       "        DNMT1    DNMT3A     EPAS1  ...    RNF139     SETD2    SLC2A1  \\\n",
       "0    0.386131  0.367530  0.614968  ...  0.547741  0.361620  0.420160   \n",
       "1    0.273200  0.617540  0.796869  ...  0.504091  0.518369  0.458930   \n",
       "2    0.331608  0.484501  0.455951  ...  0.567938  0.428211  0.777154   \n",
       "3    0.361682  0.573685  0.928781  ...  0.712900  0.642034  0.549619   \n",
       "4    0.412209  0.827480  0.727417  ...  0.194423  0.884044  0.572132   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "176  0.561932  0.470421  0.860390  ...  0.471142  0.556572  0.811248   \n",
       "177  0.489221  0.390458  0.179353  ...  0.955115  0.253569  0.494212   \n",
       "178  0.567767  0.520160  0.605671  ...  0.650870  0.725580  0.604427   \n",
       "179  0.455837  0.588130  0.617171  ...  0.413858  0.526880  0.527218   \n",
       "180  0.768557  0.793395  0.780118  ...  0.549350  0.420937  0.747131   \n",
       "\n",
       "         SOD2      TGM2      TP53      TSC1      TSC2     VEGFA       VHL  \n",
       "0    0.542412  0.945549  0.403803  0.411780  0.408244  0.439826  0.681580  \n",
       "1    0.538144  0.301997  0.538341  0.474109  0.451980  0.760074  0.664154  \n",
       "2    0.814317  0.481939  0.751632  0.599295  0.366011  0.499309  0.547991  \n",
       "3    0.589625  0.523759  0.481384  0.397955  0.433372  0.740142  0.650298  \n",
       "4    0.503813  0.168091  0.640258  0.548936  0.431290  0.651756  0.850528  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "176  0.020453  0.338707  0.746476  0.411366  0.465225  0.814298  0.598440  \n",
       "177  0.568933  0.000000  0.009761  0.329281  0.069080  0.386405  0.576190  \n",
       "178  0.436379  0.660174  0.295386  0.378658  0.499349  0.568772  0.696085  \n",
       "179  0.330815  0.689918  0.655716  0.710731  0.606470  0.821350  0.688050  \n",
       "180  0.522780  0.750490  0.400802  0.651068  0.468816  0.860988  0.360103  \n",
       "\n",
       "[181 rows x 41 columns]"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = genes.columns\n",
    "d = scaler.fit_transform(genes)\n",
    "genes = pd.DataFrame(d, columns=names)\n",
    "genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='../../Data/PPT-Ohmnet/mRCC_big_pool/network_edges_mrcc_151_genes_41_nodes.tsv'\n",
    "data = pd.read_csv(path, delimiter='\\t')\n",
    "edge_index1=data[data.columns[1]].to_numpy()\n",
    "edge_index2=data[data.columns[2]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = np.concatenate((edge_index1, edge_index2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HSPD1', 'HSPD1', 'CRYAB', 'CRYAB', 'CRYAB', 'CRYAB', 'CRYAB',\n",
       "       'VHL', 'VHL', 'VHL', 'VHL', 'VHL', 'VHL', 'VHL', 'GSTP1', 'GSTP1',\n",
       "       'TGM2', 'TGM2', 'SETD2', 'ERBB2', 'ERBB2', 'FLT1', 'FLT1', 'FLT4',\n",
       "       'DNMT1', 'DNMT1', 'NDRG1', 'ERN1', 'CRADD', 'NF2', 'NF2', 'PIK3CA',\n",
       "       'MTOR', 'MTOR', 'LRRK2', 'APAF1', 'KDR', 'TSC1', 'RELA', 'RELA',\n",
       "       'RELA', 'RELA', 'ATM', 'MAPK8', 'PTEN', 'PTEN', 'ARID1A', 'PTGS2',\n",
       "       'IL6', 'HSPB1', 'HSPB1', 'TP53', 'HSPA9', 'HSPD1', 'HSPA9', 'PTEN',\n",
       "       'HSPB1', 'VEGFA', 'CRADD', 'TP53', 'CASP2', 'SLC2A1', 'RNF139',\n",
       "       'ATM', 'TGM2', 'TP53', 'SOD2', 'EPAS1', 'TGM2', 'MAPK8', 'RELA',\n",
       "       'PAK1', 'TP53', 'NF2', 'PAK1', 'VEGFA', 'KDR', 'KDR', 'DNMT3A',\n",
       "       'TP53', 'TP53', 'TP53', 'CASP2', 'PAK1', 'TSC1', 'PTEN', 'TP53',\n",
       "       'MAPK8', 'HSPA9', 'TP53', 'VEGFA', 'TSC2', 'IL6', 'TP53', 'ATM',\n",
       "       'HSPA9', 'TP53', 'TP53', 'TP53', 'BAP1', 'TP53', 'TP53', 'IL6R',\n",
       "       'TP53', 'HSPA9', 'HSPA9', 'BAP1', 'LRRK2'], dtype=object)"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(edge_index)\n",
    "len(list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index1 = le.transform(edge_index1)\n",
    "edge_index2 = le.transform(edge_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = [edge_index1]+[edge_index2]\n",
    "edge_index = np.array(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17, 17,  6,  6,  6,  6,  6, 40, 40, 40, 40, 40, 40, 40, 14, 14,\n",
       "        35, 35, 32, 10, 10, 12, 12, 13,  7,  7, 24, 11,  5, 25, 25, 27,\n",
       "        23, 23, 21,  0, 20, 37, 30, 30, 30, 30,  2, 22, 28, 28,  1, 29,\n",
       "        18, 16, 16, 36, 15, 17],\n",
       "       [15, 28, 16, 39,  5, 36,  4, 33, 31,  2, 35, 36, 34,  9, 35, 22,\n",
       "        30, 26, 36, 25, 26, 39, 20, 20,  8, 36, 36, 36,  4, 26, 37, 28,\n",
       "        36, 22, 15, 36, 39, 38, 18, 36,  2, 15, 36, 36, 36,  3, 36, 36,\n",
       "        19, 36, 15, 15,  3, 21]])"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17, 17,  6,  6,  6,  6,  6, 40, 40, 40, 40, 40, 40, 40, 14, 14, 35, 35,\n",
       "         32, 10, 10, 12, 12, 13,  7,  7, 24, 11,  5, 25, 25, 27, 23, 23, 21,  0,\n",
       "         20, 37, 30, 30, 30, 30,  2, 22, 28, 28,  1, 29, 18, 16, 16, 36, 15, 17],\n",
       "        [15, 28, 16, 39,  5, 36,  4, 33, 31,  2, 35, 36, 34,  9, 35, 22, 30, 26,\n",
       "         36, 25, 26, 39, 20, 20,  8, 36, 36, 36,  4, 26, 37, 28, 36, 22, 15, 36,\n",
       "         39, 38, 18, 36,  2, 15, 36, 36, 36,  3, 36, 36, 19, 36, 15, 15,  3, 21]])"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edge_index, dtype=torch.int64)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[41], edge_index=[2, 54], y=[1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_10980/1559691492.py:11: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  x = torch.tensor([b], dtype=torch.long).reshape([-1])\n"
     ]
    }
   ],
   "source": [
    "list_data_0=[]\n",
    "list_data_1=[]\n",
    "\n",
    "for g in range(len(genes)):\n",
    "  b=[]\n",
    "  for i in genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    # a.append(Y[g])\n",
    "    a.append(i*100)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.long).reshape([-1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y.iloc[g]], dtype=torch.float).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  if y == 0:\n",
    "    list_data_0.append(data)\n",
    "  else:\n",
    "    list_data_1.append(data)\n",
    "\n",
    "print(list_data_0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Patient sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 41\n",
      "Number of charcateristics per node: 1\n",
      "Number of edges: 54\n",
      "Average node degree: 1.32\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n"
     ]
    }
   ],
   "source": [
    "data = list_data_0[0]\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of charcateristics per node: {data.num_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Train-Test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 164\n",
      "Number of test graphs: 17\n",
      "Negative cases from train: 77 of 164 = 0.4695121951219512\n",
      "Negative cases from test: 8 of 17 = 0.47058823529411764\n",
      "It should be 46.9\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(1255)\n",
    "# random.seed(125)\n",
    "# random.shuffle(list_data_0)\n",
    "# random.shuffle(list_data_1)\n",
    "\n",
    "train_dataset = list_data_0[0:77]\n",
    "test_dataset = list_data_0[77:86]\n",
    "train_dataset = train_dataset + list_data_1[0:87]\n",
    "test_dataset = test_dataset + list_data_1[87:97]\n",
    "# random.shuffle(train_dataset)\n",
    "# random.shuffle(test_dataset)\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "cont = 0\n",
    "cont1=0\n",
    "for i in train_dataset:\n",
    "    if i.y == 0:\n",
    "        cont+=1\n",
    "for i in test_dataset:\n",
    "    if i.y == 0:\n",
    "        cont1+=1\n",
    "print(\"Negative cases from train: \" + str(cont) + \" of \" + str(len(train_dataset)) + \" = \" + str(cont/len(train_dataset)))\n",
    "print(\"Negative cases from test: \" + str(cont1) + \" of \" + str(len(test_dataset)) + \" = \" + str(cont1/len(test_dataset)))\n",
    "print(\"It should be 46.9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[1312], edge_index=[2, 1728], y=[32, 1], batch=[1312], ptr=[33])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[1312], edge_index=[2, 1728], y=[32, 1], batch=[1312], ptr=[33])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[1312], edge_index=[2, 1728], y=[32, 1], batch=[1312], ptr=[33])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[1312], edge_index=[2, 1728], y=[32, 1], batch=[1312], ptr=[33])\n",
      "\n",
      "Step 5:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[1312], edge_index=[2, 1728], y=[32, 1], batch=[1312], ptr=[33])\n",
      "\n",
      "Step 6:\n",
      "=======\n",
      "Number of graphs in the current batch: 4\n",
      "DataBatch(x=[164], edge_index=[2, 216], y=[4, 1], batch=[164], ptr=[5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import SAGPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 41\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.dim = dim\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GraphConv(embed_dim, dim)\n",
    "        self.pool1 = SAGPooling(dim, ratio=0.5)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.pool2 = SAGPooling(dim, ratio=0.5)\n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=101, embedding_dim=embed_dim)\n",
    "        self.lin1 = torch.nn.Linear(1312, 50)\n",
    "        self.lin2 = torch.nn.Linear(500, 10)\n",
    "        self.lin3 = torch.nn.Linear(50, 1)\n",
    "        self.act1 = torch.nn.RReLU()\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = torch.tensor(x) #.to(torch.int)\n",
    "        # print(x.long())\n",
    "        x = self.item_embedding(x)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        # x = F.relu(self.conv2(x, edge_index))\n",
    "        # x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        # x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 #+ x2\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        # x = self.lin2(x)\n",
    "        # x = self.act1(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = torch.sigmoid(self.lin3(x)).squeeze(1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(output, data.y.squeeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        for i in range(len(output)):\n",
    "            if output[i]>0.5:\n",
    "                output[i]=1\n",
    "            else:\n",
    "                output[i]=0\n",
    "            if output[i]==data.y[i]:\n",
    "                correct=correct+1\n",
    "    # print(\"Correct: \"+str(correct) +\" of \"+str(len(loader.dataset)))\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_10980/2571792903.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x) #.to(torch.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.3750, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 1.5520, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7572, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7367, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7518, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7237, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7580, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7564, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7458, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7295, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7797, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7532, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7605, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.9279, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7456, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7583, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7644, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7287, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7435, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7194, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7169, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7301, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7514, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.7365, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.7469, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.7622, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.7217, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.7179, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.7565, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.7165, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.6861, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.6946, Train Acc: 0.5488, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.6953, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.7158, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.7759, Train Acc: 0.5793, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.6777, Train Acc: 0.5732, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.6485, Train Acc: 0.5671, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.6585, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.7111, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.7429, Train Acc: 0.5976, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.6646, Train Acc: 0.6098, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.6692, Train Acc: 0.6159, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 0.6364, Train Acc: 0.6463, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.6302, Train Acc: 0.6524, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.6192, Train Acc: 0.6220, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.6304, Train Acc: 0.6585, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.6021, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.5957, Train Acc: 0.6463, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 0.5931, Train Acc: 0.6463, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 0.7150, Train Acc: 0.5610, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 0.7132, Train Acc: 0.6463, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.6410, Train Acc: 0.7195, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.5611, Train Acc: 0.7256, Test Acc: 0.5882\n",
      "Epoch: 054, Loss: 0.5616, Train Acc: 0.7317, Test Acc: 0.5882\n",
      "Epoch: 055, Loss: 0.5211, Train Acc: 0.7378, Test Acc: 0.5882\n",
      "Epoch: 056, Loss: 0.4915, Train Acc: 0.7317, Test Acc: 0.5294\n",
      "Epoch: 057, Loss: 0.5193, Train Acc: 0.7317, Test Acc: 0.5882\n",
      "Epoch: 058, Loss: 0.4842, Train Acc: 0.7195, Test Acc: 0.5882\n",
      "Epoch: 059, Loss: 0.5093, Train Acc: 0.7439, Test Acc: 0.5882\n",
      "Epoch: 060, Loss: 0.5039, Train Acc: 0.7744, Test Acc: 0.6471\n",
      "Epoch: 061, Loss: 0.4586, Train Acc: 0.7744, Test Acc: 0.5882\n",
      "Epoch: 062, Loss: 0.4676, Train Acc: 0.7500, Test Acc: 0.5294\n",
      "Epoch: 063, Loss: 0.4640, Train Acc: 0.7317, Test Acc: 0.5882\n",
      "Epoch: 064, Loss: 0.4891, Train Acc: 0.7866, Test Acc: 0.6471\n",
      "Epoch: 065, Loss: 0.4503, Train Acc: 0.7988, Test Acc: 0.6471\n",
      "Epoch: 066, Loss: 0.4333, Train Acc: 0.8110, Test Acc: 0.6471\n",
      "Epoch: 067, Loss: 0.4120, Train Acc: 0.8293, Test Acc: 0.6471\n",
      "Epoch: 068, Loss: 0.3875, Train Acc: 0.8293, Test Acc: 0.6471\n",
      "Epoch: 069, Loss: 0.3879, Train Acc: 0.8293, Test Acc: 0.6471\n",
      "Epoch: 070, Loss: 0.4850, Train Acc: 0.6829, Test Acc: 0.5294\n",
      "Epoch: 071, Loss: 0.4319, Train Acc: 0.8598, Test Acc: 0.6471\n",
      "Epoch: 072, Loss: 0.3376, Train Acc: 0.8841, Test Acc: 0.6471\n",
      "Epoch: 073, Loss: 0.3306, Train Acc: 0.8598, Test Acc: 0.6471\n",
      "Epoch: 074, Loss: 0.3303, Train Acc: 0.8841, Test Acc: 0.7059\n",
      "Epoch: 075, Loss: 0.3049, Train Acc: 0.8841, Test Acc: 0.7059\n",
      "Epoch: 076, Loss: 0.2918, Train Acc: 0.8659, Test Acc: 0.5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 077, Loss: 0.3143, Train Acc: 0.9024, Test Acc: 0.7059\n",
      "Test accuracy: 0.7058823529411765\n",
      "Test stv: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test_avg = []\n",
    "for i in range(1):\n",
    "    model = Net(dim=656)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.6)\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(), lr=0.7)\n",
    "\n",
    "    train_epoch=[]\n",
    "    test_epoch=[]\n",
    "    epoch = 1\n",
    "    train_acc=0\n",
    "    while train_acc < 0.9 and epoch < 100:\n",
    "        loss = train(epoch)\n",
    "        train_acc = test(train_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        train_epoch.append(train_acc)\n",
    "        test_epoch.append(test_acc)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "            f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        epoch +=1\n",
    "\n",
    "    plt.plot(train_epoch, color=\"red\")\n",
    "    plt.plot(test_epoch, color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    test_avg.append(test_acc)\n",
    "\n",
    "print('Test accuracy: '+ str(np.array(test_avg).mean()))\n",
    "print('Test stv: '+ str(np.array(test_avg).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_10980/2571792903.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x) #.to(torch.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.3295, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 1.4438, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 1.0010, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7609, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7036, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7021, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.6957, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7154, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7474, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.8652, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.6982, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.6978, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7167, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7409, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.8542, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7076, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7084, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7195, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.6943, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7229, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.6914, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.6840, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.6798, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.6950, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.7208, Train Acc: 0.5427, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.6998, Train Acc: 0.5427, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.6890, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.6735, Train Acc: 0.5488, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.6803, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.6123, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 1.2115, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.5963, Train Acc: 0.6768, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.5564, Train Acc: 0.6890, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.5520, Train Acc: 0.6280, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.5672, Train Acc: 0.6402, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.4855, Train Acc: 0.6585, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.5738, Train Acc: 0.7012, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.5325, Train Acc: 0.7378, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.5427, Train Acc: 0.7195, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.4492, Train Acc: 0.7195, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.4845, Train Acc: 0.7256, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.4026, Train Acc: 0.8049, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 0.5673, Train Acc: 0.7866, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.4158, Train Acc: 0.7805, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.3690, Train Acc: 0.8354, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.3545, Train Acc: 0.7744, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.3681, Train Acc: 0.8171, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.3113, Train Acc: 0.8537, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 0.2792, Train Acc: 0.8720, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 0.2906, Train Acc: 0.7866, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 0.2903, Train Acc: 0.8598, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.2866, Train Acc: 0.8659, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.2780, Train Acc: 0.7988, Test Acc: 0.5294\n",
      "Epoch: 054, Loss: 0.2837, Train Acc: 0.8659, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 0.2326, Train Acc: 0.8780, Test Acc: 0.5294\n",
      "Epoch: 056, Loss: 0.2190, Train Acc: 0.8659, Test Acc: 0.5294\n",
      "Epoch: 057, Loss: 0.2424, Train Acc: 0.8841, Test Acc: 0.5294\n",
      "Epoch: 058, Loss: 0.2200, Train Acc: 0.8902, Test Acc: 0.5294\n",
      "Epoch: 059, Loss: 0.2114, Train Acc: 0.8841, Test Acc: 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 060, Loss: 0.2054, Train Acc: 0.9085, Test Acc: 0.4706\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 1.0994, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 2.2145, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 1.2300, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.8721, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.8628, Train Acc: 0.5244, Test Acc: 0.4706\n",
      "Epoch: 006, Loss: 0.7560, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7017, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7180, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7284, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7231, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.8588, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7154, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7396, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7123, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7109, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7718, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7458, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7188, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7692, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7078, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 1.2522, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7035, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.6859, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.6702, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.6597, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.6483, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.6789, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.6521, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.6396, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.6308, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.6146, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.5768, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.6056, Train Acc: 0.5610, Test Acc: 0.5882\n",
      "Epoch: 034, Loss: 0.5691, Train Acc: 0.5671, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.5871, Train Acc: 0.5732, Test Acc: 0.5882\n",
      "Epoch: 036, Loss: 0.6782, Train Acc: 0.5671, Test Acc: 0.5882\n",
      "Epoch: 037, Loss: 0.7253, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 038, Loss: 0.5846, Train Acc: 0.6098, Test Acc: 0.5882\n",
      "Epoch: 039, Loss: 0.5354, Train Acc: 0.6402, Test Acc: 0.5882\n",
      "Epoch: 040, Loss: 0.4604, Train Acc: 0.7317, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.4335, Train Acc: 0.7744, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.3997, Train Acc: 0.6524, Test Acc: 0.5882\n",
      "Epoch: 043, Loss: 0.4180, Train Acc: 0.6707, Test Acc: 0.5882\n",
      "Epoch: 044, Loss: 0.4389, Train Acc: 0.6646, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.3977, Train Acc: 0.7134, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.4853, Train Acc: 0.8293, Test Acc: 0.4706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 047, Loss: 0.3204, Train Acc: 0.9512, Test Acc: 0.5882\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 1.0413, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 0.8457, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7336, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7064, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7178, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7115, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7185, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7204, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7635, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7297, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7540, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7305, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7695, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7487, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7464, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7340, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7164, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7706, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7536, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7459, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.8113, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7368, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7086, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.7458, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.7182, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.7265, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.7362, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.7387, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.7102, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 1.4041, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.7174, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.6811, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.6893, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.6991, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.7098, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.6982, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.7606, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.7139, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.7167, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.7077, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.7194, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.7202, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 0.6975, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.8759, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.6769, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.6712, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.6771, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.7007, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 0.6681, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 0.7674, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 0.6785, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.6614, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.6706, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 054, Loss: 0.6605, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 0.6150, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 056, Loss: 0.6971, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 057, Loss: 0.6203, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 058, Loss: 0.6389, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 059, Loss: 0.6287, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 060, Loss: 0.7491, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 061, Loss: 0.5970, Train Acc: 0.5427, Test Acc: 0.5294\n",
      "Epoch: 062, Loss: 0.6166, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 063, Loss: 0.5830, Train Acc: 0.5732, Test Acc: 0.5294\n",
      "Epoch: 064, Loss: 0.5673, Train Acc: 0.5732, Test Acc: 0.5294\n",
      "Epoch: 065, Loss: 0.5605, Train Acc: 0.5793, Test Acc: 0.5294\n",
      "Epoch: 066, Loss: 0.5591, Train Acc: 0.5854, Test Acc: 0.5294\n",
      "Epoch: 067, Loss: 0.5579, Train Acc: 0.5976, Test Acc: 0.5294\n",
      "Epoch: 068, Loss: 0.5177, Train Acc: 0.6280, Test Acc: 0.5294\n",
      "Epoch: 069, Loss: 0.5285, Train Acc: 0.5915, Test Acc: 0.5294\n",
      "Epoch: 070, Loss: 0.7566, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 071, Loss: 0.5778, Train Acc: 0.7073, Test Acc: 0.5294\n",
      "Epoch: 072, Loss: 0.5042, Train Acc: 0.7073, Test Acc: 0.5294\n",
      "Epoch: 073, Loss: 0.5016, Train Acc: 0.6768, Test Acc: 0.4706\n",
      "Epoch: 074, Loss: 0.4476, Train Acc: 0.7195, Test Acc: 0.4706\n",
      "Epoch: 075, Loss: 0.4359, Train Acc: 0.7134, Test Acc: 0.5882\n",
      "Epoch: 076, Loss: 0.4128, Train Acc: 0.7561, Test Acc: 0.5882\n",
      "Epoch: 077, Loss: 0.3752, Train Acc: 0.7500, Test Acc: 0.5294\n",
      "Epoch: 078, Loss: 0.4055, Train Acc: 0.6829, Test Acc: 0.4706\n",
      "Epoch: 079, Loss: 0.3845, Train Acc: 0.6768, Test Acc: 0.4706\n",
      "Epoch: 080, Loss: 0.3726, Train Acc: 0.7439, Test Acc: 0.5882\n",
      "Epoch: 081, Loss: 0.3913, Train Acc: 0.7561, Test Acc: 0.5294\n",
      "Epoch: 082, Loss: 0.4105, Train Acc: 0.7195, Test Acc: 0.6471\n",
      "Epoch: 083, Loss: 0.3900, Train Acc: 0.8598, Test Acc: 0.7059\n",
      "Epoch: 084, Loss: 0.3205, Train Acc: 0.8537, Test Acc: 0.6471\n",
      "Epoch: 085, Loss: 0.3100, Train Acc: 0.8598, Test Acc: 0.6471\n",
      "Epoch: 086, Loss: 0.2916, Train Acc: 0.8110, Test Acc: 0.5294\n",
      "Epoch: 087, Loss: 0.2797, Train Acc: 0.8841, Test Acc: 0.6471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 088, Loss: 0.2076, Train Acc: 0.9146, Test Acc: 0.6471\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.9856, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 0.7416, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7156, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7021, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7107, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7025, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7257, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7060, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7470, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7263, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7203, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7107, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7595, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7015, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7187, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.6750, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 2.2976, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7169, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7101, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7082, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7170, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7408, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7205, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.6964, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.7447, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.6893, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 1.1160, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.7114, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.7036, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.6997, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.7123, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.6816, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.7572, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.6782, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.6997, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.6766, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.6951, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.6793, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.7297, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.6688, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.7091, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.6823, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 0.7063, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.6782, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.7062, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.6505, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.6440, Train Acc: 0.5610, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.6670, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 0.6745, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 0.6475, Train Acc: 0.5488, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 0.6453, Train Acc: 0.5976, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.6302, Train Acc: 0.5610, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.6129, Train Acc: 0.5488, Test Acc: 0.5294\n",
      "Epoch: 054, Loss: 0.6546, Train Acc: 0.5610, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 0.5824, Train Acc: 0.5671, Test Acc: 0.5294\n",
      "Epoch: 056, Loss: 0.6155, Train Acc: 0.5915, Test Acc: 0.4706\n",
      "Epoch: 057, Loss: 0.5846, Train Acc: 0.6037, Test Acc: 0.4706\n",
      "Epoch: 058, Loss: 0.6696, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 059, Loss: 0.6449, Train Acc: 0.6159, Test Acc: 0.5294\n",
      "Epoch: 060, Loss: 0.5724, Train Acc: 0.6098, Test Acc: 0.4706\n",
      "Epoch: 061, Loss: 0.6011, Train Acc: 0.6341, Test Acc: 0.4706\n",
      "Epoch: 062, Loss: 0.5648, Train Acc: 0.6098, Test Acc: 0.4706\n",
      "Epoch: 063, Loss: 0.5662, Train Acc: 0.6402, Test Acc: 0.5294\n",
      "Epoch: 064, Loss: 0.5529, Train Acc: 0.6402, Test Acc: 0.4706\n",
      "Epoch: 065, Loss: 0.5930, Train Acc: 0.6646, Test Acc: 0.5294\n",
      "Epoch: 066, Loss: 0.5454, Train Acc: 0.6280, Test Acc: 0.5294\n",
      "Epoch: 067, Loss: 0.5626, Train Acc: 0.6890, Test Acc: 0.4706\n",
      "Epoch: 068, Loss: 0.5356, Train Acc: 0.6890, Test Acc: 0.4706\n",
      "Epoch: 069, Loss: 0.5124, Train Acc: 0.7622, Test Acc: 0.5882\n",
      "Epoch: 070, Loss: 0.5147, Train Acc: 0.7378, Test Acc: 0.5882\n",
      "Epoch: 071, Loss: 0.4974, Train Acc: 0.5549, Test Acc: 0.4706\n",
      "Epoch: 072, Loss: 0.7583, Train Acc: 0.7561, Test Acc: 0.5294\n",
      "Epoch: 073, Loss: 0.4975, Train Acc: 0.7805, Test Acc: 0.5294\n",
      "Epoch: 074, Loss: 0.4861, Train Acc: 0.7927, Test Acc: 0.5882\n",
      "Epoch: 075, Loss: 0.4700, Train Acc: 0.7622, Test Acc: 0.5294\n",
      "Epoch: 076, Loss: 0.4634, Train Acc: 0.7927, Test Acc: 0.5294\n",
      "Epoch: 077, Loss: 0.4630, Train Acc: 0.7256, Test Acc: 0.5294\n",
      "Epoch: 078, Loss: 0.4630, Train Acc: 0.7744, Test Acc: 0.5294\n",
      "Epoch: 079, Loss: 0.4575, Train Acc: 0.7988, Test Acc: 0.5294\n",
      "Epoch: 080, Loss: 0.4350, Train Acc: 0.7622, Test Acc: 0.5294\n",
      "Epoch: 081, Loss: 0.4181, Train Acc: 0.7866, Test Acc: 0.5294\n",
      "Epoch: 082, Loss: 0.4315, Train Acc: 0.7195, Test Acc: 0.5882\n",
      "Epoch: 083, Loss: 0.4399, Train Acc: 0.6280, Test Acc: 0.4118\n",
      "Epoch: 084, Loss: 0.5766, Train Acc: 0.8171, Test Acc: 0.4706\n",
      "Epoch: 085, Loss: 0.3938, Train Acc: 0.8293, Test Acc: 0.4706\n",
      "Epoch: 086, Loss: 0.4228, Train Acc: 0.8293, Test Acc: 0.4706\n",
      "Epoch: 087, Loss: 0.3902, Train Acc: 0.8476, Test Acc: 0.4706\n",
      "Epoch: 088, Loss: 0.3769, Train Acc: 0.8537, Test Acc: 0.4118\n",
      "Epoch: 089, Loss: 0.3476, Train Acc: 0.8659, Test Acc: 0.4706\n",
      "Epoch: 090, Loss: 0.3471, Train Acc: 0.8537, Test Acc: 0.4118\n",
      "Epoch: 091, Loss: 0.3197, Train Acc: 0.8720, Test Acc: 0.4118\n",
      "Epoch: 092, Loss: 0.3433, Train Acc: 0.8659, Test Acc: 0.5294\n",
      "Epoch: 093, Loss: 0.3229, Train Acc: 0.8598, Test Acc: 0.4706\n",
      "Epoch: 094, Loss: 0.3220, Train Acc: 0.8598, Test Acc: 0.4706\n",
      "Epoch: 095, Loss: 0.3183, Train Acc: 0.7988, Test Acc: 0.5294\n",
      "Epoch: 096, Loss: 0.3930, Train Acc: 0.7012, Test Acc: 0.4118\n",
      "Epoch: 097, Loss: 0.4073, Train Acc: 0.8354, Test Acc: 0.4706\n",
      "Epoch: 098, Loss: 0.3251, Train Acc: 0.8537, Test Acc: 0.4706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 099, Loss: 0.2965, Train Acc: 0.8659, Test Acc: 0.4118\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.7369, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 0.7805, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7277, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7235, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7152, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7197, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7142, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7185, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7243, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7205, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7208, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7144, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7129, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7230, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7244, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7062, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7090, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7085, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7044, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7074, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7058, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7067, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7030, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.6840, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 2.3378, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.7229, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.7226, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.7147, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.7011, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.7108, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.7237, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.7207, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.7233, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.7259, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.7239, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.7390, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.7046, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.7905, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.7165, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.7282, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.7292, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.7377, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 0.7334, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.7376, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.6985, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.7257, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.7077, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.7239, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 0.7093, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 0.7100, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 0.7198, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.7037, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.7105, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 054, Loss: 0.8510, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 0.7196, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 056, Loss: 0.7123, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 057, Loss: 0.7323, Train Acc: 0.5488, Test Acc: 0.5294\n",
      "Epoch: 058, Loss: 0.6947, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 059, Loss: 0.7355, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 060, Loss: 0.7228, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 061, Loss: 0.7631, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 062, Loss: 0.7119, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 063, Loss: 0.7071, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 064, Loss: 0.6850, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 065, Loss: 0.6946, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 066, Loss: 0.7044, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 067, Loss: 0.6946, Train Acc: 0.5671, Test Acc: 0.5294\n",
      "Epoch: 068, Loss: 0.6446, Train Acc: 0.5427, Test Acc: 0.5294\n",
      "Epoch: 069, Loss: 0.6602, Train Acc: 0.5854, Test Acc: 0.5294\n",
      "Epoch: 070, Loss: 0.6429, Train Acc: 0.5427, Test Acc: 0.5294\n",
      "Epoch: 071, Loss: 0.7250, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 072, Loss: 0.7129, Train Acc: 0.5854, Test Acc: 0.5294\n",
      "Epoch: 073, Loss: 0.6494, Train Acc: 0.6524, Test Acc: 0.5294\n",
      "Epoch: 074, Loss: 0.6546, Train Acc: 0.6159, Test Acc: 0.5294\n",
      "Epoch: 075, Loss: 0.6342, Train Acc: 0.6402, Test Acc: 0.5294\n",
      "Epoch: 076, Loss: 0.6059, Train Acc: 0.6402, Test Acc: 0.5294\n",
      "Epoch: 077, Loss: 0.5896, Train Acc: 0.6280, Test Acc: 0.5294\n",
      "Epoch: 078, Loss: 0.5801, Train Acc: 0.6585, Test Acc: 0.5294\n",
      "Epoch: 079, Loss: 0.5743, Train Acc: 0.6707, Test Acc: 0.5294\n",
      "Epoch: 080, Loss: 0.5602, Train Acc: 0.6524, Test Acc: 0.5294\n",
      "Epoch: 081, Loss: 0.5465, Train Acc: 0.7012, Test Acc: 0.5882\n",
      "Epoch: 082, Loss: 0.5147, Train Acc: 0.7317, Test Acc: 0.6471\n",
      "Epoch: 083, Loss: 0.5355, Train Acc: 0.7195, Test Acc: 0.5294\n",
      "Epoch: 084, Loss: 0.5271, Train Acc: 0.7317, Test Acc: 0.5882\n",
      "Epoch: 085, Loss: 0.4925, Train Acc: 0.7439, Test Acc: 0.6471\n",
      "Epoch: 086, Loss: 0.4977, Train Acc: 0.7256, Test Acc: 0.6471\n",
      "Epoch: 087, Loss: 0.4929, Train Acc: 0.7561, Test Acc: 0.6471\n",
      "Epoch: 088, Loss: 1.1528, Train Acc: 0.7439, Test Acc: 0.5294\n",
      "Epoch: 089, Loss: 0.5151, Train Acc: 0.8110, Test Acc: 0.7059\n",
      "Epoch: 090, Loss: 0.4732, Train Acc: 0.7683, Test Acc: 0.6471\n",
      "Epoch: 091, Loss: 0.4679, Train Acc: 0.8293, Test Acc: 0.7059\n",
      "Epoch: 092, Loss: 0.4179, Train Acc: 0.8354, Test Acc: 0.7647\n",
      "Epoch: 093, Loss: 0.3900, Train Acc: 0.8537, Test Acc: 0.7647\n",
      "Epoch: 094, Loss: 0.4129, Train Acc: 0.8537, Test Acc: 0.7647\n",
      "Epoch: 095, Loss: 0.3617, Train Acc: 0.8537, Test Acc: 0.7647\n",
      "Epoch: 096, Loss: 0.3705, Train Acc: 0.8720, Test Acc: 0.7647\n",
      "Epoch: 097, Loss: 0.3528, Train Acc: 0.8720, Test Acc: 0.8235\n",
      "Epoch: 098, Loss: 0.3256, Train Acc: 0.8659, Test Acc: 0.7647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 099, Loss: 0.3481, Train Acc: 0.8476, Test Acc: 0.7647\n",
      "Test accuracy: 0.5764705882352941\n",
      "Test stv: 0.12561268531801542\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test_avg = []\n",
    "for i in range(5):\n",
    "    model = Net(dim=656)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.6)\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(), lr=0.7)\n",
    "\n",
    "    train_epoch=[]\n",
    "    test_epoch=[]\n",
    "    epoch = 1\n",
    "    train_acc=0\n",
    "    while train_acc < 0.9 and epoch < 100:\n",
    "        loss = train(epoch)\n",
    "        train_acc = test(train_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        train_epoch.append(train_acc)\n",
    "        test_epoch.append(test_acc)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "            f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        epoch +=1\n",
    "\n",
    "    plt.plot(train_epoch, color=\"red\")\n",
    "    plt.plot(test_epoch, color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    test_avg.append(test_acc)\n",
    "\n",
    "print('Test accuracy: '+ str(np.array(test_avg).mean()))\n",
    "print('Test stv: '+ str(np.array(test_avg).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_10980/2571792903.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x) #.to(torch.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.3568, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 0.7576, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7205, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7074, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7158, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7134, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7128, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7138, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7129, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7145, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7307, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7207, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7146, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7343, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7030, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7539, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7278, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7300, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7154, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7139, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7272, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7553, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7153, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.7094, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.6935, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.6970, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.7506, Train Acc: 0.5488, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.7274, Train Acc: 0.5732, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.6835, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.7003, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.7063, Train Acc: 0.5915, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.6704, Train Acc: 0.5732, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.6781, Train Acc: 0.5915, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.7765, Train Acc: 0.5610, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.7147, Train Acc: 0.6098, Test Acc: 0.6471\n",
      "Epoch: 036, Loss: 0.6862, Train Acc: 0.5976, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.6720, Train Acc: 0.6280, Test Acc: 0.5882\n",
      "Epoch: 038, Loss: 0.6651, Train Acc: 0.6220, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.6319, Train Acc: 0.6402, Test Acc: 0.5882\n",
      "Epoch: 040, Loss: 0.6517, Train Acc: 0.5976, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.6813, Train Acc: 0.6707, Test Acc: 0.6471\n",
      "Epoch: 042, Loss: 0.6066, Train Acc: 0.6707, Test Acc: 0.5882\n",
      "Epoch: 043, Loss: 0.6322, Train Acc: 0.6524, Test Acc: 0.5882\n",
      "Epoch: 044, Loss: 0.6271, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.6336, Train Acc: 0.6890, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.5925, Train Acc: 0.6524, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.6022, Train Acc: 0.6890, Test Acc: 0.5882\n",
      "Epoch: 048, Loss: 0.5910, Train Acc: 0.6890, Test Acc: 0.5882\n",
      "Epoch: 049, Loss: 0.5688, Train Acc: 0.6829, Test Acc: 0.5882\n",
      "Epoch: 050, Loss: 0.5706, Train Acc: 0.6768, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 0.6011, Train Acc: 0.6280, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.6333, Train Acc: 0.6646, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.6178, Train Acc: 0.6890, Test Acc: 0.5882\n",
      "Epoch: 054, Loss: 0.6218, Train Acc: 0.6829, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 0.5984, Train Acc: 0.6524, Test Acc: 0.5294\n",
      "Epoch: 056, Loss: 0.5995, Train Acc: 0.7012, Test Acc: 0.5294\n",
      "Epoch: 057, Loss: 0.5777, Train Acc: 0.6890, Test Acc: 0.5294\n",
      "Epoch: 058, Loss: 0.5878, Train Acc: 0.7073, Test Acc: 0.5882\n",
      "Epoch: 059, Loss: 0.5703, Train Acc: 0.7073, Test Acc: 0.5294\n",
      "Epoch: 060, Loss: 0.5652, Train Acc: 0.7256, Test Acc: 0.5882\n",
      "Epoch: 061, Loss: 0.5602, Train Acc: 0.6829, Test Acc: 0.5294\n",
      "Epoch: 062, Loss: 0.5840, Train Acc: 0.6768, Test Acc: 0.5294\n",
      "Epoch: 063, Loss: 0.5818, Train Acc: 0.6890, Test Acc: 0.5882\n",
      "Epoch: 064, Loss: 0.5665, Train Acc: 0.7378, Test Acc: 0.5882\n",
      "Epoch: 065, Loss: 0.5513, Train Acc: 0.7256, Test Acc: 0.5882\n",
      "Epoch: 066, Loss: 0.5480, Train Acc: 0.7500, Test Acc: 0.5882\n",
      "Epoch: 067, Loss: 0.5370, Train Acc: 0.7256, Test Acc: 0.5294\n",
      "Epoch: 068, Loss: 0.5235, Train Acc: 0.7561, Test Acc: 0.5882\n",
      "Epoch: 069, Loss: 0.5565, Train Acc: 0.7500, Test Acc: 0.5882\n",
      "Epoch: 070, Loss: 0.5374, Train Acc: 0.7500, Test Acc: 0.5294\n",
      "Epoch: 071, Loss: 0.5118, Train Acc: 0.7439, Test Acc: 0.5294\n",
      "Epoch: 072, Loss: 0.5008, Train Acc: 0.7683, Test Acc: 0.4706\n",
      "Epoch: 073, Loss: 0.4738, Train Acc: 0.7744, Test Acc: 0.4706\n",
      "Epoch: 074, Loss: 0.5391, Train Acc: 0.7744, Test Acc: 0.5294\n",
      "Epoch: 075, Loss: 0.6114, Train Acc: 0.7317, Test Acc: 0.4706\n",
      "Epoch: 076, Loss: 0.5193, Train Acc: 0.7866, Test Acc: 0.4706\n",
      "Epoch: 077, Loss: 0.4743, Train Acc: 0.7927, Test Acc: 0.4706\n",
      "Epoch: 078, Loss: 0.4808, Train Acc: 0.7744, Test Acc: 0.4706\n",
      "Epoch: 079, Loss: 0.5091, Train Acc: 0.6768, Test Acc: 0.5294\n",
      "Epoch: 080, Loss: 0.5290, Train Acc: 0.7805, Test Acc: 0.4706\n",
      "Epoch: 081, Loss: 0.4852, Train Acc: 0.7378, Test Acc: 0.4706\n",
      "Epoch: 082, Loss: 0.4970, Train Acc: 0.7866, Test Acc: 0.5882\n",
      "Epoch: 083, Loss: 0.4763, Train Acc: 0.8110, Test Acc: 0.5882\n",
      "Epoch: 084, Loss: 0.4284, Train Acc: 0.8110, Test Acc: 0.5882\n",
      "Epoch: 085, Loss: 0.4613, Train Acc: 0.8110, Test Acc: 0.5882\n",
      "Epoch: 086, Loss: 0.4275, Train Acc: 0.8110, Test Acc: 0.5882\n",
      "Epoch: 087, Loss: 0.4437, Train Acc: 0.8232, Test Acc: 0.5882\n",
      "Epoch: 088, Loss: 0.4055, Train Acc: 0.8110, Test Acc: 0.5882\n",
      "Epoch: 089, Loss: 0.4363, Train Acc: 0.7988, Test Acc: 0.5294\n",
      "Epoch: 090, Loss: 0.4289, Train Acc: 0.8293, Test Acc: 0.5882\n",
      "Epoch: 091, Loss: 0.4078, Train Acc: 0.7988, Test Acc: 0.5882\n",
      "Epoch: 092, Loss: 0.4046, Train Acc: 0.8354, Test Acc: 0.5882\n",
      "Epoch: 093, Loss: 0.4016, Train Acc: 0.8476, Test Acc: 0.5882\n",
      "Epoch: 094, Loss: 0.4004, Train Acc: 0.8476, Test Acc: 0.5882\n",
      "Epoch: 095, Loss: 0.3775, Train Acc: 0.8476, Test Acc: 0.5882\n",
      "Epoch: 096, Loss: 0.3757, Train Acc: 0.8476, Test Acc: 0.5882\n",
      "Epoch: 097, Loss: 0.3697, Train Acc: 0.8476, Test Acc: 0.5882\n",
      "Epoch: 098, Loss: 0.3697, Train Acc: 0.8476, Test Acc: 0.5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 099, Loss: 0.3799, Train Acc: 0.8415, Test Acc: 0.5882\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 1.3321, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 0.7126, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 1.2545, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.9706, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7488, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7302, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7045, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7581, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7068, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7406, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7071, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7702, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.8191, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7098, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7216, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.6950, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.9796, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7085, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.6862, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.6814, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7209, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.6896, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.6717, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.7080, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.6652, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.6461, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.6682, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.6463, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.6495, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.7506, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.6155, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.6405, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.7384, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.6646, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.5870, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.5513, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.5671, Train Acc: 0.5427, Test Acc: 0.4706\n",
      "Epoch: 038, Loss: 0.5880, Train Acc: 0.5915, Test Acc: 0.4706\n",
      "Epoch: 039, Loss: 0.5132, Train Acc: 0.7073, Test Acc: 0.4118\n",
      "Epoch: 040, Loss: 0.4882, Train Acc: 0.7317, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.4746, Train Acc: 0.5854, Test Acc: 0.4706\n",
      "Epoch: 042, Loss: 0.7585, Train Acc: 0.8293, Test Acc: 0.7059\n",
      "Epoch: 043, Loss: 0.4806, Train Acc: 0.8049, Test Acc: 0.5882\n",
      "Epoch: 044, Loss: 0.4518, Train Acc: 0.8049, Test Acc: 0.5882\n",
      "Epoch: 045, Loss: 0.4657, Train Acc: 0.7805, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.4362, Train Acc: 0.7683, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.3987, Train Acc: 0.7500, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.3873, Train Acc: 0.7439, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 0.3715, Train Acc: 0.7988, Test Acc: 0.4706\n",
      "Epoch: 050, Loss: 0.3448, Train Acc: 0.8049, Test Acc: 0.4706\n",
      "Epoch: 051, Loss: 0.3343, Train Acc: 0.7500, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.3729, Train Acc: 0.8293, Test Acc: 0.4706\n",
      "Epoch: 053, Loss: 0.3617, Train Acc: 0.7683, Test Acc: 0.5294\n",
      "Epoch: 054, Loss: 0.3724, Train Acc: 0.8720, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 0.3315, Train Acc: 0.8841, Test Acc: 0.5882\n",
      "Epoch: 056, Loss: 0.3216, Train Acc: 0.8780, Test Acc: 0.5882\n",
      "Epoch: 057, Loss: 0.3237, Train Acc: 0.7988, Test Acc: 0.5882\n",
      "Epoch: 058, Loss: 0.3504, Train Acc: 0.8354, Test Acc: 0.5882\n",
      "Epoch: 059, Loss: 0.3196, Train Acc: 0.8659, Test Acc: 0.5882\n",
      "Epoch: 060, Loss: 0.3389, Train Acc: 0.8659, Test Acc: 0.5294\n",
      "Epoch: 061, Loss: 0.3292, Train Acc: 0.8841, Test Acc: 0.5294\n",
      "Epoch: 062, Loss: 0.3145, Train Acc: 0.8780, Test Acc: 0.5294\n",
      "Epoch: 063, Loss: 0.2666, Train Acc: 0.6463, Test Acc: 0.5882\n",
      "Epoch: 064, Loss: 0.8775, Train Acc: 0.8841, Test Acc: 0.7059\n",
      "Epoch: 065, Loss: 0.3437, Train Acc: 0.8841, Test Acc: 0.7059\n",
      "Epoch: 066, Loss: 0.3064, Train Acc: 0.8902, Test Acc: 0.7059\n",
      "Epoch: 067, Loss: 0.2882, Train Acc: 0.8902, Test Acc: 0.7059\n",
      "Epoch: 068, Loss: 0.2642, Train Acc: 0.8902, Test Acc: 0.7059\n",
      "Epoch: 069, Loss: 0.2761, Train Acc: 0.8902, Test Acc: 0.7059\n",
      "Epoch: 070, Loss: 0.2740, Train Acc: 0.8963, Test Acc: 0.7059\n",
      "Epoch: 071, Loss: 0.2602, Train Acc: 0.8963, Test Acc: 0.6471\n",
      "Epoch: 072, Loss: 0.2760, Train Acc: 0.8537, Test Acc: 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 073, Loss: 0.2739, Train Acc: 0.9024, Test Acc: 0.6471\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.8423, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 0.8172, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7316, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7373, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7265, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7324, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7558, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7627, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7669, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7442, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7235, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 3.2555, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7603, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7237, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7419, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.6936, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7277, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7199, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7557, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7394, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7974, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7221, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7224, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.7257, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.7471, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.7876, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.7312, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.7735, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.7300, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.7102, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.7420, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.6814, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 1.8137, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.7239, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.6889, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.6743, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.7367, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.6925, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.6920, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.6643, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.7377, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.7132, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 0.6841, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.6696, Train Acc: 0.5427, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.6889, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.7128, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.6555, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.6305, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 0.7199, Train Acc: 0.5671, Test Acc: 0.5882\n",
      "Epoch: 050, Loss: 0.6298, Train Acc: 0.5671, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 0.6173, Train Acc: 0.5732, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.5763, Train Acc: 0.5488, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.6986, Train Acc: 0.5915, Test Acc: 0.5294\n",
      "Epoch: 054, Loss: 0.5796, Train Acc: 0.6280, Test Acc: 0.5882\n",
      "Epoch: 055, Loss: 0.5375, Train Acc: 0.6159, Test Acc: 0.5882\n",
      "Epoch: 056, Loss: 0.5219, Train Acc: 0.5976, Test Acc: 0.5294\n",
      "Epoch: 057, Loss: 0.5797, Train Acc: 0.6585, Test Acc: 0.5882\n",
      "Epoch: 058, Loss: 0.4490, Train Acc: 0.6402, Test Acc: 0.6471\n",
      "Epoch: 059, Loss: 0.6135, Train Acc: 0.7561, Test Acc: 0.5882\n",
      "Epoch: 060, Loss: 0.4547, Train Acc: 0.7744, Test Acc: 0.6471\n",
      "Epoch: 061, Loss: 0.4004, Train Acc: 0.7988, Test Acc: 0.6471\n",
      "Epoch: 062, Loss: 0.3393, Train Acc: 0.7805, Test Acc: 0.5882\n",
      "Epoch: 063, Loss: 0.3921, Train Acc: 0.7683, Test Acc: 0.5294\n",
      "Epoch: 064, Loss: 0.3284, Train Acc: 0.7866, Test Acc: 0.5882\n",
      "Epoch: 065, Loss: 0.3116, Train Acc: 0.8293, Test Acc: 0.7059\n",
      "Epoch: 066, Loss: 0.3231, Train Acc: 0.7927, Test Acc: 0.5294\n",
      "Epoch: 067, Loss: 0.3053, Train Acc: 0.8232, Test Acc: 0.6471\n",
      "Epoch: 068, Loss: 0.2473, Train Acc: 0.8720, Test Acc: 0.4706\n",
      "Epoch: 069, Loss: 0.1923, Train Acc: 0.8720, Test Acc: 0.6471\n",
      "Epoch: 070, Loss: 0.2734, Train Acc: 0.8659, Test Acc: 0.6471\n",
      "Epoch: 071, Loss: 0.2071, Train Acc: 0.8780, Test Acc: 0.5882\n",
      "Epoch: 072, Loss: 0.2015, Train Acc: 0.8963, Test Acc: 0.5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 073, Loss: 0.1845, Train Acc: 0.9085, Test Acc: 0.7647\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 1.4235, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 1.9164, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7095, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7128, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7166, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7102, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7191, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7204, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7196, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7381, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7171, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7407, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7323, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7498, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7330, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7132, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7253, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7197, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7316, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7173, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7238, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7340, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7186, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.7042, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.7186, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.7163, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.7150, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.7146, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.7170, Train Acc: 0.5793, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.6926, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.7342, Train Acc: 0.5732, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.7018, Train Acc: 0.5732, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.7070, Train Acc: 0.5854, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.6643, Train Acc: 0.5793, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.6864, Train Acc: 0.5793, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.6992, Train Acc: 0.5854, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.6779, Train Acc: 0.5854, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.6735, Train Acc: 0.5671, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.6681, Train Acc: 0.6220, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.6467, Train Acc: 0.6220, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.6373, Train Acc: 0.6220, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.6553, Train Acc: 0.5732, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 0.6924, Train Acc: 0.6646, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.5999, Train Acc: 0.6707, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.5999, Train Acc: 0.6585, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.6010, Train Acc: 0.6951, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.5720, Train Acc: 0.7134, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.5585, Train Acc: 0.6707, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 0.5899, Train Acc: 0.7012, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 0.5737, Train Acc: 0.7317, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 0.5521, Train Acc: 0.7195, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.5675, Train Acc: 0.7378, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.5494, Train Acc: 0.7378, Test Acc: 0.5294\n",
      "Epoch: 054, Loss: 0.5411, Train Acc: 0.7378, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 0.5852, Train Acc: 0.7317, Test Acc: 0.5294\n",
      "Epoch: 056, Loss: 0.5416, Train Acc: 0.7439, Test Acc: 0.5294\n",
      "Epoch: 057, Loss: 0.5479, Train Acc: 0.7439, Test Acc: 0.5294\n",
      "Epoch: 058, Loss: 0.5173, Train Acc: 0.7439, Test Acc: 0.5882\n",
      "Epoch: 059, Loss: 0.5422, Train Acc: 0.7378, Test Acc: 0.5294\n",
      "Epoch: 060, Loss: 0.5582, Train Acc: 0.6951, Test Acc: 0.5294\n",
      "Epoch: 061, Loss: 0.5753, Train Acc: 0.7378, Test Acc: 0.5294\n",
      "Epoch: 062, Loss: 0.5235, Train Acc: 0.7744, Test Acc: 0.4706\n",
      "Epoch: 063, Loss: 0.4797, Train Acc: 0.7866, Test Acc: 0.5294\n",
      "Epoch: 064, Loss: 0.4824, Train Acc: 0.7683, Test Acc: 0.5294\n",
      "Epoch: 065, Loss: 0.4987, Train Acc: 0.7927, Test Acc: 0.5294\n",
      "Epoch: 066, Loss: 0.4873, Train Acc: 0.7317, Test Acc: 0.5294\n",
      "Epoch: 067, Loss: 0.5216, Train Acc: 0.8049, Test Acc: 0.5294\n",
      "Epoch: 068, Loss: 0.4954, Train Acc: 0.7561, Test Acc: 0.5294\n",
      "Epoch: 069, Loss: 0.4876, Train Acc: 0.8232, Test Acc: 0.5294\n",
      "Epoch: 070, Loss: 0.4153, Train Acc: 0.8354, Test Acc: 0.4706\n",
      "Epoch: 071, Loss: 0.3968, Train Acc: 0.8354, Test Acc: 0.4706\n",
      "Epoch: 072, Loss: 0.4088, Train Acc: 0.8354, Test Acc: 0.4706\n",
      "Epoch: 073, Loss: 0.4043, Train Acc: 0.8354, Test Acc: 0.4706\n",
      "Epoch: 074, Loss: 0.3955, Train Acc: 0.8415, Test Acc: 0.5294\n",
      "Epoch: 075, Loss: 0.4132, Train Acc: 0.8476, Test Acc: 0.5294\n",
      "Epoch: 076, Loss: 0.3803, Train Acc: 0.8476, Test Acc: 0.5294\n",
      "Epoch: 077, Loss: 0.4263, Train Acc: 0.7988, Test Acc: 0.5294\n",
      "Epoch: 078, Loss: 0.4005, Train Acc: 0.7744, Test Acc: 0.5294\n",
      "Epoch: 079, Loss: 0.4376, Train Acc: 0.8537, Test Acc: 0.5294\n",
      "Epoch: 080, Loss: 0.3500, Train Acc: 0.8659, Test Acc: 0.5294\n",
      "Epoch: 081, Loss: 0.4798, Train Acc: 0.8354, Test Acc: 0.5294\n",
      "Epoch: 082, Loss: 0.3613, Train Acc: 0.8720, Test Acc: 0.5294\n",
      "Epoch: 083, Loss: 0.3572, Train Acc: 0.8720, Test Acc: 0.5294\n",
      "Epoch: 084, Loss: 0.3359, Train Acc: 0.8720, Test Acc: 0.5294\n",
      "Epoch: 085, Loss: 0.3306, Train Acc: 0.8780, Test Acc: 0.5294\n",
      "Epoch: 086, Loss: 0.3027, Train Acc: 0.8780, Test Acc: 0.5294\n",
      "Epoch: 087, Loss: 0.3140, Train Acc: 0.8780, Test Acc: 0.5882\n",
      "Epoch: 088, Loss: 0.3092, Train Acc: 0.8780, Test Acc: 0.5294\n",
      "Epoch: 089, Loss: 0.3318, Train Acc: 0.8780, Test Acc: 0.5294\n",
      "Epoch: 090, Loss: 0.3234, Train Acc: 0.8720, Test Acc: 0.5882\n",
      "Epoch: 091, Loss: 0.3345, Train Acc: 0.8841, Test Acc: 0.5294\n",
      "Epoch: 092, Loss: 0.2880, Train Acc: 0.8841, Test Acc: 0.5882\n",
      "Epoch: 093, Loss: 0.2994, Train Acc: 0.8537, Test Acc: 0.5294\n",
      "Epoch: 094, Loss: 0.3393, Train Acc: 0.7988, Test Acc: 0.5294\n",
      "Epoch: 095, Loss: 0.3360, Train Acc: 0.8902, Test Acc: 0.5294\n",
      "Epoch: 096, Loss: 0.2984, Train Acc: 0.8902, Test Acc: 0.5294\n",
      "Epoch: 097, Loss: 0.2861, Train Acc: 0.8902, Test Acc: 0.5882\n",
      "Epoch: 098, Loss: 0.5444, Train Acc: 0.8293, Test Acc: 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 099, Loss: 0.3039, Train Acc: 0.8963, Test Acc: 0.6471\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.9347, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 0.7891, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7263, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7172, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7202, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7269, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7193, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7236, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7162, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7156, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.9827, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7477, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7239, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7241, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7189, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7252, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7240, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7335, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7344, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7250, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7358, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.6970, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7591, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.7164, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 1.1240, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.7440, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.7200, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.7114, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.7108, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.7246, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.7122, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.7317, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.7179, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.7308, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.7358, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.7334, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.7209, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.7428, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.7196, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.7866, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.7265, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.7072, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 0.7051, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.7086, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.7234, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.7056, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.7122, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.7218, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 0.7572, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 0.7078, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 0.7009, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.6902, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.6800, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 054, Loss: 0.6716, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 0.6551, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 056, Loss: 0.7846, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 057, Loss: 0.7120, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 058, Loss: 0.7495, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 059, Loss: 0.7000, Train Acc: 0.5427, Test Acc: 0.5294\n",
      "Epoch: 060, Loss: 0.6665, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 061, Loss: 0.7458, Train Acc: 0.5793, Test Acc: 0.5294\n",
      "Epoch: 062, Loss: 0.6098, Train Acc: 0.5488, Test Acc: 0.5294\n",
      "Epoch: 063, Loss: 0.6965, Train Acc: 0.6280, Test Acc: 0.5294\n",
      "Epoch: 064, Loss: 0.5916, Train Acc: 0.6402, Test Acc: 0.5294\n",
      "Epoch: 065, Loss: 0.5222, Train Acc: 0.6037, Test Acc: 0.5294\n",
      "Epoch: 066, Loss: 0.5581, Train Acc: 0.6463, Test Acc: 0.5294\n",
      "Epoch: 067, Loss: 0.5438, Train Acc: 0.6220, Test Acc: 0.5294\n",
      "Epoch: 068, Loss: 0.5267, Train Acc: 0.6402, Test Acc: 0.5294\n",
      "Epoch: 069, Loss: 0.5942, Train Acc: 0.6159, Test Acc: 0.5294\n",
      "Epoch: 070, Loss: 0.4903, Train Acc: 0.7195, Test Acc: 0.5294\n",
      "Epoch: 071, Loss: 0.4564, Train Acc: 0.7439, Test Acc: 0.5294\n",
      "Epoch: 072, Loss: 0.4207, Train Acc: 0.7561, Test Acc: 0.5882\n",
      "Epoch: 073, Loss: 0.4022, Train Acc: 0.7805, Test Acc: 0.5882\n",
      "Epoch: 074, Loss: 0.5569, Train Acc: 0.6098, Test Acc: 0.5294\n",
      "Epoch: 075, Loss: 0.5454, Train Acc: 0.7927, Test Acc: 0.5882\n",
      "Epoch: 076, Loss: 0.3710, Train Acc: 0.8171, Test Acc: 0.6471\n",
      "Epoch: 077, Loss: 0.3272, Train Acc: 0.8049, Test Acc: 0.6471\n",
      "Epoch: 078, Loss: 0.3133, Train Acc: 0.7866, Test Acc: 0.5882\n",
      "Epoch: 079, Loss: 0.3241, Train Acc: 0.8293, Test Acc: 0.6471\n",
      "Epoch: 080, Loss: 0.2931, Train Acc: 0.8415, Test Acc: 0.5882\n",
      "Epoch: 081, Loss: 0.2627, Train Acc: 0.8780, Test Acc: 0.6471\n",
      "Epoch: 082, Loss: 0.2556, Train Acc: 0.8659, Test Acc: 0.6471\n",
      "Epoch: 083, Loss: 0.2271, Train Acc: 0.8476, Test Acc: 0.6471\n",
      "Epoch: 084, Loss: 0.2720, Train Acc: 0.7988, Test Acc: 0.5882\n",
      "Epoch: 085, Loss: 0.2699, Train Acc: 0.8780, Test Acc: 0.6471\n",
      "Epoch: 086, Loss: 0.2141, Train Acc: 0.8659, Test Acc: 0.6471\n",
      "Epoch: 087, Loss: 0.2046, Train Acc: 0.8720, Test Acc: 0.6471\n",
      "Epoch: 088, Loss: 0.2075, Train Acc: 0.8476, Test Acc: 0.5882\n",
      "Epoch: 089, Loss: 0.2506, Train Acc: 0.8720, Test Acc: 0.5882\n",
      "Epoch: 090, Loss: 0.2277, Train Acc: 0.8598, Test Acc: 0.6471\n",
      "Epoch: 091, Loss: 0.2072, Train Acc: 0.8902, Test Acc: 0.6471\n",
      "Epoch: 092, Loss: 0.2961, Train Acc: 0.8720, Test Acc: 0.5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 093, Loss: 0.2156, Train Acc: 0.9146, Test Acc: 0.5882\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.8945, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 0.7988, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7145, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7129, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7235, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7269, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7201, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7277, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7593, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7351, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7221, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7008, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7400, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7370, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7197, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7190, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.8568, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7330, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7196, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7067, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7197, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7251, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7314, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.7853, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.7172, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.6917, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.7494, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.7032, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.6942, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.8686, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.7005, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.6984, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.7192, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.6842, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.7005, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.6882, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.6870, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.6859, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.6962, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.7059, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.6934, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.6804, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 0.6766, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.6629, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.6484, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.8504, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.6508, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.6611, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 0.6241, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 0.6912, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 0.6456, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.6606, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.6518, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 054, Loss: 0.6323, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 1.4778, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 056, Loss: 0.6386, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 057, Loss: 0.6251, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 058, Loss: 0.6349, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 059, Loss: 0.6297, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 060, Loss: 0.6058, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 061, Loss: 0.6256, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 062, Loss: 0.6039, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 063, Loss: 0.6165, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 064, Loss: 0.5754, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 065, Loss: 0.8160, Train Acc: 0.5793, Test Acc: 0.5882\n",
      "Epoch: 066, Loss: 0.5742, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 067, Loss: 0.5605, Train Acc: 0.6037, Test Acc: 0.5882\n",
      "Epoch: 068, Loss: 0.5528, Train Acc: 0.5915, Test Acc: 0.6471\n",
      "Epoch: 069, Loss: 0.5311, Train Acc: 0.6220, Test Acc: 0.5882\n",
      "Epoch: 070, Loss: 0.5135, Train Acc: 0.6159, Test Acc: 0.6471\n",
      "Epoch: 071, Loss: 0.5282, Train Acc: 0.6280, Test Acc: 0.6471\n",
      "Epoch: 072, Loss: 0.5191, Train Acc: 0.7439, Test Acc: 0.5294\n",
      "Epoch: 073, Loss: 0.5296, Train Acc: 0.6159, Test Acc: 0.5294\n",
      "Epoch: 074, Loss: 0.5869, Train Acc: 0.8537, Test Acc: 0.6471\n",
      "Epoch: 075, Loss: 0.4577, Train Acc: 0.8537, Test Acc: 0.5882\n",
      "Epoch: 076, Loss: 0.4232, Train Acc: 0.8720, Test Acc: 0.5882\n",
      "Epoch: 077, Loss: 0.4050, Train Acc: 0.8537, Test Acc: 0.6471\n",
      "Epoch: 078, Loss: 0.4001, Train Acc: 0.8354, Test Acc: 0.5882\n",
      "Epoch: 079, Loss: 0.3586, Train Acc: 0.7988, Test Acc: 0.5294\n",
      "Epoch: 080, Loss: 0.3747, Train Acc: 0.7988, Test Acc: 0.6471\n",
      "Epoch: 081, Loss: 0.3323, Train Acc: 0.8232, Test Acc: 0.6471\n",
      "Epoch: 082, Loss: 0.3944, Train Acc: 0.8963, Test Acc: 0.6471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 083, Loss: 0.3244, Train Acc: 0.9268, Test Acc: 0.6471\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 1.1026, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 0.7278, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7185, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7150, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7241, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7183, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7157, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.8460, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7397, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7409, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7188, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7160, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7648, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7198, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7245, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7169, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7122, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.9548, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7283, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7217, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7244, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7227, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7237, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.7038, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.7298, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.8306, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.7162, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.7144, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.7170, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.7222, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.7223, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.7274, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.7531, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.7507, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.7430, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.7424, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.7215, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.7199, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.7142, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.7270, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.7321, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.7263, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 0.7353, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.7086, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.6916, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.7845, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.7254, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.7085, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 0.7142, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 0.7159, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 0.7079, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.7359, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.7000, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 054, Loss: 0.6880, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 0.7051, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 056, Loss: 0.6816, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 057, Loss: 0.6766, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 058, Loss: 0.6514, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 059, Loss: 0.6640, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 060, Loss: 0.6470, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 061, Loss: 0.6885, Train Acc: 0.5427, Test Acc: 0.5294\n",
      "Epoch: 062, Loss: 0.7089, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 063, Loss: 0.6272, Train Acc: 0.5854, Test Acc: 0.5294\n",
      "Epoch: 064, Loss: 0.6157, Train Acc: 0.5854, Test Acc: 0.5294\n",
      "Epoch: 065, Loss: 0.5951, Train Acc: 0.5976, Test Acc: 0.5294\n",
      "Epoch: 066, Loss: 0.6026, Train Acc: 0.5976, Test Acc: 0.5294\n",
      "Epoch: 067, Loss: 0.5855, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 068, Loss: 0.5275, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 069, Loss: 0.5299, Train Acc: 0.6280, Test Acc: 0.5294\n",
      "Epoch: 070, Loss: 0.6400, Train Acc: 0.6402, Test Acc: 0.5294\n",
      "Epoch: 071, Loss: 0.5326, Train Acc: 0.6037, Test Acc: 0.5294\n",
      "Epoch: 072, Loss: 0.5452, Train Acc: 0.6646, Test Acc: 0.5294\n",
      "Epoch: 073, Loss: 0.4778, Train Acc: 0.6524, Test Acc: 0.5294\n",
      "Epoch: 074, Loss: 0.4735, Train Acc: 0.6951, Test Acc: 0.4706\n",
      "Epoch: 075, Loss: 0.4254, Train Acc: 0.6585, Test Acc: 0.4706\n",
      "Epoch: 076, Loss: 0.4626, Train Acc: 0.7012, Test Acc: 0.5294\n",
      "Epoch: 077, Loss: 0.4025, Train Acc: 0.7317, Test Acc: 0.4706\n",
      "Epoch: 078, Loss: 0.3729, Train Acc: 0.7378, Test Acc: 0.4118\n",
      "Epoch: 079, Loss: 0.4181, Train Acc: 0.7378, Test Acc: 0.4706\n",
      "Epoch: 080, Loss: 0.3629, Train Acc: 0.6524, Test Acc: 0.5294\n",
      "Epoch: 081, Loss: 0.6336, Train Acc: 0.7622, Test Acc: 0.5294\n",
      "Epoch: 082, Loss: 0.3511, Train Acc: 0.8293, Test Acc: 0.4706\n",
      "Epoch: 083, Loss: 0.2982, Train Acc: 0.8110, Test Acc: 0.4706\n",
      "Epoch: 084, Loss: 0.2944, Train Acc: 0.8354, Test Acc: 0.4118\n",
      "Epoch: 085, Loss: 0.2785, Train Acc: 0.8476, Test Acc: 0.5294\n",
      "Epoch: 086, Loss: 0.2474, Train Acc: 0.8415, Test Acc: 0.4706\n",
      "Epoch: 087, Loss: 0.2239, Train Acc: 0.8476, Test Acc: 0.4706\n",
      "Epoch: 088, Loss: 0.2244, Train Acc: 0.8354, Test Acc: 0.4706\n",
      "Epoch: 089, Loss: 0.2039, Train Acc: 0.8110, Test Acc: 0.5294\n",
      "Epoch: 090, Loss: 0.1971, Train Acc: 0.8780, Test Acc: 0.4706\n",
      "Epoch: 091, Loss: 0.2960, Train Acc: 0.8232, Test Acc: 0.4706\n",
      "Epoch: 092, Loss: 0.2120, Train Acc: 0.8598, Test Acc: 0.5294\n",
      "Epoch: 093, Loss: 0.2092, Train Acc: 0.8902, Test Acc: 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 094, Loss: 0.1813, Train Acc: 0.9146, Test Acc: 0.4118\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 1.1234, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 1.4658, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7397, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7214, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7136, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7466, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7156, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7519, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7273, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7321, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7660, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7346, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.8031, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7529, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7187, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7429, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7469, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7308, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7517, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7253, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7274, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7388, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7326, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 1.0242, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.7027, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.7051, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.6880, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.7037, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.6887, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.7209, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.6945, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.7058, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.6887, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.6959, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.7104, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.7051, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.6983, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.6891, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.6703, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.7770, Train Acc: 0.5610, Test Acc: 0.6471\n",
      "Epoch: 041, Loss: 0.6360, Train Acc: 0.5488, Test Acc: 0.5882\n",
      "Epoch: 042, Loss: 0.6591, Train Acc: 0.5549, Test Acc: 0.5882\n",
      "Epoch: 043, Loss: 0.6492, Train Acc: 0.5427, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.6946, Train Acc: 0.5488, Test Acc: 0.5882\n",
      "Epoch: 045, Loss: 0.6213, Train Acc: 0.5610, Test Acc: 0.6471\n",
      "Epoch: 046, Loss: 0.6149, Train Acc: 0.6524, Test Acc: 0.6471\n",
      "Epoch: 047, Loss: 0.6092, Train Acc: 0.7256, Test Acc: 0.7059\n",
      "Epoch: 048, Loss: 0.5584, Train Acc: 0.6768, Test Acc: 0.6471\n",
      "Epoch: 049, Loss: 0.6584, Train Acc: 0.6707, Test Acc: 0.7059\n",
      "Epoch: 050, Loss: 0.5727, Train Acc: 0.7256, Test Acc: 0.7059\n",
      "Epoch: 051, Loss: 0.5643, Train Acc: 0.7317, Test Acc: 0.5882\n",
      "Epoch: 052, Loss: 0.5321, Train Acc: 0.6951, Test Acc: 0.7059\n",
      "Epoch: 053, Loss: 0.5303, Train Acc: 0.7378, Test Acc: 0.7059\n",
      "Epoch: 054, Loss: 0.5006, Train Acc: 0.7683, Test Acc: 0.6471\n",
      "Epoch: 055, Loss: 0.5504, Train Acc: 0.7378, Test Acc: 0.7647\n",
      "Epoch: 056, Loss: 0.4822, Train Acc: 0.8110, Test Acc: 0.7059\n",
      "Epoch: 057, Loss: 0.4307, Train Acc: 0.8354, Test Acc: 0.7059\n",
      "Epoch: 058, Loss: 0.4061, Train Acc: 0.8415, Test Acc: 0.7059\n",
      "Epoch: 059, Loss: 0.3547, Train Acc: 0.8537, Test Acc: 0.7059\n",
      "Epoch: 060, Loss: 0.3451, Train Acc: 0.8598, Test Acc: 0.7059\n",
      "Epoch: 061, Loss: 0.3753, Train Acc: 0.7683, Test Acc: 0.7647\n",
      "Epoch: 062, Loss: 0.4336, Train Acc: 0.8354, Test Acc: 0.7647\n",
      "Epoch: 063, Loss: 0.3650, Train Acc: 0.8841, Test Acc: 0.5882\n",
      "Epoch: 064, Loss: 0.2723, Train Acc: 0.8902, Test Acc: 0.6471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 065, Loss: 0.2949, Train Acc: 0.9024, Test Acc: 0.5882\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 1.2798, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 1.7430, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7347, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7402, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7562, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7632, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7458, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7260, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7398, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7490, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7488, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7418, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7575, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7632, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7397, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7547, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7347, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7678, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7586, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7248, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7437, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7265, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7180, Train Acc: 0.5671, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.6778, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.7044, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.7236, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.7435, Train Acc: 0.6220, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.6767, Train Acc: 0.6037, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.7209, Train Acc: 0.5671, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.7152, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.6669, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.6713, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.6591, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.6740, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.6692, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.6407, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.6423, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.6565, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.6495, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.6526, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.6441, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.6613, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 0.6501, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.6370, Train Acc: 0.6402, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.6790, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.6654, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.6485, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.6436, Train Acc: 0.6402, Test Acc: 0.4706\n",
      "Epoch: 049, Loss: 0.6526, Train Acc: 0.6341, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 0.6500, Train Acc: 0.6402, Test Acc: 0.4706\n",
      "Epoch: 051, Loss: 0.6254, Train Acc: 0.6524, Test Acc: 0.4706\n",
      "Epoch: 052, Loss: 0.6176, Train Acc: 0.6402, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.6522, Train Acc: 0.6402, Test Acc: 0.4706\n",
      "Epoch: 054, Loss: 0.6500, Train Acc: 0.6402, Test Acc: 0.4706\n",
      "Epoch: 055, Loss: 0.6211, Train Acc: 0.6646, Test Acc: 0.4706\n",
      "Epoch: 056, Loss: 0.6188, Train Acc: 0.6402, Test Acc: 0.5294\n",
      "Epoch: 057, Loss: 0.6624, Train Acc: 0.6463, Test Acc: 0.5294\n",
      "Epoch: 058, Loss: 0.6388, Train Acc: 0.6585, Test Acc: 0.4706\n",
      "Epoch: 059, Loss: 0.6049, Train Acc: 0.6646, Test Acc: 0.4118\n",
      "Epoch: 060, Loss: 0.6383, Train Acc: 0.6646, Test Acc: 0.4118\n",
      "Epoch: 061, Loss: 0.6216, Train Acc: 0.6585, Test Acc: 0.4118\n",
      "Epoch: 062, Loss: 0.5976, Train Acc: 0.6463, Test Acc: 0.4706\n",
      "Epoch: 063, Loss: 0.6508, Train Acc: 0.6585, Test Acc: 0.4118\n",
      "Epoch: 064, Loss: 0.6210, Train Acc: 0.6585, Test Acc: 0.4118\n",
      "Epoch: 065, Loss: 0.5912, Train Acc: 0.6646, Test Acc: 0.4118\n",
      "Epoch: 066, Loss: 0.5962, Train Acc: 0.6646, Test Acc: 0.4118\n",
      "Epoch: 067, Loss: 0.6195, Train Acc: 0.6646, Test Acc: 0.4118\n",
      "Epoch: 068, Loss: 0.5995, Train Acc: 0.6646, Test Acc: 0.4118\n",
      "Epoch: 069, Loss: 0.5779, Train Acc: 0.6646, Test Acc: 0.4706\n",
      "Epoch: 070, Loss: 0.6807, Train Acc: 0.6463, Test Acc: 0.5294\n",
      "Epoch: 071, Loss: 0.6499, Train Acc: 0.6646, Test Acc: 0.4706\n",
      "Epoch: 072, Loss: 0.5982, Train Acc: 0.6890, Test Acc: 0.4118\n",
      "Epoch: 073, Loss: 0.5902, Train Acc: 0.6768, Test Acc: 0.4118\n",
      "Epoch: 074, Loss: 0.6056, Train Acc: 0.6951, Test Acc: 0.3529\n",
      "Epoch: 075, Loss: 0.5694, Train Acc: 0.7012, Test Acc: 0.3529\n",
      "Epoch: 076, Loss: 0.5706, Train Acc: 0.7134, Test Acc: 0.3529\n",
      "Epoch: 077, Loss: 0.7336, Train Acc: 0.6829, Test Acc: 0.4118\n",
      "Epoch: 078, Loss: 0.5844, Train Acc: 0.7195, Test Acc: 0.4118\n",
      "Epoch: 079, Loss: 0.5374, Train Acc: 0.7134, Test Acc: 0.4118\n",
      "Epoch: 080, Loss: 0.5498, Train Acc: 0.7195, Test Acc: 0.4118\n",
      "Epoch: 081, Loss: 0.5230, Train Acc: 0.7378, Test Acc: 0.3529\n",
      "Epoch: 082, Loss: 0.5098, Train Acc: 0.7500, Test Acc: 0.4118\n",
      "Epoch: 083, Loss: 0.5216, Train Acc: 0.7500, Test Acc: 0.3529\n",
      "Epoch: 084, Loss: 0.5007, Train Acc: 0.7561, Test Acc: 0.3529\n",
      "Epoch: 085, Loss: 0.5121, Train Acc: 0.7317, Test Acc: 0.3529\n",
      "Epoch: 086, Loss: 0.4985, Train Acc: 0.7866, Test Acc: 0.3529\n",
      "Epoch: 087, Loss: 0.4435, Train Acc: 0.7622, Test Acc: 0.4118\n",
      "Epoch: 088, Loss: 0.5030, Train Acc: 0.6951, Test Acc: 0.4706\n",
      "Epoch: 089, Loss: 0.6129, Train Acc: 0.6951, Test Acc: 0.4706\n",
      "Epoch: 090, Loss: 0.5506, Train Acc: 0.7500, Test Acc: 0.4118\n",
      "Epoch: 091, Loss: 0.4895, Train Acc: 0.7866, Test Acc: 0.4118\n",
      "Epoch: 092, Loss: 0.4525, Train Acc: 0.7988, Test Acc: 0.4118\n",
      "Epoch: 093, Loss: 0.4486, Train Acc: 0.7927, Test Acc: 0.4118\n",
      "Epoch: 094, Loss: 0.4393, Train Acc: 0.7927, Test Acc: 0.4118\n",
      "Epoch: 095, Loss: 0.4397, Train Acc: 0.8049, Test Acc: 0.4118\n",
      "Epoch: 096, Loss: 0.4120, Train Acc: 0.8049, Test Acc: 0.4118\n",
      "Epoch: 097, Loss: 0.4154, Train Acc: 0.8049, Test Acc: 0.4118\n",
      "Epoch: 098, Loss: 0.4263, Train Acc: 0.8049, Test Acc: 0.4118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 099, Loss: 0.4116, Train Acc: 0.8049, Test Acc: 0.4118\n",
      "Net(\n",
      "  (conv1): GraphConv(41, 656)\n",
      "  (pool1): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(656, 656)\n",
      "  (pool2): SAGPooling(GraphConv, 656, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 41)\n",
      "  (lin1): Linear(in_features=1312, out_features=50, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.7588, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 002, Loss: 0.7300, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 003, Loss: 0.7261, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 004, Loss: 0.7286, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 005, Loss: 0.7186, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 006, Loss: 0.7157, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 007, Loss: 0.7186, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 008, Loss: 0.7265, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 009, Loss: 0.7291, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 010, Loss: 0.7519, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 011, Loss: 0.7204, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 012, Loss: 0.7182, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 013, Loss: 0.7372, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 014, Loss: 0.7322, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 015, Loss: 0.7182, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 016, Loss: 0.7280, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 017, Loss: 0.7280, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 018, Loss: 0.7330, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 019, Loss: 0.7216, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 020, Loss: 0.7149, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 021, Loss: 0.7419, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 022, Loss: 0.7125, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 023, Loss: 0.7133, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 024, Loss: 0.7108, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 025, Loss: 0.7290, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 026, Loss: 0.7004, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 027, Loss: 0.7270, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 028, Loss: 0.7233, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 029, Loss: 0.7333, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 030, Loss: 0.7105, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 031, Loss: 0.7828, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 032, Loss: 0.7301, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 033, Loss: 0.7075, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 034, Loss: 0.7268, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 035, Loss: 0.6962, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 036, Loss: 0.7021, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 037, Loss: 0.7177, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 038, Loss: 0.7149, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 039, Loss: 0.7073, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 040, Loss: 0.7015, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 041, Loss: 0.7113, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 042, Loss: 0.7236, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 043, Loss: 0.7201, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 044, Loss: 0.6978, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 045, Loss: 0.8021, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 046, Loss: 0.7062, Train Acc: 0.5488, Test Acc: 0.5294\n",
      "Epoch: 047, Loss: 0.6993, Train Acc: 0.5488, Test Acc: 0.5294\n",
      "Epoch: 048, Loss: 0.7056, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 049, Loss: 0.6905, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 050, Loss: 0.6889, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 051, Loss: 0.6956, Train Acc: 0.5427, Test Acc: 0.5294\n",
      "Epoch: 052, Loss: 0.7198, Train Acc: 0.5305, Test Acc: 0.5294\n",
      "Epoch: 053, Loss: 0.7030, Train Acc: 0.5610, Test Acc: 0.5882\n",
      "Epoch: 054, Loss: 0.7042, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 055, Loss: 0.6951, Train Acc: 0.5854, Test Acc: 0.5882\n",
      "Epoch: 056, Loss: 0.6811, Train Acc: 0.5793, Test Acc: 0.5294\n",
      "Epoch: 057, Loss: 0.6688, Train Acc: 0.5793, Test Acc: 0.5882\n",
      "Epoch: 058, Loss: 0.6826, Train Acc: 0.5549, Test Acc: 0.5294\n",
      "Epoch: 059, Loss: 0.6813, Train Acc: 0.5671, Test Acc: 0.5294\n",
      "Epoch: 060, Loss: 0.6799, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 061, Loss: 0.7122, Train Acc: 0.5366, Test Acc: 0.5294\n",
      "Epoch: 062, Loss: 0.6916, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 063, Loss: 0.6830, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 064, Loss: 0.6598, Train Acc: 0.5854, Test Acc: 0.5882\n",
      "Epoch: 065, Loss: 0.6620, Train Acc: 0.5732, Test Acc: 0.5294\n",
      "Epoch: 066, Loss: 0.6907, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 067, Loss: 0.6889, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 068, Loss: 0.6579, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 069, Loss: 0.7533, Train Acc: 0.5854, Test Acc: 0.5294\n",
      "Epoch: 070, Loss: 0.6658, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 071, Loss: 0.7305, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 072, Loss: 0.6603, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 073, Loss: 0.6583, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 074, Loss: 0.6670, Train Acc: 0.5915, Test Acc: 0.5294\n",
      "Epoch: 075, Loss: 0.6446, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 076, Loss: 0.6558, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 077, Loss: 0.6597, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 078, Loss: 0.6522, Train Acc: 0.5915, Test Acc: 0.5882\n",
      "Epoch: 079, Loss: 0.6515, Train Acc: 0.5915, Test Acc: 0.5294\n",
      "Epoch: 080, Loss: 0.6521, Train Acc: 0.5915, Test Acc: 0.5294\n",
      "Epoch: 081, Loss: 0.6366, Train Acc: 0.5976, Test Acc: 0.5294\n",
      "Epoch: 082, Loss: 0.6637, Train Acc: 0.6220, Test Acc: 0.5882\n",
      "Epoch: 083, Loss: 0.6155, Train Acc: 0.6098, Test Acc: 0.5294\n",
      "Epoch: 084, Loss: 0.6304, Train Acc: 0.5732, Test Acc: 0.5294\n",
      "Epoch: 085, Loss: 0.6327, Train Acc: 0.6037, Test Acc: 0.5294\n",
      "Epoch: 086, Loss: 0.7064, Train Acc: 0.6402, Test Acc: 0.5882\n",
      "Epoch: 087, Loss: 0.7245, Train Acc: 0.6280, Test Acc: 0.5294\n",
      "Epoch: 088, Loss: 0.6187, Train Acc: 0.6524, Test Acc: 0.5882\n",
      "Epoch: 089, Loss: 0.5960, Train Acc: 0.6524, Test Acc: 0.5882\n",
      "Epoch: 090, Loss: 0.5971, Train Acc: 0.6402, Test Acc: 0.5882\n",
      "Epoch: 091, Loss: 0.5906, Train Acc: 0.6585, Test Acc: 0.5882\n",
      "Epoch: 092, Loss: 0.5613, Train Acc: 0.6707, Test Acc: 0.5882\n",
      "Epoch: 093, Loss: 0.5380, Train Acc: 0.6707, Test Acc: 0.5882\n",
      "Epoch: 094, Loss: 0.5628, Train Acc: 0.6585, Test Acc: 0.5294\n",
      "Epoch: 095, Loss: 0.5651, Train Acc: 0.6768, Test Acc: 0.5294\n",
      "Epoch: 096, Loss: 0.5416, Train Acc: 0.6829, Test Acc: 0.5882\n",
      "Epoch: 097, Loss: 0.5187, Train Acc: 0.6890, Test Acc: 0.5294\n",
      "Epoch: 098, Loss: 0.4991, Train Acc: 0.6341, Test Acc: 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 099, Loss: 0.5440, Train Acc: 0.7012, Test Acc: 0.5882\n",
      "Test accuracy: 0.5882352941176471\n",
      "Test stv: 0.10188534162169867\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test_avg = []\n",
    "for i in range(10):\n",
    "    model = Net(dim=656)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.6)\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(), lr=0.7)\n",
    "\n",
    "    train_epoch=[]\n",
    "    test_epoch=[]\n",
    "    epoch = 1\n",
    "    train_acc=0\n",
    "    while train_acc < 0.9 and epoch < 100:\n",
    "        loss = train(epoch)\n",
    "        train_acc = test(train_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        train_epoch.append(train_acc)\n",
    "        test_epoch.append(test_acc)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "            f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        epoch +=1\n",
    "\n",
    "    plt.plot(train_epoch, color=\"red\")\n",
    "    plt.plot(test_epoch, color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    test_avg.append(test_acc)\n",
    "\n",
    "print('Test accuracy: '+ str(np.array(test_avg).mean()))\n",
    "print('Test stv: '+ str(np.array(test_avg).std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb15f1e0f376981e7b6e1fc44ae8b8146823f10f258bcd6e448b0230b889fc06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
