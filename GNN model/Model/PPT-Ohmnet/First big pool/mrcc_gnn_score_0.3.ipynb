{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cpu.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gene matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APAF1</th>\n",
       "      <th>ARID1A</th>\n",
       "      <th>ATM</th>\n",
       "      <th>BAP1</th>\n",
       "      <th>EPAS1</th>\n",
       "      <th>ERBB2</th>\n",
       "      <th>GSTP1</th>\n",
       "      <th>HSPB1</th>\n",
       "      <th>HSPD1</th>\n",
       "      <th>IL6</th>\n",
       "      <th>...</th>\n",
       "      <th>RELA</th>\n",
       "      <th>RNF139</th>\n",
       "      <th>SETD2</th>\n",
       "      <th>SLC2A1</th>\n",
       "      <th>SOD2</th>\n",
       "      <th>TGM2</th>\n",
       "      <th>TP53</th>\n",
       "      <th>TSC1</th>\n",
       "      <th>TSC2</th>\n",
       "      <th>VHL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.668769</td>\n",
       "      <td>33.848026</td>\n",
       "      <td>35.942429</td>\n",
       "      <td>33.677294</td>\n",
       "      <td>37.95811</td>\n",
       "      <td>35.32243</td>\n",
       "      <td>36.48088</td>\n",
       "      <td>38.25591</td>\n",
       "      <td>37.02204</td>\n",
       "      <td>29.74353</td>\n",
       "      <td>...</td>\n",
       "      <td>33.09884</td>\n",
       "      <td>32.46554</td>\n",
       "      <td>32.58565</td>\n",
       "      <td>33.38586</td>\n",
       "      <td>38.67433</td>\n",
       "      <td>38.50142</td>\n",
       "      <td>33.83518</td>\n",
       "      <td>32.93402</td>\n",
       "      <td>34.93520</td>\n",
       "      <td>32.30615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.337493</td>\n",
       "      <td>33.843513</td>\n",
       "      <td>35.988225</td>\n",
       "      <td>32.643149</td>\n",
       "      <td>38.83281</td>\n",
       "      <td>33.71706</td>\n",
       "      <td>36.21403</td>\n",
       "      <td>37.41814</td>\n",
       "      <td>36.48920</td>\n",
       "      <td>24.29608</td>\n",
       "      <td>...</td>\n",
       "      <td>33.06941</td>\n",
       "      <td>32.27190</td>\n",
       "      <td>33.19915</td>\n",
       "      <td>33.69538</td>\n",
       "      <td>38.64559</td>\n",
       "      <td>34.33752</td>\n",
       "      <td>34.44810</td>\n",
       "      <td>33.16630</td>\n",
       "      <td>35.08304</td>\n",
       "      <td>32.19988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.818198</td>\n",
       "      <td>33.516005</td>\n",
       "      <td>36.193587</td>\n",
       "      <td>32.368866</td>\n",
       "      <td>37.19345</td>\n",
       "      <td>33.38917</td>\n",
       "      <td>35.34069</td>\n",
       "      <td>37.94992</td>\n",
       "      <td>36.51745</td>\n",
       "      <td>33.97471</td>\n",
       "      <td>...</td>\n",
       "      <td>33.64965</td>\n",
       "      <td>32.55514</td>\n",
       "      <td>32.84628</td>\n",
       "      <td>36.23588</td>\n",
       "      <td>40.50559</td>\n",
       "      <td>35.50178</td>\n",
       "      <td>35.41980</td>\n",
       "      <td>33.63282</td>\n",
       "      <td>34.79244</td>\n",
       "      <td>31.49147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.601293</td>\n",
       "      <td>34.197698</td>\n",
       "      <td>36.578348</td>\n",
       "      <td>31.895400</td>\n",
       "      <td>39.46713</td>\n",
       "      <td>33.22340</td>\n",
       "      <td>35.36208</td>\n",
       "      <td>37.86790</td>\n",
       "      <td>37.13423</td>\n",
       "      <td>31.49884</td>\n",
       "      <td>...</td>\n",
       "      <td>33.14439</td>\n",
       "      <td>33.19823</td>\n",
       "      <td>33.68316</td>\n",
       "      <td>34.41938</td>\n",
       "      <td>38.99231</td>\n",
       "      <td>35.77236</td>\n",
       "      <td>34.18862</td>\n",
       "      <td>32.88250</td>\n",
       "      <td>35.02014</td>\n",
       "      <td>32.11538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.593121</td>\n",
       "      <td>33.351460</td>\n",
       "      <td>36.807497</td>\n",
       "      <td>33.968348</td>\n",
       "      <td>38.49884</td>\n",
       "      <td>33.40876</td>\n",
       "      <td>34.26885</td>\n",
       "      <td>35.26187</td>\n",
       "      <td>36.15876</td>\n",
       "      <td>33.92214</td>\n",
       "      <td>...</td>\n",
       "      <td>32.91133</td>\n",
       "      <td>30.89813</td>\n",
       "      <td>34.63036</td>\n",
       "      <td>34.59911</td>\n",
       "      <td>38.41437</td>\n",
       "      <td>33.47112</td>\n",
       "      <td>34.91241</td>\n",
       "      <td>33.44515</td>\n",
       "      <td>35.01310</td>\n",
       "      <td>33.33646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>32.603769</td>\n",
       "      <td>34.133940</td>\n",
       "      <td>35.318612</td>\n",
       "      <td>33.843872</td>\n",
       "      <td>39.13826</td>\n",
       "      <td>33.62978</td>\n",
       "      <td>35.75912</td>\n",
       "      <td>37.34151</td>\n",
       "      <td>35.52103</td>\n",
       "      <td>34.47504</td>\n",
       "      <td>...</td>\n",
       "      <td>33.45578</td>\n",
       "      <td>32.12573</td>\n",
       "      <td>33.34867</td>\n",
       "      <td>36.50807</td>\n",
       "      <td>35.15898</td>\n",
       "      <td>34.57504</td>\n",
       "      <td>35.39631</td>\n",
       "      <td>32.93248</td>\n",
       "      <td>35.12781</td>\n",
       "      <td>31.79913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>33.619701</td>\n",
       "      <td>32.373330</td>\n",
       "      <td>35.771711</td>\n",
       "      <td>32.519967</td>\n",
       "      <td>35.86338</td>\n",
       "      <td>31.25871</td>\n",
       "      <td>37.02994</td>\n",
       "      <td>38.71080</td>\n",
       "      <td>36.75353</td>\n",
       "      <td>32.12763</td>\n",
       "      <td>...</td>\n",
       "      <td>32.33496</td>\n",
       "      <td>34.27276</td>\n",
       "      <td>32.16275</td>\n",
       "      <td>33.97705</td>\n",
       "      <td>38.85295</td>\n",
       "      <td>32.38354</td>\n",
       "      <td>32.04003</td>\n",
       "      <td>32.62658</td>\n",
       "      <td>33.78873</td>\n",
       "      <td>31.66344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>33.316811</td>\n",
       "      <td>34.118843</td>\n",
       "      <td>36.008091</td>\n",
       "      <td>33.115209</td>\n",
       "      <td>37.91340</td>\n",
       "      <td>32.66502</td>\n",
       "      <td>35.47039</td>\n",
       "      <td>38.35448</td>\n",
       "      <td>35.94739</td>\n",
       "      <td>29.04315</td>\n",
       "      <td>...</td>\n",
       "      <td>33.26864</td>\n",
       "      <td>32.92305</td>\n",
       "      <td>34.01015</td>\n",
       "      <td>34.85694</td>\n",
       "      <td>37.96021</td>\n",
       "      <td>36.65499</td>\n",
       "      <td>33.34126</td>\n",
       "      <td>32.81059</td>\n",
       "      <td>35.24316</td>\n",
       "      <td>32.39461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>33.046782</td>\n",
       "      <td>33.833796</td>\n",
       "      <td>37.008936</td>\n",
       "      <td>32.895151</td>\n",
       "      <td>37.96870</td>\n",
       "      <td>33.57688</td>\n",
       "      <td>33.76634</td>\n",
       "      <td>36.74006</td>\n",
       "      <td>35.82772</td>\n",
       "      <td>29.64267</td>\n",
       "      <td>...</td>\n",
       "      <td>33.53102</td>\n",
       "      <td>31.87160</td>\n",
       "      <td>33.23246</td>\n",
       "      <td>34.24055</td>\n",
       "      <td>37.24924</td>\n",
       "      <td>36.84744</td>\n",
       "      <td>34.98283</td>\n",
       "      <td>34.04810</td>\n",
       "      <td>35.60526</td>\n",
       "      <td>32.34561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>32.170042</td>\n",
       "      <td>33.739764</td>\n",
       "      <td>35.937812</td>\n",
       "      <td>33.404526</td>\n",
       "      <td>38.75226</td>\n",
       "      <td>32.10887</td>\n",
       "      <td>33.23928</td>\n",
       "      <td>37.84644</td>\n",
       "      <td>37.90058</td>\n",
       "      <td>33.21729</td>\n",
       "      <td>...</td>\n",
       "      <td>33.56852</td>\n",
       "      <td>32.47268</td>\n",
       "      <td>32.81781</td>\n",
       "      <td>35.99620</td>\n",
       "      <td>38.54211</td>\n",
       "      <td>37.23935</td>\n",
       "      <td>33.82151</td>\n",
       "      <td>33.82576</td>\n",
       "      <td>35.13995</td>\n",
       "      <td>30.34566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         APAF1     ARID1A        ATM       BAP1     EPAS1     ERBB2     GSTP1  \\\n",
       "0    32.668769  33.848026  35.942429  33.677294  37.95811  35.32243  36.48088   \n",
       "1    32.337493  33.843513  35.988225  32.643149  38.83281  33.71706  36.21403   \n",
       "2    31.818198  33.516005  36.193587  32.368866  37.19345  33.38917  35.34069   \n",
       "3    32.601293  34.197698  36.578348  31.895400  39.46713  33.22340  35.36208   \n",
       "4    33.593121  33.351460  36.807497  33.968348  38.49884  33.40876  34.26885   \n",
       "..         ...        ...        ...        ...       ...       ...       ...   \n",
       "176  32.603769  34.133940  35.318612  33.843872  39.13826  33.62978  35.75912   \n",
       "177  33.619701  32.373330  35.771711  32.519967  35.86338  31.25871  37.02994   \n",
       "178  33.316811  34.118843  36.008091  33.115209  37.91340  32.66502  35.47039   \n",
       "179  33.046782  33.833796  37.008936  32.895151  37.96870  33.57688  33.76634   \n",
       "180  32.170042  33.739764  35.937812  33.404526  38.75226  32.10887  33.23928   \n",
       "\n",
       "        HSPB1     HSPD1       IL6  ...      RELA    RNF139     SETD2  \\\n",
       "0    38.25591  37.02204  29.74353  ...  33.09884  32.46554  32.58565   \n",
       "1    37.41814  36.48920  24.29608  ...  33.06941  32.27190  33.19915   \n",
       "2    37.94992  36.51745  33.97471  ...  33.64965  32.55514  32.84628   \n",
       "3    37.86790  37.13423  31.49884  ...  33.14439  33.19823  33.68316   \n",
       "4    35.26187  36.15876  33.92214  ...  32.91133  30.89813  34.63036   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "176  37.34151  35.52103  34.47504  ...  33.45578  32.12573  33.34867   \n",
       "177  38.71080  36.75353  32.12763  ...  32.33496  34.27276  32.16275   \n",
       "178  38.35448  35.94739  29.04315  ...  33.26864  32.92305  34.01015   \n",
       "179  36.74006  35.82772  29.64267  ...  33.53102  31.87160  33.23246   \n",
       "180  37.84644  37.90058  33.21729  ...  33.56852  32.47268  32.81781   \n",
       "\n",
       "       SLC2A1      SOD2      TGM2      TP53      TSC1      TSC2       VHL  \n",
       "0    33.38586  38.67433  38.50142  33.83518  32.93402  34.93520  32.30615  \n",
       "1    33.69538  38.64559  34.33752  34.44810  33.16630  35.08304  32.19988  \n",
       "2    36.23588  40.50559  35.50178  35.41980  33.63282  34.79244  31.49147  \n",
       "3    34.41938  38.99231  35.77236  34.18862  32.88250  35.02014  32.11538  \n",
       "4    34.59911  38.41437  33.47112  34.91241  33.44515  35.01310  33.33646  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "176  36.50807  35.15898  34.57504  35.39631  32.93248  35.12781  31.79913  \n",
       "177  33.97705  38.85295  32.38354  32.04003  32.62658  33.78873  31.66344  \n",
       "178  34.85694  37.96021  36.65499  33.34126  32.81059  35.24316  32.39461  \n",
       "179  34.24055  37.24924  36.84744  34.98283  34.04810  35.60526  32.34561  \n",
       "180  35.99620  38.54211  37.23935  33.82151  33.82576  35.13995  30.34566  \n",
       "\n",
       "[181 rows x 28 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes = pd.read_csv('..\\..\\Data\\PPT-Ohmnet\\mRCC_big_pool\\mrcc_protein_matrix_80_genes_28_nodes.csv')\n",
    "Y = genes.Y\n",
    "\n",
    "genes = genes.iloc[:,1:29] \n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APAF1</th>\n",
       "      <th>ARID1A</th>\n",
       "      <th>ATM</th>\n",
       "      <th>BAP1</th>\n",
       "      <th>EPAS1</th>\n",
       "      <th>ERBB2</th>\n",
       "      <th>GSTP1</th>\n",
       "      <th>HSPB1</th>\n",
       "      <th>HSPD1</th>\n",
       "      <th>IL6</th>\n",
       "      <th>...</th>\n",
       "      <th>RELA</th>\n",
       "      <th>RNF139</th>\n",
       "      <th>SETD2</th>\n",
       "      <th>SLC2A1</th>\n",
       "      <th>SOD2</th>\n",
       "      <th>TGM2</th>\n",
       "      <th>TP53</th>\n",
       "      <th>TSC1</th>\n",
       "      <th>TSC2</th>\n",
       "      <th>VHL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.610274</td>\n",
       "      <td>0.474298</td>\n",
       "      <td>0.551095</td>\n",
       "      <td>0.703386</td>\n",
       "      <td>0.614968</td>\n",
       "      <td>0.879366</td>\n",
       "      <td>0.697909</td>\n",
       "      <td>0.655601</td>\n",
       "      <td>0.643912</td>\n",
       "      <td>0.477940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368870</td>\n",
       "      <td>0.547741</td>\n",
       "      <td>0.361620</td>\n",
       "      <td>0.420160</td>\n",
       "      <td>0.542412</td>\n",
       "      <td>0.945549</td>\n",
       "      <td>0.403803</td>\n",
       "      <td>0.411780</td>\n",
       "      <td>0.408244</td>\n",
       "      <td>0.681580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.536117</td>\n",
       "      <td>0.472846</td>\n",
       "      <td>0.561963</td>\n",
       "      <td>0.465055</td>\n",
       "      <td>0.796869</td>\n",
       "      <td>0.573713</td>\n",
       "      <td>0.653713</td>\n",
       "      <td>0.472156</td>\n",
       "      <td>0.521794</td>\n",
       "      <td>0.081270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360624</td>\n",
       "      <td>0.504091</td>\n",
       "      <td>0.518369</td>\n",
       "      <td>0.458930</td>\n",
       "      <td>0.538144</td>\n",
       "      <td>0.301997</td>\n",
       "      <td>0.538341</td>\n",
       "      <td>0.474109</td>\n",
       "      <td>0.451980</td>\n",
       "      <td>0.664154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.419872</td>\n",
       "      <td>0.367512</td>\n",
       "      <td>0.610698</td>\n",
       "      <td>0.401843</td>\n",
       "      <td>0.455951</td>\n",
       "      <td>0.511285</td>\n",
       "      <td>0.509070</td>\n",
       "      <td>0.588599</td>\n",
       "      <td>0.528269</td>\n",
       "      <td>0.786044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523208</td>\n",
       "      <td>0.567938</td>\n",
       "      <td>0.428211</td>\n",
       "      <td>0.777154</td>\n",
       "      <td>0.814317</td>\n",
       "      <td>0.481939</td>\n",
       "      <td>0.751632</td>\n",
       "      <td>0.599295</td>\n",
       "      <td>0.366011</td>\n",
       "      <td>0.547991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.595169</td>\n",
       "      <td>0.586761</td>\n",
       "      <td>0.702007</td>\n",
       "      <td>0.292727</td>\n",
       "      <td>0.928781</td>\n",
       "      <td>0.479723</td>\n",
       "      <td>0.512613</td>\n",
       "      <td>0.570639</td>\n",
       "      <td>0.669624</td>\n",
       "      <td>0.605757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381633</td>\n",
       "      <td>0.712900</td>\n",
       "      <td>0.642034</td>\n",
       "      <td>0.549619</td>\n",
       "      <td>0.589625</td>\n",
       "      <td>0.523759</td>\n",
       "      <td>0.481384</td>\n",
       "      <td>0.397955</td>\n",
       "      <td>0.433372</td>\n",
       "      <td>0.650298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.817191</td>\n",
       "      <td>0.314590</td>\n",
       "      <td>0.756387</td>\n",
       "      <td>0.770463</td>\n",
       "      <td>0.727417</td>\n",
       "      <td>0.515014</td>\n",
       "      <td>0.331552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446063</td>\n",
       "      <td>0.782216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316330</td>\n",
       "      <td>0.194423</td>\n",
       "      <td>0.884044</td>\n",
       "      <td>0.572132</td>\n",
       "      <td>0.503813</td>\n",
       "      <td>0.168091</td>\n",
       "      <td>0.640258</td>\n",
       "      <td>0.548936</td>\n",
       "      <td>0.431290</td>\n",
       "      <td>0.850528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.595723</td>\n",
       "      <td>0.566255</td>\n",
       "      <td>0.403055</td>\n",
       "      <td>0.741776</td>\n",
       "      <td>0.860390</td>\n",
       "      <td>0.557095</td>\n",
       "      <td>0.578371</td>\n",
       "      <td>0.455376</td>\n",
       "      <td>0.299906</td>\n",
       "      <td>0.822477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468885</td>\n",
       "      <td>0.471142</td>\n",
       "      <td>0.556572</td>\n",
       "      <td>0.811248</td>\n",
       "      <td>0.020453</td>\n",
       "      <td>0.338707</td>\n",
       "      <td>0.746476</td>\n",
       "      <td>0.411366</td>\n",
       "      <td>0.465225</td>\n",
       "      <td>0.598440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.823141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510581</td>\n",
       "      <td>0.436667</td>\n",
       "      <td>0.179353</td>\n",
       "      <td>0.105657</td>\n",
       "      <td>0.788844</td>\n",
       "      <td>0.755208</td>\n",
       "      <td>0.582374</td>\n",
       "      <td>0.651544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154831</td>\n",
       "      <td>0.955115</td>\n",
       "      <td>0.253569</td>\n",
       "      <td>0.494212</td>\n",
       "      <td>0.568933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.329281</td>\n",
       "      <td>0.069080</td>\n",
       "      <td>0.576190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.755339</td>\n",
       "      <td>0.561399</td>\n",
       "      <td>0.566677</td>\n",
       "      <td>0.573847</td>\n",
       "      <td>0.605671</td>\n",
       "      <td>0.373411</td>\n",
       "      <td>0.530551</td>\n",
       "      <td>0.677185</td>\n",
       "      <td>0.397621</td>\n",
       "      <td>0.426940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416448</td>\n",
       "      <td>0.650870</td>\n",
       "      <td>0.725580</td>\n",
       "      <td>0.604427</td>\n",
       "      <td>0.436379</td>\n",
       "      <td>0.660174</td>\n",
       "      <td>0.295386</td>\n",
       "      <td>0.378658</td>\n",
       "      <td>0.499349</td>\n",
       "      <td>0.696085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.694893</td>\n",
       "      <td>0.469721</td>\n",
       "      <td>0.804191</td>\n",
       "      <td>0.523132</td>\n",
       "      <td>0.617171</td>\n",
       "      <td>0.547024</td>\n",
       "      <td>0.248326</td>\n",
       "      <td>0.323678</td>\n",
       "      <td>0.370194</td>\n",
       "      <td>0.470596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489967</td>\n",
       "      <td>0.413858</td>\n",
       "      <td>0.526880</td>\n",
       "      <td>0.527218</td>\n",
       "      <td>0.330815</td>\n",
       "      <td>0.689918</td>\n",
       "      <td>0.655716</td>\n",
       "      <td>0.710731</td>\n",
       "      <td>0.606470</td>\n",
       "      <td>0.688050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.498633</td>\n",
       "      <td>0.439478</td>\n",
       "      <td>0.549999</td>\n",
       "      <td>0.640524</td>\n",
       "      <td>0.780118</td>\n",
       "      <td>0.267523</td>\n",
       "      <td>0.161034</td>\n",
       "      <td>0.565940</td>\n",
       "      <td>0.845258</td>\n",
       "      <td>0.730891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500475</td>\n",
       "      <td>0.549350</td>\n",
       "      <td>0.420937</td>\n",
       "      <td>0.747131</td>\n",
       "      <td>0.522780</td>\n",
       "      <td>0.750490</td>\n",
       "      <td>0.400802</td>\n",
       "      <td>0.651068</td>\n",
       "      <td>0.468816</td>\n",
       "      <td>0.360103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        APAF1    ARID1A       ATM      BAP1     EPAS1     ERBB2     GSTP1  \\\n",
       "0    0.610274  0.474298  0.551095  0.703386  0.614968  0.879366  0.697909   \n",
       "1    0.536117  0.472846  0.561963  0.465055  0.796869  0.573713  0.653713   \n",
       "2    0.419872  0.367512  0.610698  0.401843  0.455951  0.511285  0.509070   \n",
       "3    0.595169  0.586761  0.702007  0.292727  0.928781  0.479723  0.512613   \n",
       "4    0.817191  0.314590  0.756387  0.770463  0.727417  0.515014  0.331552   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "176  0.595723  0.566255  0.403055  0.741776  0.860390  0.557095  0.578371   \n",
       "177  0.823141  0.000000  0.510581  0.436667  0.179353  0.105657  0.788844   \n",
       "178  0.755339  0.561399  0.566677  0.573847  0.605671  0.373411  0.530551   \n",
       "179  0.694893  0.469721  0.804191  0.523132  0.617171  0.547024  0.248326   \n",
       "180  0.498633  0.439478  0.549999  0.640524  0.780118  0.267523  0.161034   \n",
       "\n",
       "        HSPB1     HSPD1       IL6  ...      RELA    RNF139     SETD2  \\\n",
       "0    0.655601  0.643912  0.477940  ...  0.368870  0.547741  0.361620   \n",
       "1    0.472156  0.521794  0.081270  ...  0.360624  0.504091  0.518369   \n",
       "2    0.588599  0.528269  0.786044  ...  0.523208  0.567938  0.428211   \n",
       "3    0.570639  0.669624  0.605757  ...  0.381633  0.712900  0.642034   \n",
       "4    0.000000  0.446063  0.782216  ...  0.316330  0.194423  0.884044   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "176  0.455376  0.299906  0.822477  ...  0.468885  0.471142  0.556572   \n",
       "177  0.755208  0.582374  0.651544  ...  0.154831  0.955115  0.253569   \n",
       "178  0.677185  0.397621  0.426940  ...  0.416448  0.650870  0.725580   \n",
       "179  0.323678  0.370194  0.470596  ...  0.489967  0.413858  0.526880   \n",
       "180  0.565940  0.845258  0.730891  ...  0.500475  0.549350  0.420937   \n",
       "\n",
       "       SLC2A1      SOD2      TGM2      TP53      TSC1      TSC2       VHL  \n",
       "0    0.420160  0.542412  0.945549  0.403803  0.411780  0.408244  0.681580  \n",
       "1    0.458930  0.538144  0.301997  0.538341  0.474109  0.451980  0.664154  \n",
       "2    0.777154  0.814317  0.481939  0.751632  0.599295  0.366011  0.547991  \n",
       "3    0.549619  0.589625  0.523759  0.481384  0.397955  0.433372  0.650298  \n",
       "4    0.572132  0.503813  0.168091  0.640258  0.548936  0.431290  0.850528  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "176  0.811248  0.020453  0.338707  0.746476  0.411366  0.465225  0.598440  \n",
       "177  0.494212  0.568933  0.000000  0.009761  0.329281  0.069080  0.576190  \n",
       "178  0.604427  0.436379  0.660174  0.295386  0.378658  0.499349  0.696085  \n",
       "179  0.527218  0.330815  0.689918  0.655716  0.710731  0.606470  0.688050  \n",
       "180  0.747131  0.522780  0.750490  0.400802  0.651068  0.468816  0.360103  \n",
       "\n",
       "[181 rows x 28 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = genes.columns\n",
    "d = scaler.fit_transform(genes)\n",
    "genes = pd.DataFrame(d, columns=names)\n",
    "genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='../../Data/PPT-Ohmnet/mRCC_big_pool/network_edges_mrcc_80_genes_28_nodes.tsv'\n",
    "data = pd.read_csv(path, delimiter='\\t')\n",
    "edge_index1=data[data.columns[1]].to_numpy()\n",
    "edge_index2=data[data.columns[2]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = np.concatenate((edge_index1, edge_index2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['VHL', 'VHL', 'VHL', 'VHL', 'VHL', 'VHL', 'VHL', 'GSTP1', 'GSTP1',\n",
       "       'TGM2', 'TGM2', 'SETD2', 'ERBB2', 'ERBB2', 'NDRG1', 'NF2', 'NF2',\n",
       "       'PIK3CA', 'MTOR', 'MTOR', 'APAF1', 'TSC1', 'RELA', 'RELA', 'RELA',\n",
       "       'ATM', 'MAPK8', 'PTEN', 'PTEN', 'ARID1A', 'PTGS2', 'HSPB1',\n",
       "       'HSPD1', 'SLC2A1', 'RNF139', 'ATM', 'TGM2', 'TP53', 'SOD2',\n",
       "       'EPAS1', 'TGM2', 'MAPK8', 'RELA', 'PAK1', 'TP53', 'NF2', 'PAK1',\n",
       "       'TP53', 'PAK1', 'TSC1', 'PTEN', 'TP53', 'MAPK8', 'TP53', 'TSC2',\n",
       "       'IL6', 'TP53', 'ATM', 'TP53', 'TP53', 'TP53', 'BAP1', 'TP53',\n",
       "       'TP53', 'TP53', 'PTEN'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(edge_index)\n",
    "len(list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index1 = le.transform(edge_index1)\n",
    "edge_index2 = le.transform(edge_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = [edge_index1]+[edge_index2]\n",
    "edge_index = np.array(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27, 27, 27, 27, 27, 27, 27,  6,  6, 23, 23, 20,  5,  5, 12, 13,\n",
       "        13, 15, 11, 11,  0, 25, 18, 18, 18,  2, 10, 16, 16,  1, 17,  7,\n",
       "         8],\n",
       "       [21, 19,  2, 23, 24, 22,  4, 23, 10, 18, 14, 24, 13, 14, 24, 14,\n",
       "        25, 16, 24, 10, 24, 26,  9, 24,  2, 24, 24, 24,  3, 24, 24, 24,\n",
       "        16]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27, 27, 27, 27, 27, 27, 27,  6,  6, 23, 23, 20,  5,  5, 12, 13, 13, 15,\n",
       "         11, 11,  0, 25, 18, 18, 18,  2, 10, 16, 16,  1, 17,  7,  8],\n",
       "        [21, 19,  2, 23, 24, 22,  4, 23, 10, 18, 14, 24, 13, 14, 24, 14, 25, 16,\n",
       "         24, 10, 24, 26,  9, 24,  2, 24, 24, 24,  3, 24, 24, 24, 16]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edge_index, dtype=torch.int64)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[28], edge_index=[2, 33], y=[1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_4456/1559691492.py:11: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  x = torch.tensor([b], dtype=torch.long).reshape([-1])\n"
     ]
    }
   ],
   "source": [
    "list_data_0=[]\n",
    "list_data_1=[]\n",
    "\n",
    "for g in range(len(genes)):\n",
    "  b=[]\n",
    "  for i in genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    # a.append(Y[g])\n",
    "    a.append(i*100)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.long).reshape([-1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y.iloc[g]], dtype=torch.float).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  if y == 0:\n",
    "    list_data_0.append(data)\n",
    "  else:\n",
    "    list_data_1.append(data)\n",
    "\n",
    "print(list_data_0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Patient sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 28\n",
      "Number of charcateristics per node: 1\n",
      "Number of edges: 33\n",
      "Average node degree: 1.18\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n"
     ]
    }
   ],
   "source": [
    "data = list_data_0[0]\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of charcateristics per node: {data.num_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Train-Test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 153\n",
      "Number of test graphs: 28\n",
      "Negative cases from train: 72 of 153 = 0.47058823529411764\n",
      "Negative cases from test: 13 of 28 = 0.4642857142857143\n",
      "It should be 46.9\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(1255)\n",
    "# random.seed(125)\n",
    "# random.shuffle(list_data_0)\n",
    "# random.shuffle(list_data_1)\n",
    "\n",
    "train_dataset = list_data_0[0:72]\n",
    "test_dataset = list_data_0[72:86]\n",
    "train_dataset = train_dataset + list_data_1[0:81]\n",
    "test_dataset = test_dataset + list_data_1[81:97]\n",
    "# random.shuffle(train_dataset)\n",
    "# random.shuffle(test_dataset)\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "cont = 0\n",
    "cont1=0\n",
    "for i in train_dataset:\n",
    "    if i.y == 0:\n",
    "        cont+=1\n",
    "for i in test_dataset:\n",
    "    if i.y == 0:\n",
    "        cont1+=1\n",
    "print(\"Negative cases from train: \" + str(cont) + \" of \" + str(len(train_dataset)) + \" = \" + str(cont/len(train_dataset)))\n",
    "print(\"Negative cases from test: \" + str(cont1) + \" of \" + str(len(test_dataset)) + \" = \" + str(cont1/len(test_dataset)))\n",
    "print(\"It should be 46.9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[896], edge_index=[2, 1056], y=[32, 1], batch=[896], ptr=[33])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[896], edge_index=[2, 1056], y=[32, 1], batch=[896], ptr=[33])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[896], edge_index=[2, 1056], y=[32, 1], batch=[896], ptr=[33])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[896], edge_index=[2, 1056], y=[32, 1], batch=[896], ptr=[33])\n",
      "\n",
      "Step 5:\n",
      "=======\n",
      "Number of graphs in the current batch: 25\n",
      "DataBatch(x=[700], edge_index=[2, 825], y=[25, 1], batch=[700], ptr=[26])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import SAGPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 28\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.dim = dim\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GraphConv(embed_dim, dim)\n",
    "        self.pool1 = SAGPooling(dim, ratio=0.5)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.pool2 = SAGPooling(dim, ratio=0.5)\n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=101, embedding_dim=embed_dim)\n",
    "        self.lin1 = torch.nn.Linear(448, 100)\n",
    "        self.lin2 = torch.nn.Linear(100, 10)\n",
    "        self.lin3 = torch.nn.Linear(100, 1)\n",
    "        self.act1 = torch.nn.RReLU()\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = torch.tensor(x) #.to(torch.int)\n",
    "        # print(x.long())\n",
    "        x = self.item_embedding(x)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        # x = F.relu(self.conv2(x, edge_index))\n",
    "        # x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        # x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 #+ x2\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        # x = self.lin2(x)\n",
    "        # x = self.act1(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = torch.sigmoid(self.lin3(x)).squeeze(1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(output, data.y.squeeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        for i in range(len(output)):\n",
    "            if output[i]>0.5:\n",
    "                output[i]=1\n",
    "            else:\n",
    "                output[i]=0\n",
    "            if output[i]==data.y[i]:\n",
    "                correct=correct+1\n",
    "    # print(\"Correct: \"+str(correct) +\" of \"+str(len(loader.dataset)))\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): GraphConv(28, 224)\n",
      "  (pool1): SAGPooling(GraphConv, 224, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(224, 224)\n",
      "  (pool2): SAGPooling(GraphConv, 224, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 28)\n",
      "  (lin1): Linear(in_features=448, out_features=100, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.8706, Train Acc: 0.5294, Test Acc: 0.5357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\AppData\\Local\\Temp/ipykernel_4456/4260732989.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x) #.to(torch.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.7180, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 003, Loss: 0.7077, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 004, Loss: 0.7074, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 005, Loss: 0.7057, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 006, Loss: 0.7003, Train Acc: 0.5359, Test Acc: 0.5714\n",
      "Epoch: 007, Loss: 0.6990, Train Acc: 0.5359, Test Acc: 0.5714\n",
      "Epoch: 008, Loss: 0.7005, Train Acc: 0.5490, Test Acc: 0.5714\n",
      "Epoch: 009, Loss: 0.6964, Train Acc: 0.5556, Test Acc: 0.5714\n",
      "Epoch: 010, Loss: 0.6919, Train Acc: 0.5621, Test Acc: 0.5714\n",
      "Epoch: 011, Loss: 0.6909, Train Acc: 0.5817, Test Acc: 0.5714\n",
      "Epoch: 012, Loss: 0.7063, Train Acc: 0.5556, Test Acc: 0.5714\n",
      "Epoch: 013, Loss: 0.7039, Train Acc: 0.5556, Test Acc: 0.5714\n",
      "Epoch: 014, Loss: 0.6960, Train Acc: 0.6013, Test Acc: 0.5357\n",
      "Epoch: 015, Loss: 0.6730, Train Acc: 0.6144, Test Acc: 0.5714\n",
      "Epoch: 016, Loss: 0.6592, Train Acc: 0.6471, Test Acc: 0.5357\n",
      "Epoch: 017, Loss: 0.6354, Train Acc: 0.6275, Test Acc: 0.5714\n",
      "Epoch: 018, Loss: 0.7183, Train Acc: 0.5686, Test Acc: 0.5714\n",
      "Epoch: 019, Loss: 0.6764, Train Acc: 0.6601, Test Acc: 0.5714\n",
      "Epoch: 020, Loss: 0.6217, Train Acc: 0.6732, Test Acc: 0.5714\n",
      "Epoch: 021, Loss: 0.6072, Train Acc: 0.7059, Test Acc: 0.5357\n",
      "Epoch: 022, Loss: 0.5777, Train Acc: 0.6993, Test Acc: 0.5357\n",
      "Epoch: 023, Loss: 0.5691, Train Acc: 0.6993, Test Acc: 0.5714\n",
      "Epoch: 024, Loss: 0.5702, Train Acc: 0.7255, Test Acc: 0.5714\n",
      "Epoch: 025, Loss: 0.5401, Train Acc: 0.7386, Test Acc: 0.5714\n",
      "Epoch: 026, Loss: 0.5709, Train Acc: 0.6928, Test Acc: 0.5357\n",
      "Epoch: 027, Loss: 0.5881, Train Acc: 0.7059, Test Acc: 0.5714\n",
      "Epoch: 028, Loss: 0.5608, Train Acc: 0.7451, Test Acc: 0.5714\n",
      "Epoch: 029, Loss: 0.5102, Train Acc: 0.7843, Test Acc: 0.5714\n",
      "Epoch: 030, Loss: 0.4835, Train Acc: 0.7778, Test Acc: 0.5714\n",
      "Epoch: 031, Loss: 0.4796, Train Acc: 0.7908, Test Acc: 0.5714\n",
      "Epoch: 032, Loss: 0.4541, Train Acc: 0.7908, Test Acc: 0.5714\n",
      "Epoch: 033, Loss: 0.4723, Train Acc: 0.7778, Test Acc: 0.5714\n",
      "Epoch: 034, Loss: 0.4760, Train Acc: 0.7908, Test Acc: 0.5714\n",
      "Epoch: 035, Loss: 0.4483, Train Acc: 0.7778, Test Acc: 0.5714\n",
      "Epoch: 036, Loss: 0.4653, Train Acc: 0.7843, Test Acc: 0.5714\n",
      "Epoch: 037, Loss: 0.4564, Train Acc: 0.7516, Test Acc: 0.5714\n",
      "Epoch: 038, Loss: 0.4503, Train Acc: 0.7124, Test Acc: 0.5714\n",
      "Epoch: 039, Loss: 0.4803, Train Acc: 0.7255, Test Acc: 0.5357\n",
      "Epoch: 040, Loss: 0.4859, Train Acc: 0.7320, Test Acc: 0.5714\n",
      "Epoch: 041, Loss: 0.4751, Train Acc: 0.7908, Test Acc: 0.5714\n",
      "Epoch: 042, Loss: 0.4340, Train Acc: 0.8235, Test Acc: 0.5714\n",
      "Epoch: 043, Loss: 0.3973, Train Acc: 0.7908, Test Acc: 0.5714\n",
      "Epoch: 044, Loss: 0.4331, Train Acc: 0.8301, Test Acc: 0.5714\n",
      "Epoch: 045, Loss: 0.8258, Train Acc: 0.7843, Test Acc: 0.5357\n",
      "Epoch: 046, Loss: 0.4214, Train Acc: 0.8497, Test Acc: 0.5714\n",
      "Epoch: 047, Loss: 0.3825, Train Acc: 0.8497, Test Acc: 0.5357\n",
      "Epoch: 048, Loss: 0.3559, Train Acc: 0.8627, Test Acc: 0.5714\n",
      "Epoch: 049, Loss: 0.3351, Train Acc: 0.8889, Test Acc: 0.5714\n",
      "Epoch: 050, Loss: 0.3039, Train Acc: 0.8954, Test Acc: 0.6071\n",
      "Epoch: 051, Loss: 0.2861, Train Acc: 0.8954, Test Acc: 0.6071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 052, Loss: 0.2726, Train Acc: 0.9085, Test Acc: 0.6071\n",
      "Net(\n",
      "  (conv1): GraphConv(28, 224)\n",
      "  (pool1): SAGPooling(GraphConv, 224, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(224, 224)\n",
      "  (pool2): SAGPooling(GraphConv, 224, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 28)\n",
      "  (lin1): Linear(in_features=448, out_features=100, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.8747, Train Acc: 0.5163, Test Acc: 0.5357\n",
      "Epoch: 002, Loss: 0.7085, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 003, Loss: 0.7015, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 004, Loss: 0.6985, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 005, Loss: 0.6979, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 006, Loss: 0.6971, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 007, Loss: 0.6942, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 008, Loss: 0.6880, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 009, Loss: 0.7217, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 010, Loss: 0.6840, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 011, Loss: 0.6834, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 012, Loss: 0.6835, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 013, Loss: 0.6804, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 014, Loss: 0.7546, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 015, Loss: 0.6500, Train Acc: 0.5556, Test Acc: 0.5357\n",
      "Epoch: 016, Loss: 0.6476, Train Acc: 0.5490, Test Acc: 0.5357\n",
      "Epoch: 017, Loss: 0.6899, Train Acc: 0.5817, Test Acc: 0.5714\n",
      "Epoch: 018, Loss: 0.6462, Train Acc: 0.6340, Test Acc: 0.5714\n",
      "Epoch: 019, Loss: 0.6267, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 020, Loss: 0.6513, Train Acc: 0.6340, Test Acc: 0.5714\n",
      "Epoch: 021, Loss: 0.6100, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 022, Loss: 0.7570, Train Acc: 0.6471, Test Acc: 0.5714\n",
      "Epoch: 023, Loss: 0.6036, Train Acc: 0.6471, Test Acc: 0.5000\n",
      "Epoch: 024, Loss: 0.5989, Train Acc: 0.6993, Test Acc: 0.5000\n",
      "Epoch: 025, Loss: 0.5865, Train Acc: 0.5752, Test Acc: 0.5357\n",
      "Epoch: 026, Loss: 0.5906, Train Acc: 0.6797, Test Acc: 0.5357\n",
      "Epoch: 027, Loss: 0.5564, Train Acc: 0.6863, Test Acc: 0.5714\n",
      "Epoch: 028, Loss: 0.5335, Train Acc: 0.6013, Test Acc: 0.5357\n",
      "Epoch: 029, Loss: 0.6652, Train Acc: 0.7124, Test Acc: 0.5357\n",
      "Epoch: 030, Loss: 0.5226, Train Acc: 0.7255, Test Acc: 0.5000\n",
      "Epoch: 031, Loss: 0.4912, Train Acc: 0.7386, Test Acc: 0.5000\n",
      "Epoch: 032, Loss: 0.4805, Train Acc: 0.7386, Test Acc: 0.5357\n",
      "Epoch: 033, Loss: 0.4874, Train Acc: 0.7843, Test Acc: 0.5000\n",
      "Epoch: 034, Loss: 0.4368, Train Acc: 0.7843, Test Acc: 0.5357\n",
      "Epoch: 035, Loss: 0.3985, Train Acc: 0.7908, Test Acc: 0.6429\n",
      "Epoch: 036, Loss: 0.4043, Train Acc: 0.7974, Test Acc: 0.6071\n",
      "Epoch: 037, Loss: 0.3644, Train Acc: 0.8105, Test Acc: 0.6071\n",
      "Epoch: 038, Loss: 0.3934, Train Acc: 0.6013, Test Acc: 0.5357\n",
      "Epoch: 039, Loss: 0.4443, Train Acc: 0.7582, Test Acc: 0.6429\n",
      "Epoch: 040, Loss: 0.4951, Train Acc: 0.8366, Test Acc: 0.6786\n",
      "Epoch: 041, Loss: 0.2859, Train Acc: 0.8497, Test Acc: 0.6429\n",
      "Epoch: 042, Loss: 0.2485, Train Acc: 0.8170, Test Acc: 0.6071\n",
      "Epoch: 043, Loss: 0.2773, Train Acc: 0.8693, Test Acc: 0.6429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 044, Loss: 0.3322, Train Acc: 0.8366, Test Acc: 0.5714\n",
      "Epoch: 045, Loss: 0.2520, Train Acc: 0.9085, Test Acc: 0.5357\n",
      "Net(\n",
      "  (conv1): GraphConv(28, 224)\n",
      "  (pool1): SAGPooling(GraphConv, 224, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(224, 224)\n",
      "  (pool2): SAGPooling(GraphConv, 224, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 28)\n",
      "  (lin1): Linear(in_features=448, out_features=100, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.9620, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 002, Loss: 0.8161, Train Acc: 0.4967, Test Acc: 0.5357\n",
      "Epoch: 003, Loss: 0.7005, Train Acc: 0.5359, Test Acc: 0.5714\n",
      "Epoch: 004, Loss: 0.6985, Train Acc: 0.5621, Test Acc: 0.7143\n",
      "Epoch: 005, Loss: 0.6975, Train Acc: 0.5752, Test Acc: 0.6071\n",
      "Epoch: 006, Loss: 0.6951, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 007, Loss: 0.6959, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 008, Loss: 0.6923, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 009, Loss: 0.6906, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 010, Loss: 0.6860, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 011, Loss: 0.6917, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 012, Loss: 0.6882, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 013, Loss: 0.6849, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 014, Loss: 0.6715, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 015, Loss: 0.7678, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 016, Loss: 0.6536, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 017, Loss: 0.6509, Train Acc: 0.5490, Test Acc: 0.5357\n",
      "Epoch: 018, Loss: 0.6510, Train Acc: 0.5490, Test Acc: 0.5357\n",
      "Epoch: 019, Loss: 0.6197, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 020, Loss: 0.7033, Train Acc: 0.5556, Test Acc: 0.5714\n",
      "Epoch: 021, Loss: 0.6585, Train Acc: 0.5556, Test Acc: 0.6071\n",
      "Epoch: 022, Loss: 0.5927, Train Acc: 0.5556, Test Acc: 0.5357\n",
      "Epoch: 023, Loss: 0.5811, Train Acc: 0.5686, Test Acc: 0.5357\n",
      "Epoch: 024, Loss: 0.5745, Train Acc: 0.5817, Test Acc: 0.5357\n",
      "Epoch: 025, Loss: 0.5497, Train Acc: 0.6209, Test Acc: 0.5714\n",
      "Epoch: 026, Loss: 0.5761, Train Acc: 0.6340, Test Acc: 0.6071\n",
      "Epoch: 027, Loss: 0.5703, Train Acc: 0.6405, Test Acc: 0.6071\n",
      "Epoch: 028, Loss: 0.5048, Train Acc: 0.6471, Test Acc: 0.5714\n",
      "Epoch: 029, Loss: 0.4435, Train Acc: 0.5686, Test Acc: 0.5357\n",
      "Epoch: 030, Loss: 0.7682, Train Acc: 0.7124, Test Acc: 0.6071\n",
      "Epoch: 031, Loss: 0.4429, Train Acc: 0.7712, Test Acc: 0.6429\n",
      "Epoch: 032, Loss: 0.3965, Train Acc: 0.8105, Test Acc: 0.6429\n",
      "Epoch: 033, Loss: 0.3667, Train Acc: 0.7908, Test Acc: 0.5357\n",
      "Epoch: 034, Loss: 0.3791, Train Acc: 0.8627, Test Acc: 0.5714\n",
      "Epoch: 035, Loss: 0.3369, Train Acc: 0.8235, Test Acc: 0.5714\n",
      "Epoch: 036, Loss: 0.3539, Train Acc: 0.8824, Test Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 037, Loss: 0.2832, Train Acc: 0.8627, Test Acc: 0.5714\n",
      "Epoch: 038, Loss: 0.2867, Train Acc: 0.9150, Test Acc: 0.6429\n",
      "Net(\n",
      "  (conv1): GraphConv(28, 224)\n",
      "  (pool1): SAGPooling(GraphConv, 224, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(224, 224)\n",
      "  (pool2): SAGPooling(GraphConv, 224, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 28)\n",
      "  (lin1): Linear(in_features=448, out_features=100, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 0.8040, Train Acc: 0.4902, Test Acc: 0.5000\n",
      "Epoch: 002, Loss: 0.7112, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 003, Loss: 0.6990, Train Acc: 0.5229, Test Acc: 0.5357\n",
      "Epoch: 004, Loss: 0.7014, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 005, Loss: 0.7000, Train Acc: 0.5229, Test Acc: 0.5357\n",
      "Epoch: 006, Loss: 0.6958, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 007, Loss: 0.6976, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 008, Loss: 0.7050, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 009, Loss: 0.6959, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 010, Loss: 0.6970, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 011, Loss: 0.7318, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 012, Loss: 0.6988, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 013, Loss: 0.6811, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 014, Loss: 0.7120, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 015, Loss: 0.7015, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 016, Loss: 0.6953, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 017, Loss: 0.6888, Train Acc: 0.5490, Test Acc: 0.5357\n",
      "Epoch: 018, Loss: 0.6887, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 019, Loss: 0.7317, Train Acc: 0.5752, Test Acc: 0.5357\n",
      "Epoch: 020, Loss: 0.6806, Train Acc: 0.5817, Test Acc: 0.5357\n",
      "Epoch: 021, Loss: 0.6710, Train Acc: 0.5686, Test Acc: 0.5357\n",
      "Epoch: 022, Loss: 0.6632, Train Acc: 0.5686, Test Acc: 0.5357\n",
      "Epoch: 023, Loss: 0.6935, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 024, Loss: 0.6848, Train Acc: 0.6144, Test Acc: 0.5357\n",
      "Epoch: 025, Loss: 0.6460, Train Acc: 0.5882, Test Acc: 0.5357\n",
      "Epoch: 026, Loss: 0.6526, Train Acc: 0.6078, Test Acc: 0.5714\n",
      "Epoch: 027, Loss: 0.6483, Train Acc: 0.6144, Test Acc: 0.5714\n",
      "Epoch: 028, Loss: 0.6372, Train Acc: 0.6144, Test Acc: 0.5357\n",
      "Epoch: 029, Loss: 0.6224, Train Acc: 0.6209, Test Acc: 0.5357\n",
      "Epoch: 030, Loss: 0.6315, Train Acc: 0.6144, Test Acc: 0.5357\n",
      "Epoch: 031, Loss: 0.6112, Train Acc: 0.6275, Test Acc: 0.5714\n",
      "Epoch: 032, Loss: 0.6046, Train Acc: 0.6340, Test Acc: 0.5714\n",
      "Epoch: 033, Loss: 0.7097, Train Acc: 0.6013, Test Acc: 0.5357\n",
      "Epoch: 034, Loss: 0.6229, Train Acc: 0.6405, Test Acc: 0.5714\n",
      "Epoch: 035, Loss: 0.5811, Train Acc: 0.6471, Test Acc: 0.5714\n",
      "Epoch: 036, Loss: 0.5576, Train Acc: 0.6601, Test Acc: 0.5714\n",
      "Epoch: 037, Loss: 0.5456, Train Acc: 0.6667, Test Acc: 0.5714\n",
      "Epoch: 038, Loss: 0.5533, Train Acc: 0.6340, Test Acc: 0.5357\n",
      "Epoch: 039, Loss: 0.5673, Train Acc: 0.6667, Test Acc: 0.5357\n",
      "Epoch: 040, Loss: 0.5469, Train Acc: 0.6471, Test Acc: 0.5357\n",
      "Epoch: 041, Loss: 0.5290, Train Acc: 0.6732, Test Acc: 0.5357\n",
      "Epoch: 042, Loss: 0.5222, Train Acc: 0.6993, Test Acc: 0.6071\n",
      "Epoch: 043, Loss: 0.5253, Train Acc: 0.7124, Test Acc: 0.5714\n",
      "Epoch: 044, Loss: 0.5292, Train Acc: 0.7059, Test Acc: 0.5357\n",
      "Epoch: 045, Loss: 0.5081, Train Acc: 0.6536, Test Acc: 0.5357\n",
      "Epoch: 046, Loss: 0.4971, Train Acc: 0.7451, Test Acc: 0.5357\n",
      "Epoch: 047, Loss: 0.4762, Train Acc: 0.7451, Test Acc: 0.5714\n",
      "Epoch: 048, Loss: 0.4717, Train Acc: 0.7190, Test Acc: 0.5357\n",
      "Epoch: 049, Loss: 0.4639, Train Acc: 0.7582, Test Acc: 0.6071\n",
      "Epoch: 050, Loss: 0.4457, Train Acc: 0.7451, Test Acc: 0.5357\n",
      "Epoch: 051, Loss: 0.4460, Train Acc: 0.7647, Test Acc: 0.5714\n",
      "Epoch: 052, Loss: 0.4167, Train Acc: 0.8105, Test Acc: 0.5714\n",
      "Epoch: 053, Loss: 0.4179, Train Acc: 0.8105, Test Acc: 0.5357\n",
      "Epoch: 054, Loss: 0.3981, Train Acc: 0.8039, Test Acc: 0.5357\n",
      "Epoch: 055, Loss: 0.4108, Train Acc: 0.7908, Test Acc: 0.6071\n",
      "Epoch: 056, Loss: 0.4202, Train Acc: 0.7451, Test Acc: 0.5714\n",
      "Epoch: 057, Loss: 0.4207, Train Acc: 0.7974, Test Acc: 0.5714\n",
      "Epoch: 058, Loss: 0.3605, Train Acc: 0.8170, Test Acc: 0.5357\n",
      "Epoch: 059, Loss: 0.3531, Train Acc: 0.8235, Test Acc: 0.5714\n",
      "Epoch: 060, Loss: 0.3455, Train Acc: 0.8170, Test Acc: 0.5714\n",
      "Epoch: 061, Loss: 0.3481, Train Acc: 0.8366, Test Acc: 0.6071\n",
      "Epoch: 062, Loss: 0.3393, Train Acc: 0.8170, Test Acc: 0.5357\n",
      "Epoch: 063, Loss: 0.3460, Train Acc: 0.8170, Test Acc: 0.5357\n",
      "Epoch: 064, Loss: 0.3232, Train Acc: 0.8366, Test Acc: 0.5714\n",
      "Epoch: 065, Loss: 0.3227, Train Acc: 0.8366, Test Acc: 0.5357\n",
      "Epoch: 066, Loss: 0.3807, Train Acc: 0.8301, Test Acc: 0.6429\n",
      "Epoch: 067, Loss: 0.3641, Train Acc: 0.8431, Test Acc: 0.5714\n",
      "Epoch: 068, Loss: 0.3226, Train Acc: 0.8431, Test Acc: 0.6071\n",
      "Epoch: 069, Loss: 0.2991, Train Acc: 0.8497, Test Acc: 0.6071\n",
      "Epoch: 070, Loss: 0.3030, Train Acc: 0.8497, Test Acc: 0.6071\n",
      "Epoch: 071, Loss: 0.2877, Train Acc: 0.8562, Test Acc: 0.6071\n",
      "Epoch: 072, Loss: 0.2774, Train Acc: 0.8562, Test Acc: 0.6071\n",
      "Epoch: 073, Loss: 0.2789, Train Acc: 0.8497, Test Acc: 0.6071\n",
      "Epoch: 074, Loss: 0.2704, Train Acc: 0.8497, Test Acc: 0.6071\n",
      "Epoch: 075, Loss: 0.2754, Train Acc: 0.8562, Test Acc: 0.6071\n",
      "Epoch: 076, Loss: 0.2715, Train Acc: 0.8627, Test Acc: 0.5714\n",
      "Epoch: 077, Loss: 0.2499, Train Acc: 0.8693, Test Acc: 0.5714\n",
      "Epoch: 078, Loss: 0.2408, Train Acc: 0.8693, Test Acc: 0.5714\n",
      "Epoch: 079, Loss: 0.2484, Train Acc: 0.8824, Test Acc: 0.5714\n",
      "Epoch: 080, Loss: 0.2386, Train Acc: 0.8758, Test Acc: 0.5714\n",
      "Epoch: 081, Loss: 0.2187, Train Acc: 0.8758, Test Acc: 0.5714\n",
      "Epoch: 082, Loss: 0.2285, Train Acc: 0.8627, Test Acc: 0.5714\n",
      "Epoch: 083, Loss: 0.2299, Train Acc: 0.8627, Test Acc: 0.6071\n",
      "Epoch: 084, Loss: 0.2842, Train Acc: 0.8824, Test Acc: 0.5357\n",
      "Epoch: 085, Loss: 0.2013, Train Acc: 0.8824, Test Acc: 0.5357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 086, Loss: 0.1877, Train Acc: 0.8889, Test Acc: 0.5000\n",
      "Epoch: 087, Loss: 0.1918, Train Acc: 0.8954, Test Acc: 0.5714\n",
      "Epoch: 088, Loss: 0.1766, Train Acc: 0.9020, Test Acc: 0.6786\n",
      "Net(\n",
      "  (conv1): GraphConv(28, 224)\n",
      "  (pool1): SAGPooling(GraphConv, 224, ratio=0.5, multiplier=1.0)\n",
      "  (conv2): GraphConv(224, 224)\n",
      "  (pool2): SAGPooling(GraphConv, 224, ratio=0.5, multiplier=1.0)\n",
      "  (item_embedding): Embedding(101, 28)\n",
      "  (lin1): Linear(in_features=448, out_features=100, bias=True)\n",
      "  (lin2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (lin3): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (act1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
      ")\n",
      "Epoch: 001, Loss: 1.1634, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 002, Loss: 1.7458, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 003, Loss: 0.8384, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 004, Loss: 0.6982, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 005, Loss: 0.6999, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 006, Loss: 0.6958, Train Acc: 0.5490, Test Acc: 0.5357\n",
      "Epoch: 007, Loss: 0.6955, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 008, Loss: 0.6939, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 009, Loss: 0.6914, Train Acc: 0.5490, Test Acc: 0.5357\n",
      "Epoch: 010, Loss: 0.6898, Train Acc: 0.5425, Test Acc: 0.5357\n",
      "Epoch: 011, Loss: 0.6887, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 012, Loss: 0.6902, Train Acc: 0.5359, Test Acc: 0.5357\n",
      "Epoch: 013, Loss: 0.6948, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 014, Loss: 0.7073, Train Acc: 0.5556, Test Acc: 0.5357\n",
      "Epoch: 015, Loss: 0.6902, Train Acc: 0.5556, Test Acc: 0.5357\n",
      "Epoch: 016, Loss: 0.6946, Train Acc: 0.5752, Test Acc: 0.5357\n",
      "Epoch: 017, Loss: 0.6617, Train Acc: 0.5817, Test Acc: 0.5357\n",
      "Epoch: 018, Loss: 0.6626, Train Acc: 0.5948, Test Acc: 0.5357\n",
      "Epoch: 019, Loss: 0.6436, Train Acc: 0.5294, Test Acc: 0.5357\n",
      "Epoch: 020, Loss: 1.0606, Train Acc: 0.5621, Test Acc: 0.5357\n",
      "Epoch: 021, Loss: 0.6721, Train Acc: 0.6144, Test Acc: 0.5357\n",
      "Epoch: 022, Loss: 0.6118, Train Acc: 0.6275, Test Acc: 0.5357\n",
      "Epoch: 023, Loss: 0.5971, Train Acc: 0.6667, Test Acc: 0.5000\n",
      "Epoch: 024, Loss: 0.5464, Train Acc: 0.6471, Test Acc: 0.5357\n",
      "Epoch: 025, Loss: 0.5838, Train Acc: 0.6275, Test Acc: 0.5357\n",
      "Epoch: 026, Loss: 0.5961, Train Acc: 0.5948, Test Acc: 0.5357\n",
      "Epoch: 027, Loss: 0.5457, Train Acc: 0.6667, Test Acc: 0.5357\n",
      "Epoch: 028, Loss: 0.5624, Train Acc: 0.7124, Test Acc: 0.5714\n",
      "Epoch: 029, Loss: 0.4510, Train Acc: 0.7451, Test Acc: 0.6071\n",
      "Epoch: 030, Loss: 0.4162, Train Acc: 0.8039, Test Acc: 0.6071\n",
      "Epoch: 031, Loss: 0.3642, Train Acc: 0.8105, Test Acc: 0.6071\n",
      "Epoch: 032, Loss: 0.4683, Train Acc: 0.7451, Test Acc: 0.6429\n",
      "Epoch: 033, Loss: 0.3333, Train Acc: 0.8235, Test Acc: 0.6429\n",
      "Epoch: 034, Loss: 0.3343, Train Acc: 0.8039, Test Acc: 0.6429\n",
      "Epoch: 035, Loss: 0.2952, Train Acc: 0.7843, Test Acc: 0.6071\n",
      "Epoch: 036, Loss: 0.3379, Train Acc: 0.8301, Test Acc: 0.5714\n",
      "Epoch: 037, Loss: 0.2633, Train Acc: 0.9020, Test Acc: 0.5714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6071428571428571\n",
      "Test stv: 0.050507627227610555\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test_avg = []\n",
    "for i in range(5):\n",
    "    model = Net(dim=224)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.6)\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(), lr=1)\n",
    "\n",
    "    train_epoch=[]\n",
    "    test_epoch=[]\n",
    "    epoch = 1\n",
    "    train_acc=0\n",
    "    while train_acc < 0.9 and epoch < 100:\n",
    "        loss = train(epoch)\n",
    "        train_acc = test(train_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        train_epoch.append(train_acc)\n",
    "        test_epoch.append(test_acc)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "            f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        epoch +=1\n",
    "\n",
    "    plt.plot(train_epoch, color=\"red\")\n",
    "    plt.plot(test_epoch, color=\"blue\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    test_avg.append(test_acc)\n",
    "\n",
    "print('Test accuracy: '+ str(np.array(test_avg).mean()))\n",
    "print('Test stv: '+ str(np.array(test_avg).std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb15f1e0f376981e7b6e1fc44ae8b8146823f10f258bcd6e448b0230b889fc06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
