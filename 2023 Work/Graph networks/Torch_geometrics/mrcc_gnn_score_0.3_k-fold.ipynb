{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1744,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cpu.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1745,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gene matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1746,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A3GALT2</th>\n",
       "      <th>AADAC</th>\n",
       "      <th>AAMP</th>\n",
       "      <th>AARD</th>\n",
       "      <th>AARSD1</th>\n",
       "      <th>AASDH</th>\n",
       "      <th>...</th>\n",
       "      <th>ZSCAN5B</th>\n",
       "      <th>ZSWIM2</th>\n",
       "      <th>ZSWIM3</th>\n",
       "      <th>ZSWIM7</th>\n",
       "      <th>ZUFSP</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.187983</td>\n",
       "      <td>33.643965</td>\n",
       "      <td>37.130851</td>\n",
       "      <td>22.869853</td>\n",
       "      <td>21.807184</td>\n",
       "      <td>22.446140</td>\n",
       "      <td>34.740118</td>\n",
       "      <td>21.710004</td>\n",
       "      <td>32.883502</td>\n",
       "      <td>31.650847</td>\n",
       "      <td>...</td>\n",
       "      <td>21.64163</td>\n",
       "      <td>21.42780</td>\n",
       "      <td>28.95039</td>\n",
       "      <td>31.76873</td>\n",
       "      <td>27.82477</td>\n",
       "      <td>33.26816</td>\n",
       "      <td>30.04056</td>\n",
       "      <td>33.81903</td>\n",
       "      <td>33.22469</td>\n",
       "      <td>31.23383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.483638</td>\n",
       "      <td>28.866645</td>\n",
       "      <td>37.945043</td>\n",
       "      <td>24.142100</td>\n",
       "      <td>21.807184</td>\n",
       "      <td>22.446140</td>\n",
       "      <td>34.265360</td>\n",
       "      <td>27.454455</td>\n",
       "      <td>33.407541</td>\n",
       "      <td>32.839872</td>\n",
       "      <td>...</td>\n",
       "      <td>29.27301</td>\n",
       "      <td>21.42780</td>\n",
       "      <td>28.35341</td>\n",
       "      <td>32.32573</td>\n",
       "      <td>30.30568</td>\n",
       "      <td>31.53633</td>\n",
       "      <td>29.94062</td>\n",
       "      <td>33.67262</td>\n",
       "      <td>33.10198</td>\n",
       "      <td>32.44643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.092851</td>\n",
       "      <td>26.684228</td>\n",
       "      <td>36.851946</td>\n",
       "      <td>30.406352</td>\n",
       "      <td>21.807184</td>\n",
       "      <td>22.446140</td>\n",
       "      <td>34.708417</td>\n",
       "      <td>21.710004</td>\n",
       "      <td>32.137371</td>\n",
       "      <td>32.721578</td>\n",
       "      <td>...</td>\n",
       "      <td>29.06719</td>\n",
       "      <td>21.42780</td>\n",
       "      <td>29.05322</td>\n",
       "      <td>32.87654</td>\n",
       "      <td>30.86495</td>\n",
       "      <td>30.74474</td>\n",
       "      <td>31.95812</td>\n",
       "      <td>33.96047</td>\n",
       "      <td>32.97135</td>\n",
       "      <td>32.20607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.975010</td>\n",
       "      <td>32.521494</td>\n",
       "      <td>38.453517</td>\n",
       "      <td>28.229357</td>\n",
       "      <td>26.484608</td>\n",
       "      <td>29.199458</td>\n",
       "      <td>35.123918</td>\n",
       "      <td>25.877077</td>\n",
       "      <td>33.514752</td>\n",
       "      <td>32.029916</td>\n",
       "      <td>...</td>\n",
       "      <td>24.01564</td>\n",
       "      <td>21.42780</td>\n",
       "      <td>29.98474</td>\n",
       "      <td>32.03101</td>\n",
       "      <td>31.17308</td>\n",
       "      <td>31.63196</td>\n",
       "      <td>30.71732</td>\n",
       "      <td>34.54254</td>\n",
       "      <td>33.24885</td>\n",
       "      <td>32.78278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.915750</td>\n",
       "      <td>23.219176</td>\n",
       "      <td>38.931987</td>\n",
       "      <td>22.869853</td>\n",
       "      <td>21.807184</td>\n",
       "      <td>22.446140</td>\n",
       "      <td>33.197341</td>\n",
       "      <td>21.710004</td>\n",
       "      <td>33.642120</td>\n",
       "      <td>33.612674</td>\n",
       "      <td>...</td>\n",
       "      <td>21.64163</td>\n",
       "      <td>21.42780</td>\n",
       "      <td>28.24700</td>\n",
       "      <td>23.82298</td>\n",
       "      <td>32.42940</td>\n",
       "      <td>31.75620</td>\n",
       "      <td>30.75149</td>\n",
       "      <td>34.24375</td>\n",
       "      <td>34.86720</td>\n",
       "      <td>32.73361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>30.954407</td>\n",
       "      <td>26.161567</td>\n",
       "      <td>38.376185</td>\n",
       "      <td>24.438322</td>\n",
       "      <td>23.084563</td>\n",
       "      <td>20.633840</td>\n",
       "      <td>33.113039</td>\n",
       "      <td>25.777452</td>\n",
       "      <td>33.527049</td>\n",
       "      <td>33.832243</td>\n",
       "      <td>...</td>\n",
       "      <td>22.83467</td>\n",
       "      <td>21.64052</td>\n",
       "      <td>28.18196</td>\n",
       "      <td>32.14078</td>\n",
       "      <td>31.28319</td>\n",
       "      <td>31.62442</td>\n",
       "      <td>30.78532</td>\n",
       "      <td>34.41045</td>\n",
       "      <td>33.21697</td>\n",
       "      <td>32.22166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>33.110149</td>\n",
       "      <td>26.161567</td>\n",
       "      <td>36.537388</td>\n",
       "      <td>24.438322</td>\n",
       "      <td>23.084563</td>\n",
       "      <td>24.832315</td>\n",
       "      <td>34.450999</td>\n",
       "      <td>21.428450</td>\n",
       "      <td>33.327444</td>\n",
       "      <td>32.673865</td>\n",
       "      <td>...</td>\n",
       "      <td>22.83467</td>\n",
       "      <td>21.64052</td>\n",
       "      <td>27.34719</td>\n",
       "      <td>34.40001</td>\n",
       "      <td>32.49032</td>\n",
       "      <td>31.40147</td>\n",
       "      <td>31.50200</td>\n",
       "      <td>32.69615</td>\n",
       "      <td>32.18970</td>\n",
       "      <td>32.59554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>32.880283</td>\n",
       "      <td>31.822942</td>\n",
       "      <td>38.265866</td>\n",
       "      <td>24.438322</td>\n",
       "      <td>23.084563</td>\n",
       "      <td>28.212990</td>\n",
       "      <td>33.676334</td>\n",
       "      <td>21.428450</td>\n",
       "      <td>32.251713</td>\n",
       "      <td>33.279514</td>\n",
       "      <td>...</td>\n",
       "      <td>22.83467</td>\n",
       "      <td>21.64052</td>\n",
       "      <td>27.95018</td>\n",
       "      <td>30.64208</td>\n",
       "      <td>31.87077</td>\n",
       "      <td>31.21094</td>\n",
       "      <td>30.91066</td>\n",
       "      <td>35.02409</td>\n",
       "      <td>34.26388</td>\n",
       "      <td>32.82148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>30.658715</td>\n",
       "      <td>32.930512</td>\n",
       "      <td>36.882090</td>\n",
       "      <td>24.438322</td>\n",
       "      <td>23.084563</td>\n",
       "      <td>20.633840</td>\n",
       "      <td>33.808583</td>\n",
       "      <td>21.428450</td>\n",
       "      <td>32.865904</td>\n",
       "      <td>31.904001</td>\n",
       "      <td>...</td>\n",
       "      <td>22.83467</td>\n",
       "      <td>21.64052</td>\n",
       "      <td>27.96503</td>\n",
       "      <td>31.93408</td>\n",
       "      <td>29.99395</td>\n",
       "      <td>31.12324</td>\n",
       "      <td>28.76312</td>\n",
       "      <td>34.07224</td>\n",
       "      <td>34.35116</td>\n",
       "      <td>31.98965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>27.118678</td>\n",
       "      <td>33.128697</td>\n",
       "      <td>35.626724</td>\n",
       "      <td>24.438322</td>\n",
       "      <td>23.084563</td>\n",
       "      <td>20.633840</td>\n",
       "      <td>35.757142</td>\n",
       "      <td>21.428450</td>\n",
       "      <td>33.022989</td>\n",
       "      <td>31.822097</td>\n",
       "      <td>...</td>\n",
       "      <td>22.83467</td>\n",
       "      <td>21.64052</td>\n",
       "      <td>27.32332</td>\n",
       "      <td>30.64108</td>\n",
       "      <td>30.95359</td>\n",
       "      <td>32.79170</td>\n",
       "      <td>33.26885</td>\n",
       "      <td>34.72915</td>\n",
       "      <td>33.65903</td>\n",
       "      <td>32.91379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          A1BG       A1CF        A2M      A2ML1    A3GALT2      AADAC  \\\n",
       "0    30.187983  33.643965  37.130851  22.869853  21.807184  22.446140   \n",
       "1    30.483638  28.866645  37.945043  24.142100  21.807184  22.446140   \n",
       "2    30.092851  26.684228  36.851946  30.406352  21.807184  22.446140   \n",
       "3    30.975010  32.521494  38.453517  28.229357  26.484608  29.199458   \n",
       "4    26.915750  23.219176  38.931987  22.869853  21.807184  22.446140   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "306  30.954407  26.161567  38.376185  24.438322  23.084563  20.633840   \n",
       "307  33.110149  26.161567  36.537388  24.438322  23.084563  24.832315   \n",
       "308  32.880283  31.822942  38.265866  24.438322  23.084563  28.212990   \n",
       "309  30.658715  32.930512  36.882090  24.438322  23.084563  20.633840   \n",
       "310  27.118678  33.128697  35.626724  24.438322  23.084563  20.633840   \n",
       "\n",
       "          AAMP       AARD     AARSD1      AASDH  ...   ZSCAN5B    ZSWIM2  \\\n",
       "0    34.740118  21.710004  32.883502  31.650847  ...  21.64163  21.42780   \n",
       "1    34.265360  27.454455  33.407541  32.839872  ...  29.27301  21.42780   \n",
       "2    34.708417  21.710004  32.137371  32.721578  ...  29.06719  21.42780   \n",
       "3    35.123918  25.877077  33.514752  32.029916  ...  24.01564  21.42780   \n",
       "4    33.197341  21.710004  33.642120  33.612674  ...  21.64163  21.42780   \n",
       "..         ...        ...        ...        ...  ...       ...       ...   \n",
       "306  33.113039  25.777452  33.527049  33.832243  ...  22.83467  21.64052   \n",
       "307  34.450999  21.428450  33.327444  32.673865  ...  22.83467  21.64052   \n",
       "308  33.676334  21.428450  32.251713  33.279514  ...  22.83467  21.64052   \n",
       "309  33.808583  21.428450  32.865904  31.904001  ...  22.83467  21.64052   \n",
       "310  35.757142  21.428450  33.022989  31.822097  ...  22.83467  21.64052   \n",
       "\n",
       "       ZSWIM3    ZSWIM7     ZUFSP    ZWILCH     ZWINT       ZYX     ZZEF1  \\\n",
       "0    28.95039  31.76873  27.82477  33.26816  30.04056  33.81903  33.22469   \n",
       "1    28.35341  32.32573  30.30568  31.53633  29.94062  33.67262  33.10198   \n",
       "2    29.05322  32.87654  30.86495  30.74474  31.95812  33.96047  32.97135   \n",
       "3    29.98474  32.03101  31.17308  31.63196  30.71732  34.54254  33.24885   \n",
       "4    28.24700  23.82298  32.42940  31.75620  30.75149  34.24375  34.86720   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "306  28.18196  32.14078  31.28319  31.62442  30.78532  34.41045  33.21697   \n",
       "307  27.34719  34.40001  32.49032  31.40147  31.50200  32.69615  32.18970   \n",
       "308  27.95018  30.64208  31.87077  31.21094  30.91066  35.02409  34.26388   \n",
       "309  27.96503  31.93408  29.99395  31.12324  28.76312  34.07224  34.35116   \n",
       "310  27.32332  30.64108  30.95359  32.79170  33.26885  34.72915  33.65903   \n",
       "\n",
       "         ZZZ3  \n",
       "0    31.23383  \n",
       "1    32.44643  \n",
       "2    32.20607  \n",
       "3    32.78278  \n",
       "4    32.73361  \n",
       "..        ...  \n",
       "306  32.22166  \n",
       "307  32.59554  \n",
       "308  32.82148  \n",
       "309  31.98965  \n",
       "310  32.91379  \n",
       "\n",
       "[311 rows x 10000 columns]"
      ]
     },
     "execution_count": 1746,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes = pd.read_csv('../GNN+RN/Data/10000_most_relevant_genes_biogrid_found.csv')\n",
    "Y = genes.Y\n",
    "\n",
    "genes = genes.iloc[:,1:10001] \n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A3GALT2</th>\n",
       "      <th>AADAC</th>\n",
       "      <th>AAMP</th>\n",
       "      <th>AARD</th>\n",
       "      <th>AARSD1</th>\n",
       "      <th>AASDH</th>\n",
       "      <th>...</th>\n",
       "      <th>ZSCAN5B</th>\n",
       "      <th>ZSWIM2</th>\n",
       "      <th>ZSWIM3</th>\n",
       "      <th>ZSWIM7</th>\n",
       "      <th>ZUFSP</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.614056</td>\n",
       "      <td>0.845335</td>\n",
       "      <td>0.488904</td>\n",
       "      <td>0.224501</td>\n",
       "      <td>0.179749</td>\n",
       "      <td>0.166779</td>\n",
       "      <td>0.653407</td>\n",
       "      <td>0.123933</td>\n",
       "      <td>0.368894</td>\n",
       "      <td>0.299150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114796</td>\n",
       "      <td>0.059637</td>\n",
       "      <td>0.695951</td>\n",
       "      <td>0.782164</td>\n",
       "      <td>0.519968</td>\n",
       "      <td>0.905521</td>\n",
       "      <td>0.707656</td>\n",
       "      <td>0.319148</td>\n",
       "      <td>0.319166</td>\n",
       "      <td>0.196466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.628746</td>\n",
       "      <td>0.528869</td>\n",
       "      <td>0.642611</td>\n",
       "      <td>0.308414</td>\n",
       "      <td>0.179749</td>\n",
       "      <td>0.166779</td>\n",
       "      <td>0.564839</td>\n",
       "      <td>0.638562</td>\n",
       "      <td>0.499013</td>\n",
       "      <td>0.544557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621585</td>\n",
       "      <td>0.059637</td>\n",
       "      <td>0.636101</td>\n",
       "      <td>0.827143</td>\n",
       "      <td>0.748853</td>\n",
       "      <td>0.728920</td>\n",
       "      <td>0.698606</td>\n",
       "      <td>0.277535</td>\n",
       "      <td>0.289026</td>\n",
       "      <td>0.533031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.609329</td>\n",
       "      <td>0.384299</td>\n",
       "      <td>0.436252</td>\n",
       "      <td>0.721581</td>\n",
       "      <td>0.179749</td>\n",
       "      <td>0.166779</td>\n",
       "      <td>0.647493</td>\n",
       "      <td>0.123933</td>\n",
       "      <td>0.183630</td>\n",
       "      <td>0.520142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607917</td>\n",
       "      <td>0.059637</td>\n",
       "      <td>0.706261</td>\n",
       "      <td>0.871623</td>\n",
       "      <td>0.800450</td>\n",
       "      <td>0.648199</td>\n",
       "      <td>0.881304</td>\n",
       "      <td>0.359348</td>\n",
       "      <td>0.256940</td>\n",
       "      <td>0.466317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.653160</td>\n",
       "      <td>0.770979</td>\n",
       "      <td>0.738603</td>\n",
       "      <td>0.577995</td>\n",
       "      <td>0.674302</td>\n",
       "      <td>0.640172</td>\n",
       "      <td>0.725006</td>\n",
       "      <td>0.497250</td>\n",
       "      <td>0.525634</td>\n",
       "      <td>0.377388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272451</td>\n",
       "      <td>0.059637</td>\n",
       "      <td>0.799651</td>\n",
       "      <td>0.803344</td>\n",
       "      <td>0.828878</td>\n",
       "      <td>0.738672</td>\n",
       "      <td>0.768942</td>\n",
       "      <td>0.524786</td>\n",
       "      <td>0.325101</td>\n",
       "      <td>0.626387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.451473</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.828930</td>\n",
       "      <td>0.224501</td>\n",
       "      <td>0.179749</td>\n",
       "      <td>0.166779</td>\n",
       "      <td>0.365597</td>\n",
       "      <td>0.123933</td>\n",
       "      <td>0.557259</td>\n",
       "      <td>0.704058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114796</td>\n",
       "      <td>0.059637</td>\n",
       "      <td>0.625432</td>\n",
       "      <td>0.140523</td>\n",
       "      <td>0.944784</td>\n",
       "      <td>0.751341</td>\n",
       "      <td>0.772036</td>\n",
       "      <td>0.439863</td>\n",
       "      <td>0.722610</td>\n",
       "      <td>0.612739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0.652136</td>\n",
       "      <td>0.349676</td>\n",
       "      <td>0.724004</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.314809</td>\n",
       "      <td>0.039741</td>\n",
       "      <td>0.349870</td>\n",
       "      <td>0.488324</td>\n",
       "      <td>0.528687</td>\n",
       "      <td>0.749376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194024</td>\n",
       "      <td>0.079748</td>\n",
       "      <td>0.618912</td>\n",
       "      <td>0.812208</td>\n",
       "      <td>0.839036</td>\n",
       "      <td>0.737903</td>\n",
       "      <td>0.775099</td>\n",
       "      <td>0.487243</td>\n",
       "      <td>0.317270</td>\n",
       "      <td>0.470644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.759246</td>\n",
       "      <td>0.349676</td>\n",
       "      <td>0.376868</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.314809</td>\n",
       "      <td>0.334045</td>\n",
       "      <td>0.599471</td>\n",
       "      <td>0.098709</td>\n",
       "      <td>0.479125</td>\n",
       "      <td>0.510294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194024</td>\n",
       "      <td>0.079748</td>\n",
       "      <td>0.535221</td>\n",
       "      <td>0.994647</td>\n",
       "      <td>0.950405</td>\n",
       "      <td>0.715168</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064946</td>\n",
       "      <td>0.574417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.747825</td>\n",
       "      <td>0.724704</td>\n",
       "      <td>0.703177</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.314809</td>\n",
       "      <td>0.571023</td>\n",
       "      <td>0.454955</td>\n",
       "      <td>0.098709</td>\n",
       "      <td>0.212021</td>\n",
       "      <td>0.635296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194024</td>\n",
       "      <td>0.079748</td>\n",
       "      <td>0.595674</td>\n",
       "      <td>0.691184</td>\n",
       "      <td>0.893246</td>\n",
       "      <td>0.695739</td>\n",
       "      <td>0.786450</td>\n",
       "      <td>0.661653</td>\n",
       "      <td>0.574419</td>\n",
       "      <td>0.637128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.637445</td>\n",
       "      <td>0.798074</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.314809</td>\n",
       "      <td>0.039741</td>\n",
       "      <td>0.479626</td>\n",
       "      <td>0.098709</td>\n",
       "      <td>0.364525</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194024</td>\n",
       "      <td>0.079748</td>\n",
       "      <td>0.597163</td>\n",
       "      <td>0.795516</td>\n",
       "      <td>0.720093</td>\n",
       "      <td>0.686796</td>\n",
       "      <td>0.591975</td>\n",
       "      <td>0.391116</td>\n",
       "      <td>0.595857</td>\n",
       "      <td>0.406248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.461556</td>\n",
       "      <td>0.811202</td>\n",
       "      <td>0.204949</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.314809</td>\n",
       "      <td>0.039741</td>\n",
       "      <td>0.843135</td>\n",
       "      <td>0.098709</td>\n",
       "      <td>0.403529</td>\n",
       "      <td>0.334495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194024</td>\n",
       "      <td>0.079748</td>\n",
       "      <td>0.532828</td>\n",
       "      <td>0.691103</td>\n",
       "      <td>0.808628</td>\n",
       "      <td>0.856935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.577824</td>\n",
       "      <td>0.425852</td>\n",
       "      <td>0.662749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1BG      A1CF       A2M     A2ML1   A3GALT2     AADAC      AAMP  \\\n",
       "0    0.614056  0.845335  0.488904  0.224501  0.179749  0.166779  0.653407   \n",
       "1    0.628746  0.528869  0.642611  0.308414  0.179749  0.166779  0.564839   \n",
       "2    0.609329  0.384299  0.436252  0.721581  0.179749  0.166779  0.647493   \n",
       "3    0.653160  0.770979  0.738603  0.577995  0.674302  0.640172  0.725006   \n",
       "4    0.451473  0.154762  0.828930  0.224501  0.179749  0.166779  0.365597   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "306  0.652136  0.349676  0.724004  0.327951  0.314809  0.039741  0.349870   \n",
       "307  0.759246  0.349676  0.376868  0.327951  0.314809  0.334045  0.599471   \n",
       "308  0.747825  0.724704  0.703177  0.327951  0.314809  0.571023  0.454955   \n",
       "309  0.637445  0.798074  0.441942  0.327951  0.314809  0.039741  0.479626   \n",
       "310  0.461556  0.811202  0.204949  0.327951  0.314809  0.039741  0.843135   \n",
       "\n",
       "         AARD    AARSD1     AASDH  ...   ZSCAN5B    ZSWIM2    ZSWIM3  \\\n",
       "0    0.123933  0.368894  0.299150  ...  0.114796  0.059637  0.695951   \n",
       "1    0.638562  0.499013  0.544557  ...  0.621585  0.059637  0.636101   \n",
       "2    0.123933  0.183630  0.520142  ...  0.607917  0.059637  0.706261   \n",
       "3    0.497250  0.525634  0.377388  ...  0.272451  0.059637  0.799651   \n",
       "4    0.123933  0.557259  0.704058  ...  0.114796  0.059637  0.625432   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "306  0.488324  0.528687  0.749376  ...  0.194024  0.079748  0.618912   \n",
       "307  0.098709  0.479125  0.510294  ...  0.194024  0.079748  0.535221   \n",
       "308  0.098709  0.212021  0.635296  ...  0.194024  0.079748  0.595674   \n",
       "309  0.098709  0.364525  0.351400  ...  0.194024  0.079748  0.597163   \n",
       "310  0.098709  0.403529  0.334495  ...  0.194024  0.079748  0.532828   \n",
       "\n",
       "       ZSWIM7     ZUFSP    ZWILCH     ZWINT       ZYX     ZZEF1      ZZZ3  \n",
       "0    0.782164  0.519968  0.905521  0.707656  0.319148  0.319166  0.196466  \n",
       "1    0.827143  0.748853  0.728920  0.698606  0.277535  0.289026  0.533031  \n",
       "2    0.871623  0.800450  0.648199  0.881304  0.359348  0.256940  0.466317  \n",
       "3    0.803344  0.828878  0.738672  0.768942  0.524786  0.325101  0.626387  \n",
       "4    0.140523  0.944784  0.751341  0.772036  0.439863  0.722610  0.612739  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "306  0.812208  0.839036  0.737903  0.775099  0.487243  0.317270  0.470644  \n",
       "307  0.994647  0.950405  0.715168  0.840000  0.000000  0.064946  0.574417  \n",
       "308  0.691184  0.893246  0.695739  0.786450  0.661653  0.574419  0.637128  \n",
       "309  0.795516  0.720093  0.686796  0.591975  0.391116  0.595857  0.406248  \n",
       "310  0.691103  0.808628  0.856935  1.000000  0.577824  0.425852  0.662749  \n",
       "\n",
       "[311 rows x 10000 columns]"
      ]
     },
     "execution_count": 1747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = genes.columns\n",
    "d = scaler.fit_transform(genes)\n",
    "genes = pd.DataFrame(d, columns=names)\n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1748,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genes, test_genes, Y_train, Y_test = train_test_split(genes, Y, test_size=0.1, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1749,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='../Biogrid/Data/biogrid_minimum.edgelist'\n",
    "G_initial = nx.read_edgelist(path)\n",
    "G = G_initial.subgraph(genes.columns)\n",
    "nx.write_edgelist(G, \"Data/conected_graph.edgelist\")\n",
    "data = pd.read_csv(\"Data/conected_graph.edgelist\", delimiter=' ')\n",
    "edge_index1=data[data.columns[0]].to_numpy()\n",
    "edge_index2=data[data.columns[1]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1750,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = np.concatenate((edge_index1, edge_index2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GRB7', 'RPS2', 'RPS2', ..., 'ZNF212', 'ZNF221', 'ZNF492'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 1751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5122"
      ]
     },
     "execution_count": 1752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(edge_index)\n",
    "len(list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1753,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index1 = le.transform(edge_index1)\n",
    "edge_index2 = le.transform(edge_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1754,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = [edge_index1]+[edge_index2]\n",
    "edge_index = np.array(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1755,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1700, 3692, 3692, ..., 5121, 4990, 5075],\n",
       "       [3692, 3734, 1425, ..., 4985, 4988, 5036]])"
      ]
     },
     "execution_count": 1755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1756,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1700, 3692, 3692,  ..., 5121, 4990, 5075],\n",
       "        [3692, 3734, 1425,  ..., 4985, 4988, 5036]])"
      ]
     },
     "execution_count": 1756,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edge_index, dtype=torch.int64)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1757,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n"
     ]
    }
   ],
   "source": [
    "train_data=[]\n",
    "for g in range(len(train_genes)):\n",
    "  b=[]\n",
    "  for i in train_genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    # a.append(Y[g])\n",
    "    # a.append(i*100)\n",
    "    a.append(i)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.float).reshape([-1,1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y_train.iloc[g]], dtype=torch.float).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  train_data.append(data)\n",
    "\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "test_data=[]\n",
    "for g in range(len(test_genes)):\n",
    "  b=[]\n",
    "  for i in test_genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    # a.append(Y[g])\n",
    "    # a.append(i*100)\n",
    "    a.append(i)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.float).reshape([-1,1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y_test.iloc[g]], dtype=torch.float).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  test_data.append(data)\n",
    "\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1759,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_graph(data,description=True):\n",
    "#     edges_raw = data.edge_index.numpy()\n",
    "#     edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
    "#     labels = data.x.numpy()\n",
    "#     G = nx.Graph()\n",
    "#     G.add_nodes_from(list(range(np.max(edges_raw))))\n",
    "#     G.add_edges_from(edges)\n",
    "#     plt.subplot(111)\n",
    "#     options = {\n",
    "#        'node_size': 100,\n",
    "#        'width': 1,\n",
    "#     }\n",
    "#     nx.draw(G, with_labels=description, node_color=labels.tolist(), cmap=plt.cm.tab10, font_weight='bold', **options)\n",
    "#     plt.show()\n",
    "\n",
    "# plot_graph(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1760,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[10000, 1], edge_index=[2, 4010], y=[1, 1])"
      ]
     },
     "execution_count": 1760,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1761,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5899],\n",
       "        [0.8862],\n",
       "        [0.6051],\n",
       "        ...,\n",
       "        [0.2787],\n",
       "        [0.3636],\n",
       "        [0.6011]])"
      ]
     },
     "execution_count": 1761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Patient sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1762,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 10000\n",
      "Number of charcateristics per node: 1\n",
      "Number of edges: 4010\n",
      "Average node degree: 0.40\n",
      "Has isolated nodes: True\n",
      "Has self-loops: False\n",
      "Is undirected: False\n",
      "Number of node features: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of charcateristics per node: {data.num_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "print(f'Number of node features: {data.num_node_features}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1763,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import SAGPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1764,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "# from torch_geometric.nn import GCNConv, GINConv\n",
    "# from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "# embed_dim = 32\n",
    "\n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, dim_h):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = GCNConv(1, dim_h)\n",
    "#         self.conv2 = GCNConv(dim_h, dim_h)\n",
    "#         self.conv3 = GCNConv(dim_h, dim_h)\n",
    "#         self.lin = Linear(dim_h, 1)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         # Node embeddings \n",
    "#         h = self.conv1(x, edge_index)\n",
    "#         h = h.relu()\n",
    "#         h = self.conv2(h, edge_index)\n",
    "#         h = h.relu()\n",
    "#         h = self.conv3(h, edge_index)\n",
    "\n",
    "#         # Graph-level readout\n",
    "#         hG = global_mean_pool(h, batch)\n",
    "\n",
    "#         # Classifier\n",
    "#         h = F.dropout(hG, p=0.5, training=self.training)\n",
    "#         h = self.lin(h)\n",
    "        \n",
    "#         return F.sigmoid(h).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1765,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "# from torch_geometric.nn import GCNConv, GINConv\n",
    "# from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "\n",
    "# class GIN(torch.nn.Module):\n",
    "#     def __init__(self, dim_h):\n",
    "#         super(GIN, self).__init__()\n",
    "#         self.conv1 = GINConv(\n",
    "#             Sequential(Linear(1, dim_h),\n",
    "#                        BatchNorm1d(dim_h), ReLU(),\n",
    "#                        Linear(dim_h, dim_h), ReLU()))\n",
    "#         self.conv2 = GINConv(\n",
    "#             Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "#                        Linear(dim_h, dim_h), ReLU()))\n",
    "#         self.conv3 = GINConv(\n",
    "#             Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "#                        Linear(dim_h, dim_h), ReLU()))\n",
    "#         self.lin1 = Linear(dim_h*3, dim_h*3)\n",
    "#         self.lin2 = Linear(dim_h*3, 1)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         # Node embeddings \n",
    "#         h1 = self.conv1(x, edge_index)\n",
    "#         h2 = self.conv2(h1, edge_index)\n",
    "#         h3 = self.conv3(h2, edge_index)\n",
    "\n",
    "#         # Graph-level readout\n",
    "#         h1 = global_add_pool(h1, batch)\n",
    "#         h2 = global_add_pool(h2, batch)\n",
    "#         h3 = global_add_pool(h3, batch)\n",
    "\n",
    "#         # Concatenate graph embeddings\n",
    "#         h = torch.cat((h1, h2, h3), dim=1)\n",
    "\n",
    "#         # Classifier\n",
    "#         h = self.lin1(h)\n",
    "#         h = h.relu()\n",
    "#         h = F.dropout(h, p=0.5, training=self.training)\n",
    "#         h = self.lin2(h)\n",
    "#         m = nn.Sigmoid()\n",
    "#         # print(F.log_softmax(h, dim=1))\n",
    "#         # print(m(h))\n",
    "#         return m(h)\n",
    "#         # return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1766,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv, GINConv\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, dim_h):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(1, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()))\n",
    "        self.lin1 = Linear(dim_h*3, dim_h*3)\n",
    "        self.lin2 = Linear(dim_h*3, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Node embeddings \n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "\n",
    "        # Graph-level readout\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        m = nn.Sigmoid()\n",
    "        # print(F.log_softmax(h, dim=1))\n",
    "        # print(m(h))\n",
    "        return m(h)\n",
    "        # return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1767,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    acc = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        # print(\"output\")\n",
    "        # print(output)\n",
    "        # print(\"argmax\")\n",
    "        # print(output.argmax(dim=1))\n",
    "        # print(\"data.y\")\n",
    "        # print(data.y)\n",
    "        # print(\"round(output)\")\n",
    "        # print(torch.round(output))\n",
    "        loss = criterion(output, data.y)  \n",
    "        total_loss += loss / len(train_loader)\n",
    "        acc += accuracy(torch.round(output), data.y) / len(train_loader)\n",
    "        # acc = accuracy_score(data.y, torch.round(output))\n",
    "        # f1score = f1_score(data.y, torch.round(output), average='weighted')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # loss_all += loss.item() * data.num_graphs\n",
    "    return total_loss, acc, 1#f1score\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def validation(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    loss = 0\n",
    "    for data in val_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss += criterion(output, data.y)#/ len(val_loader)\n",
    "        acc += accuracy(torch.round(output), data.y) / len(val_loader)\n",
    "        # acc = accuracy_score(data.y, output.argmax(dim=1))\n",
    "        # f1score = f1_score(data.y, torch.round(output), average='weighted')\n",
    "    return loss, acc, 1#f1score\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def test(model, test_data):\n",
    "    acc = 0\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "    for data in test_loader:\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        acc += accuracy(torch.round(output), data.y) / len(test_loader)\n",
    "        # f1score = f1_score(data.y, torch.round(output), average='weighted')\n",
    "    return acc, 1#f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1768,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "kf=StratifiedKFold(n_splits=5, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1769,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  [ 53  54  55  58  59  60  62  63  64  65  66  67  68  69  70  71  72  73\n",
      "  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
      " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
      " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
      " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
      " 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
      " 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235\n",
      " 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253\n",
      " 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275 276 277 278] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 56 57 61]\n",
      "223\n",
      "56\n",
      "Epoch: 000, Train loss: 39.6310, Train Acc: 0.5295, Train f1-score: 1.0000, Val loss: 70.2937, Val Acc: 0.4844, Val f1-score: 1.0000,\n",
      "Epoch: 001, Train loss: 17.5374, Train Acc: 0.4667, Train f1-score: 1.0000, Val loss: 60.5093, Val Acc: 0.4844, Val f1-score: 1.0000,\n",
      "Epoch: 002, Train loss: 14.3588, Train Acc: 0.4619, Train f1-score: 1.0000, Val loss: 45.4312, Val Acc: 0.4688, Val f1-score: 1.0000,\n",
      "Epoch: 003, Train loss: 10.3149, Train Acc: 0.4440, Train f1-score: 1.0000, Val loss: 29.0119, Val Acc: 0.4688, Val f1-score: 1.0000,\n",
      "Epoch: 004, Train loss: 6.9032, Train Acc: 0.4167, Train f1-score: 1.0000, Val loss: 17.3978, Val Acc: 0.5000, Val f1-score: 1.0000,\n",
      "Epoch: 005, Train loss: 6.3751, Train Acc: 0.4086, Train f1-score: 1.0000, Val loss: 20.3555, Val Acc: 0.4844, Val f1-score: 1.0000,\n",
      "Epoch: 006, Train loss: 7.9737, Train Acc: 0.4488, Train f1-score: 1.0000, Val loss: 20.8579, Val Acc: 0.5312, Val f1-score: 1.0000,\n",
      "Epoch: 007, Train loss: 7.9963, Train Acc: 0.4622, Train f1-score: 1.0000, Val loss: 20.6854, Val Acc: 0.5312, Val f1-score: 1.0000,\n",
      "Epoch: 008, Train loss: 7.9716, Train Acc: 0.4399, Train f1-score: 1.0000, Val loss: 20.4182, Val Acc: 0.5469, Val f1-score: 1.0000,\n",
      "Epoch: 009, Train loss: 7.5779, Train Acc: 0.4488, Train f1-score: 1.0000, Val loss: 20.3027, Val Acc: 0.5156, Val f1-score: 1.0000,\n",
      "Epoch: 010, Train loss: 7.1900, Train Acc: 0.4354, Train f1-score: 1.0000, Val loss: 20.2443, Val Acc: 0.5156, Val f1-score: 1.0000,\n",
      "Epoch: 011, Train loss: 7.1771, Train Acc: 0.4354, Train f1-score: 1.0000, Val loss: 20.2037, Val Acc: 0.5156, Val f1-score: 1.0000,\n",
      "Epoch: 012, Train loss: 6.7898, Train Acc: 0.4399, Train f1-score: 1.0000, Val loss: 20.1520, Val Acc: 0.5156, Val f1-score: 1.0000,\n",
      "Epoch: 013, Train loss: 6.4005, Train Acc: 0.4399, Train f1-score: 1.0000, Val loss: 20.0728, Val Acc: 0.4844, Val f1-score: 1.0000,\n",
      "Epoch: 014, Train loss: 6.3881, Train Acc: 0.4354, Train f1-score: 1.0000, Val loss: 20.0194, Val Acc: 0.4844, Val f1-score: 1.0000,\n",
      "Epoch: 015, Train loss: 6.3701, Train Acc: 0.4354, Train f1-score: 1.0000, Val loss: 19.9724, Val Acc: 0.4844, Val f1-score: 1.0000,\n",
      "Epoch: 016, Train loss: 6.3570, Train Acc: 0.4354, Train f1-score: 1.0000, Val loss: 19.9288, Val Acc: 0.4844, Val f1-score: 1.0000,\n",
      "Epoch: 017, Train loss: 5.9648, Train Acc: 0.4354, Train f1-score: 1.0000, Val loss: 19.8646, Val Acc: 0.4844, Val f1-score: 1.0000,\n",
      "Epoch: 018, Train loss: 5.9467, Train Acc: 0.4354, Train f1-score: 1.0000, Val loss: 19.7996, Val Acc: 0.4844, Val f1-score: 1.0000,\n",
      "Epoch: 019, Train loss: 5.9306, Train Acc: 0.4354, Train f1-score: 1.0000, Val loss: 19.7469, Val Acc: 0.4844, Val f1-score: 1.0000,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7640/631788854.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_f1score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7640/798391706.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;31m# print(\"output\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# print(output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7640/3287888089.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobal_add_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobal_add_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mh3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobal_add_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# Concatenate graph embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_geometric\\nn\\pool\\glob.py\u001b[0m in \u001b[0;36mglobal_add_pool\u001b[1;34m(x, batch, size)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'add'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_scatter\\scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \"\"\"\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sum'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'add'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mscatter_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mul'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscatter_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch_scatter\\scatter.py\u001b[0m in \u001b[0;36mscatter_sum\u001b[1;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_avg = []\n",
    "test_avg = []\n",
    "test_f1_score = []\n",
    "for train_index, val_index in kf.split(train_data, Y_train):\n",
    "    train_dataset=[]\n",
    "    val_dataset=[]\n",
    "    print(\"TRAIN: \", train_index, \"TEST:\", val_index)\n",
    "    for i in train_index:\n",
    "        train_dataset.append(train_data[i])\n",
    "    for i in val_index:\n",
    "        val_dataset.append(train_data[i])\n",
    "\n",
    "    print(len(train_dataset))\n",
    "    print(len(val_dataset))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = GIN(dim_h=8)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.6)\n",
    "    # optimizer = torch.optim.Adadelta(model.parameters(), lr=0.7)\n",
    "    train_epoch=[]\n",
    "    val_epoch=[]\n",
    "    train_loss_=[]\n",
    "    val_loss_=[]\n",
    "    epochs = 400\n",
    "    train_acc=0\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs+1):\n",
    "        train_loss, train_acc, train_f1score = train(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc, val_f1score = validation(model, val_loader, criterion)\n",
    "\n",
    "        train_loss = train_loss.detach().numpy()\n",
    "        train_loss_.append(train_loss)\n",
    "        val_loss_.append(val_loss.detach().numpy())\n",
    "        train_epoch.append(train_acc)\n",
    "        val_epoch.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch: {epoch:03d}, Train loss: {train_loss:.4f}, '\n",
    "            f'Train Acc: {train_acc:.4f}, Train f1-score: {train_f1score:.4f}, Val loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val f1-score: {val_f1score:.4f},')\n",
    "\n",
    "    test_acc, test_f1score = test(model, test_data)\n",
    "    print(\"GIN accuracy: \" + str(test_acc))\n",
    "\n",
    "    # fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    # ax1.plot(train_epoch, color=\"red\", label=\"train acc\")\n",
    "    # ax1.plot(val_epoch, color=\"blue\", label=\"test acc\")\n",
    "    # ax2.plot(train_loss_, color=\"orange\", label=\"train loss\")\n",
    "    # ax2.plot(val_loss_, color=\"purple\", label=\"test acc\")\n",
    "    # ax1.set_xlabel(\"Epoch\")\n",
    "    # ax1.set_ylabel(\"Accuracy\")\n",
    "    # ax2.set_xlabel(\"Epoch\")\n",
    "    # ax2.set_ylabel(\"Loss\")\n",
    "    # ax1.legend()\n",
    "    # ax2.legend()\n",
    "\n",
    "    plt.plot(train_epoch, color=\"red\")\n",
    "    plt.plot(val_epoch, color=\"blue\")\n",
    "    plt.plot(train_loss_, color=\"orange\")\n",
    "    plt.plot(val_loss_, color=\"purple\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    val_avg.append(val_acc)\n",
    "    test_avg.append(test_acc)\n",
    "    test_f1_score.append(test_f1score)\n",
    "\n",
    "print('Val accuracy: '+ str(np.array(val_avg).mean()))\n",
    "print('Test accuracy: '+ str(np.array(test_avg).mean()))\n",
    "print('Test f1-score: '+ str(np.array(test_f1_score).mean()))\n",
    "\n",
    "print('Val stv: '+ str(np.array(val_avg).std()))\n",
    "print('Test stv: '+ str(np.array(test_avg).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2bebc7e1a30>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_epoch, color=\"red\", label=\"train accuracy\")\n",
    "plt.plot(val_epoch, color=\"blue\", label=\"val accuracy\")\n",
    "plt.plot(train_loss_, color=\"orange\", label=\"train loss\")\n",
    "plt.plot(val_loss_, color=\"purple\", label=\"val loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy and loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb15f1e0f376981e7b6e1fc44ae8b8146823f10f258bcd6e448b0230b889fc06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
