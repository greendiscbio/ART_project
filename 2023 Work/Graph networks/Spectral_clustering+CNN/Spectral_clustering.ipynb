{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5B5_SYoDP7w",
        "outputId": "3006d4f4-c482-49d6-87ea-e3bef78626f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
            "Collecting tensorflow-intel==2.11.0\n",
            "  Using cached tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.24.0)\n",
            "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (58.1.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.2)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (13.0.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.43.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.13.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.6.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (2.4.7)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.11.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\sandr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.0)\n",
            "Installing collected packages: tensorflow-intel, tensorflow\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\sandr\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python39\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\com_github_grpc_grpc\\\\src\\\\core\\\\ext\\\\filters\\\\client_channel\\\\lb_policy\\\\grpclb\\\\client_load_reporting_filter.h'\n",
            "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
            "\n",
            "\n",
            "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
            "[notice] To update, run: C:\\Users\\sandr\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install -U tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83vq-RB2D467",
        "outputId": "650b5904-4e4d-4fb4-84e4-47a3b56a2bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.8/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from scipy>=1.8) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "pip install 'scipy>=1.8'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BEtJMupEcXr",
        "outputId": "c116a756-0610-4a93-edc8-e8aba9d6c97f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: networkx<2.7 in /usr/local/lib/python3.8/dist-packages (2.6.3)\n"
          ]
        }
      ],
      "source": [
        "pip install 'networkx<2.7'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Bga_71xCs_Z",
        "outputId": "06290c3d-97cb-4c68-ba09-c547c61f1e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        " \n",
        "drive.mount('/content/drive') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXGVj2E7CWJ4",
        "outputId": "487d044a-1c9e-4cd3-f93d-08e90cfceb59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjacency Matrix:\n",
            "[[0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]]\n",
            "       (123.08770753608935+0j)     (2.6508275013454945+0j)  \\\n",
            "0  -1.950074e-01+0.000000e+00j -1.966581e-01+0.000000e+00j   \n",
            "1  -5.408693e-01+0.000000e+00j  4.570309e-01+0.000000e+00j   \n",
            "2   4.601387e-01+0.000000e+00j -6.944627e-02+0.000000e+00j   \n",
            "3   2.838576e-01+0.000000e+00j  2.689129e-01+0.000000e+00j   \n",
            "4   2.912988e-01+0.000000e+00j  4.965845e-01+0.000000e+00j   \n",
            "5  -1.329409e-01+0.000000e+00j  1.001450e-01+0.000000e+00j   \n",
            "6  -1.656667e-01+0.000000e+00j -2.218864e-01+0.000000e+00j   \n",
            "7   1.555970e-01+0.000000e+00j  1.073738e-01+0.000000e+00j   \n",
            "8  -1.158591e-01+0.000000e+00j -1.430437e-01+0.000000e+00j   \n",
            "9  -1.615456e-01+0.000000e+00j  2.968424e-01+0.000000e+00j   \n",
            "10 -1.726709e-01+0.000000e+00j -1.503692e-03+0.000000e+00j   \n",
            "11 -7.871623e-02+0.000000e+00j  1.752005e-01+0.000000e+00j   \n",
            "12  2.801688e-02+0.000000e+00j -6.952635e-04+0.000000e+00j   \n",
            "13  7.759286e-02+0.000000e+00j  1.687312e-01+0.000000e+00j   \n",
            "14 -1.913765e-01+0.000000e+00j -2.201629e-02+0.000000e+00j   \n",
            "15 -1.489056e-01+0.000000e+00j -3.242207e-01+0.000000e+00j   \n",
            "16 -2.724470e-01+0.000000e+00j  1.804941e-01+0.000000e+00j   \n",
            "17  1.769703e-02+0.000000e+00j  2.014947e-01+0.000000e+00j   \n",
            "18  8.262171e-02+0.000000e+00j  7.512611e-02+0.000000e+00j   \n",
            "19  2.023143e-02+0.000000e+00j -7.082950e-02+0.000000e+00j   \n",
            "20  5.842412e-03+0.000000e+00j -2.712190e-02+0.000000e+00j   \n",
            "21 -1.178132e-15+0.000000e+00j -1.415439e-15+0.000000e+00j   \n",
            "22 -1.124169e-17+0.000000e+00j -9.601760e-16+0.000000e+00j   \n",
            "23  4.747529e-17+5.543584e-17j -1.904634e-17+2.688778e-17j   \n",
            "24  4.747529e-17-5.543584e-17j -1.904634e-17-2.688778e-17j   \n",
            "\n",
            "      (-2.0505938911567427+0j)     (2.0141512960097474+0j)  \\\n",
            "0  -1.982426e-01+0.000000e+00j -1.998929e-01+0.000000e+00j   \n",
            "1   2.822934e-01+0.000000e+00j  2.391158e-01+0.000000e+00j   \n",
            "2   3.405578e-01+0.000000e+00j  2.584483e-01+0.000000e+00j   \n",
            "3  -2.777941e-01+0.000000e+00j -2.121997e-01+0.000000e+00j   \n",
            "4   8.478799e-02+0.000000e+00j -9.106285e-02+0.000000e+00j   \n",
            "5  -2.015543e-01+0.000000e+00j  3.660230e-02+0.000000e+00j   \n",
            "6   2.525034e-02+0.000000e+00j -3.025453e-01+0.000000e+00j   \n",
            "7  -1.488658e-01+0.000000e+00j  1.365162e-01+0.000000e+00j   \n",
            "8   4.658983e-01+0.000000e+00j -1.335982e-01+0.000000e+00j   \n",
            "9   1.585630e-01+0.000000e+00j -3.085726e-01+0.000000e+00j   \n",
            "10  7.216413e-02+0.000000e+00j  4.063040e-01+0.000000e+00j   \n",
            "11  7.143325e-05+0.000000e+00j  8.072908e-02+0.000000e+00j   \n",
            "12  4.271657e-02+0.000000e+00j -3.624832e-04+0.000000e+00j   \n",
            "13  2.515089e-02+0.000000e+00j  7.298182e-02+0.000000e+00j   \n",
            "14 -6.164878e-02+0.000000e+00j -3.938775e-01+0.000000e+00j   \n",
            "15 -1.076241e-01+0.000000e+00j  3.117289e-01+0.000000e+00j   \n",
            "16 -3.607119e-01+0.000000e+00j  1.857998e-01+0.000000e+00j   \n",
            "17 -5.897403e-02+0.000000e+00j -1.497378e-01+0.000000e+00j   \n",
            "18 -2.763853e-01+0.000000e+00j  2.663439e-01+0.000000e+00j   \n",
            "19  4.478226e-02+0.000000e+00j -1.508306e-02+0.000000e+00j   \n",
            "20 -3.741821e-01+0.000000e+00j -2.869938e-02+0.000000e+00j   \n",
            "21  4.084187e-15+0.000000e+00j -1.119806e-15+0.000000e+00j   \n",
            "22 -6.242680e-15+0.000000e+00j  4.111592e-16+0.000000e+00j   \n",
            "23  9.178083e-17-7.346968e-18j -2.150974e-17-3.296468e-17j   \n",
            "24  9.178083e-17+7.346968e-18j -2.150974e-17+3.296468e-17j   \n",
            "\n",
            "       (-1.795640543703557+0j)    (-1.6168172372181058+0j)  \\\n",
            "0  -2.015166e-01+0.000000e+00j -1.998930e-01+0.000000e+00j   \n",
            "1   1.955207e-01+0.000000e+00j  2.372128e-01+0.000000e+00j   \n",
            "2   2.011992e-01+0.000000e+00j  2.603595e-01+0.000000e+00j   \n",
            "3  -1.281680e-01+0.000000e+00j -1.967755e-01+0.000000e+00j   \n",
            "4   1.007109e-01+0.000000e+00j  4.250567e-02+0.000000e+00j   \n",
            "5  -6.663368e-02+0.000000e+00j -1.087069e-01+0.000000e+00j   \n",
            "6  -6.359154e-02+0.000000e+00j  2.462959e-01+0.000000e+00j   \n",
            "7   9.870567e-02+0.000000e+00j  3.124591e-02+0.000000e+00j   \n",
            "8  -5.414621e-02+0.000000e+00j -3.370758e-01+0.000000e+00j   \n",
            "9  -2.362333e-01+0.000000e+00j -1.174937e-02+0.000000e+00j   \n",
            "10 -3.992765e-01+0.000000e+00j -1.419969e-01+0.000000e+00j   \n",
            "11 -1.856901e-01+0.000000e+00j -1.105581e-01+0.000000e+00j   \n",
            "12 -5.843659e-01+0.000000e+00j  5.859259e-01+0.000000e+00j   \n",
            "13 -1.848612e-01+0.000000e+00j -9.769914e-02+0.000000e+00j   \n",
            "14  3.798858e-01+0.000000e+00j  1.746043e-01+0.000000e+00j   \n",
            "15  1.547139e-01+0.000000e+00j  5.078298e-02+0.000000e+00j   \n",
            "16  1.885455e-01+0.000000e+00j -2.806418e-03+0.000000e+00j   \n",
            "17  6.849288e-03+0.000000e+00j  3.407117e-04+0.000000e+00j   \n",
            "18 -4.140572e-02+0.000000e+00j -1.371061e-01+0.000000e+00j   \n",
            "19 -2.383162e-02+0.000000e+00j  2.959450e-02+0.000000e+00j   \n",
            "20 -3.819953e-02+0.000000e+00j  4.126223e-01+0.000000e+00j   \n",
            "21 -6.098780e-16+0.000000e+00j -2.999861e-15+0.000000e+00j   \n",
            "22 -5.799637e-16+0.000000e+00j  6.502762e-15+0.000000e+00j   \n",
            "23 -1.073461e-15-4.097108e-16j  1.057471e-15+4.273351e-16j   \n",
            "24 -1.073461e-15+4.097108e-16j  1.057471e-15-4.273351e-16j   \n",
            "\n",
            "    (-1.4774450941964326+0j)  (-1.2194987033047733+0j)  \\\n",
            "0        -0.199880+0.000000j       -0.199880+0.000000j   \n",
            "1        -0.214123+0.000000j       -0.214123+0.000000j   \n",
            "2        -0.115028+0.000000j       -0.115028+0.000000j   \n",
            "3        -0.185748+0.000000j       -0.185748+0.000000j   \n",
            "4         0.250208+0.000000j        0.250208+0.000000j   \n",
            "5         0.270213+0.000000j        0.270213+0.000000j   \n",
            "6         0.000132+0.000000j        0.000132+0.000000j   \n",
            "7         0.068812+0.000000j        0.068812+0.000000j   \n",
            "8         0.009597+0.000000j        0.009597+0.000000j   \n",
            "9         0.133996+0.000000j        0.133996+0.000000j   \n",
            "10        0.013870+0.000000j        0.013870+0.000000j   \n",
            "11        0.218079+0.000000j        0.218079+0.000000j   \n",
            "12        0.007797+0.000000j        0.007797+0.000000j   \n",
            "13       -0.217256+0.000000j       -0.217256+0.000000j   \n",
            "14        0.016673+0.000000j        0.016673+0.000000j   \n",
            "15        0.116224+0.000000j        0.116224+0.000000j   \n",
            "16        0.002698+0.000000j        0.002698+0.000000j   \n",
            "17       -0.280136+0.000000j       -0.280136+0.000000j   \n",
            "18       -0.122853+0.000000j       -0.122853+0.000000j   \n",
            "19       -0.075182+0.000000j       -0.075182+0.000000j   \n",
            "20        0.006200+0.000000j        0.006200+0.000000j   \n",
            "21       -0.353553+0.000000j        0.353553+0.000000j   \n",
            "22        0.353553+0.000000j       -0.353553+0.000000j   \n",
            "23       -0.044379+0.261641j        0.044379-0.261641j   \n",
            "24       -0.044379-0.261641j        0.044379+0.261641j   \n",
            "\n",
            "       (-1.395274058347553+0j)     (-0.808644611378786+0j)  ...  \\\n",
            "0  -1.982557e-01+0.000000e+00j -1.998401e-01+0.000000e+00j  ...   \n",
            "1  -2.649727e-01+0.000000e+00j  8.799418e-02+0.000000e+00j  ...   \n",
            "2  -3.249286e-01+0.000000e+00j -1.188679e-01+0.000000e+00j  ...   \n",
            "3  -4.282847e-01+0.000000e+00j  1.309199e-01+0.000000e+00j  ...   \n",
            "4   2.104313e-01+0.000000e+00j  6.315695e-02+0.000000e+00j  ...   \n",
            "5  -2.707752e-01+0.000000e+00j  3.115335e-01+0.000000e+00j  ...   \n",
            "6  -2.341357e-01+0.000000e+00j  1.503676e-01+0.000000e+00j  ...   \n",
            "7  -1.270528e-01+0.000000e+00j  1.176756e-02+0.000000e+00j  ...   \n",
            "8  -1.432308e-01+0.000000e+00j  1.161241e-01+0.000000e+00j  ...   \n",
            "9   1.630772e-01+0.000000e+00j -1.590043e-01+0.000000e+00j  ...   \n",
            "10  1.624729e-01+0.000000e+00j  2.639799e-02+0.000000e+00j  ...   \n",
            "11 -3.063776e-01+0.000000e+00j -1.087167e-01+0.000000e+00j  ...   \n",
            "12 -1.104439e-03+0.000000e+00j  4.341217e-02+0.000000e+00j  ...   \n",
            "13  3.088267e-01+0.000000e+00j -7.962376e-02+0.000000e+00j  ...   \n",
            "14  1.554284e-01+0.000000e+00j  1.242044e-03+0.000000e+00j  ...   \n",
            "15  1.371206e-01+0.000000e+00j  2.050161e-01+0.000000e+00j  ...   \n",
            "16  2.704329e-03+0.000000e+00j -2.659744e-01+0.000000e+00j  ...   \n",
            "17  2.422412e-01+0.000000e+00j  2.851087e-01+0.000000e+00j  ...   \n",
            "18  6.150567e-02+0.000000e+00j  1.362387e-01+0.000000e+00j  ...   \n",
            "19  2.096655e-01+0.000000e+00j  1.558546e-01+0.000000e+00j  ...   \n",
            "20 -2.784873e-02+0.000000e+00j -2.895689e-02+0.000000e+00j  ...   \n",
            "21 -1.886333e-15+0.000000e+00j -5.000000e-01+0.000000e+00j  ...   \n",
            "22 -1.218551e-15+0.000000e+00j -5.000000e-01+0.000000e+00j  ...   \n",
            "23 -3.639033e-17-1.155589e-16j -8.880576e-17-1.256152e-17j  ...   \n",
            "24 -3.639033e-17+1.155589e-16j -8.880576e-17+1.256152e-17j  ...   \n",
            "\n",
            "       (0.7716452972206557+0j)     (1.0060207172687603+0j)  \\\n",
            "0  -1.998533e-01+0.000000e+00j -1.998666e-01+0.000000e+00j   \n",
            "1   1.103597e-01+0.000000e+00j  1.124288e-01+0.000000e+00j   \n",
            "2  -2.531675e-01+0.000000e+00j -2.961219e-01+0.000000e+00j   \n",
            "3   2.907289e-01+0.000000e+00j  3.286961e-01+0.000000e+00j   \n",
            "4   3.104639e-02+0.000000e+00j -3.931376e-02+0.000000e+00j   \n",
            "5  -2.303661e-01+0.000000e+00j -3.327794e-01+0.000000e+00j   \n",
            "6  -1.422470e-01+0.000000e+00j -5.350283e-02+0.000000e+00j   \n",
            "7  -4.785193e-01+0.000000e+00j  2.514304e-01+0.000000e+00j   \n",
            "8  -1.090970e-01+0.000000e+00j  1.198409e-02+0.000000e+00j   \n",
            "9  -2.269527e-01+0.000000e+00j  9.146493e-02+0.000000e+00j   \n",
            "10 -6.573038e-02+0.000000e+00j  1.631896e-01+0.000000e+00j   \n",
            "11  1.078964e-01+0.000000e+00j -4.052138e-01+0.000000e+00j   \n",
            "12  4.363327e-02+0.000000e+00j -4.420716e-04+0.000000e+00j   \n",
            "13  1.344149e-01+0.000000e+00j -4.010346e-01+0.000000e+00j   \n",
            "14  8.486482e-02+0.000000e+00j -1.487966e-01+0.000000e+00j   \n",
            "15  2.448741e-01+0.000000e+00j -5.598312e-02+0.000000e+00j   \n",
            "16 -2.665183e-01+0.000000e+00j  5.372681e-04+0.000000e+00j   \n",
            "17 -2.260890e-01+0.000000e+00j -3.423160e-01+0.000000e+00j   \n",
            "18 -5.822163e-02+0.000000e+00j -9.695368e-02+0.000000e+00j   \n",
            "19 -4.511267e-01+0.000000e+00j  2.475605e-01+0.000000e+00j   \n",
            "20  2.129021e-02+0.000000e+00j -2.761613e-03+0.000000e+00j   \n",
            "21 -1.475864e-15+0.000000e+00j  9.524781e-17+0.000000e+00j   \n",
            "22  1.736192e-15+0.000000e+00j -2.136086e-17+0.000000e+00j   \n",
            "23 -1.471524e-17+3.265127e-17j  1.030560e-16-1.088915e-16j   \n",
            "24 -1.471524e-17-3.265127e-17j  1.030560e-16+1.088915e-16j   \n",
            "\n",
            "        (1.633901826665724+0j)     (1.5806891910269132+0j)  \\\n",
            "0  -2.014772e-01+0.000000e+00j -1.998401e-01+0.000000e+00j   \n",
            "1  -5.014935e-02+0.000000e+00j  8.799418e-02+0.000000e+00j   \n",
            "2  -1.466543e-01+0.000000e+00j -1.188679e-01+0.000000e+00j   \n",
            "3  -1.315795e-01+0.000000e+00j  1.309199e-01+0.000000e+00j   \n",
            "4  -4.422482e-02+0.000000e+00j  6.315695e-02+0.000000e+00j   \n",
            "5  -1.268910e-01+0.000000e+00j  3.115335e-01+0.000000e+00j   \n",
            "6  -4.774011e-02+0.000000e+00j  1.503676e-01+0.000000e+00j   \n",
            "7  -4.212754e-01+0.000000e+00j  1.176756e-02+0.000000e+00j   \n",
            "8  -4.929977e-02+0.000000e+00j  1.161241e-01+0.000000e+00j   \n",
            "9  -3.171181e-01+0.000000e+00j -1.590043e-01+0.000000e+00j   \n",
            "10 -1.756433e-01+0.000000e+00j  2.639799e-02+0.000000e+00j   \n",
            "11  3.332385e-01+0.000000e+00j -1.087167e-01+0.000000e+00j   \n",
            "12  4.471096e-02+0.000000e+00j  4.341217e-02+0.000000e+00j   \n",
            "13 -3.447205e-01+0.000000e+00j -7.962376e-02+0.000000e+00j   \n",
            "14 -2.082324e-01+0.000000e+00j  1.242044e-03+0.000000e+00j   \n",
            "15 -3.555978e-01+0.000000e+00j  2.050161e-01+0.000000e+00j   \n",
            "16  1.826522e-01+0.000000e+00j -2.659744e-01+0.000000e+00j   \n",
            "17  1.560541e-01+0.000000e+00j  2.851087e-01+0.000000e+00j   \n",
            "18  4.769678e-02+0.000000e+00j  1.362387e-01+0.000000e+00j   \n",
            "19  3.497278e-01+0.000000e+00j  1.558546e-01+0.000000e+00j   \n",
            "20 -4.927603e-02+0.000000e+00j -2.895689e-02+0.000000e+00j   \n",
            "21 -8.019080e-16+0.000000e+00j  5.000000e-01+0.000000e+00j   \n",
            "22 -1.936726e-15+0.000000e+00j  5.000000e-01+0.000000e+00j   \n",
            "23  5.542025e-17+1.743894e-16j  1.346728e-16+9.544753e-17j   \n",
            "24  5.542025e-17-1.743894e-16j  1.346728e-16-9.544753e-17j   \n",
            "\n",
            "        (1.261802207871388+0j)  (1.39464668048268+0j)  \\\n",
            "0  -2.014773e-01+0.000000e+00j    -0.201477+0.000000j   \n",
            "1  -2.535883e-02+0.000000e+00j    -0.041712+0.000000j   \n",
            "2  -8.641103e-03+0.000000e+00j    -0.081161+0.000000j   \n",
            "3   6.389815e-02+0.000000e+00j    -0.052236+0.000000j   \n",
            "4  -2.936434e-01+0.000000e+00j    -0.026342+0.000000j   \n",
            "5   1.442752e-01+0.000000e+00j     0.208273+0.000000j   \n",
            "6  -1.482650e-01+0.000000e+00j     0.150314+0.000000j   \n",
            "7  -6.583464e-02+0.000000e+00j    -0.019236+0.000000j   \n",
            "8  -1.510678e-02+0.000000e+00j     0.112117+0.000000j   \n",
            "9   3.230955e-01+0.000000e+00j    -0.233091+0.000000j   \n",
            "10 -4.644342e-01+0.000000e+00j     0.017015+0.000000j   \n",
            "11 -1.286823e-01+0.000000e+00j    -0.269115+0.000000j   \n",
            "12  2.909943e-02+0.000000e+00j     0.035804+0.000000j   \n",
            "13  1.136720e-01+0.000000e+00j     0.257442+0.000000j   \n",
            "14 -4.751261e-01+0.000000e+00j    -0.030199+0.000000j   \n",
            "15  2.828197e-01+0.000000e+00j    -0.303945+0.000000j   \n",
            "16  1.874501e-01+0.000000e+00j     0.182112+0.000000j   \n",
            "17  1.642036e-01+0.000000e+00j    -0.156815+0.000000j   \n",
            "18 -3.179579e-01+0.000000e+00j    -0.075326+0.000000j   \n",
            "19 -2.299253e-02+0.000000e+00j    -0.131315+0.000000j   \n",
            "20 -3.382760e-02+0.000000e+00j    -0.013247+0.000000j   \n",
            "21 -1.053125e-16+0.000000e+00j     0.353553+0.000000j   \n",
            "22 -1.055090e-15+0.000000e+00j    -0.353553+0.000000j   \n",
            "23  7.153515e-17+7.011189e-17j    -0.044379+0.261641j   \n",
            "24  7.153515e-17-7.011189e-17j    -0.044379-0.261641j   \n",
            "\n",
            "       (-1.414213562373096+0j)     (1.4142135623730956+0j)  \\\n",
            "0  -2.014773e-01+0.000000e+00j -1.998400e-01+0.000000e+00j   \n",
            "1   3.317485e-02+0.000000e+00j -1.105184e-01+0.000000e+00j   \n",
            "2   3.596658e-02+0.000000e+00j  1.213137e-01+0.000000e+00j   \n",
            "3  -5.584385e-02+0.000000e+00j  1.381858e-01+0.000000e+00j   \n",
            "4  -1.197199e-01+0.000000e+00j -1.045156e-01+0.000000e+00j   \n",
            "5  -2.648308e-02+0.000000e+00j -6.802473e-02+0.000000e+00j   \n",
            "6   3.584264e-01+0.000000e+00j  4.578417e-01+0.000000e+00j   \n",
            "7  -9.634501e-02+0.000000e+00j -8.226710e-02+0.000000e+00j   \n",
            "8  -2.540390e-01+0.000000e+00j -3.947643e-01+0.000000e+00j   \n",
            "9   1.880239e-01+0.000000e+00j  1.815277e-01+0.000000e+00j   \n",
            "10  2.190911e-01+0.000000e+00j  1.230301e-01+0.000000e+00j   \n",
            "11  1.083343e-01+0.000000e+00j  2.701831e-02+0.000000e+00j   \n",
            "12 -5.426814e-01+0.000000e+00j  2.905167e-02+0.000000e+00j   \n",
            "13  1.205955e-01+0.000000e+00j -3.098176e-02+0.000000e+00j   \n",
            "14 -2.328366e-01+0.000000e+00j  9.642149e-02+0.000000e+00j   \n",
            "15 -1.421886e-01+0.000000e+00j  8.019784e-02+0.000000e+00j   \n",
            "16 -2.736229e-01+0.000000e+00j  1.925040e-01+0.000000e+00j   \n",
            "17  1.117186e-02+0.000000e+00j  1.063437e-02+0.000000e+00j   \n",
            "18 -8.483671e-02+0.000000e+00j  1.512726e-01+0.000000e+00j   \n",
            "19  4.562826e-02+0.000000e+00j -6.741320e-02+0.000000e+00j   \n",
            "20  4.168115e-01+0.000000e+00j -6.287372e-01+0.000000e+00j   \n",
            "21 -2.219549e-15+0.000000e+00j -3.394152e-15+0.000000e+00j   \n",
            "22  6.203587e-15+0.000000e+00j -9.653202e-15+0.000000e+00j   \n",
            "23 -9.582562e-16-4.269506e-16j  8.095998e-17-3.283816e-17j   \n",
            "24 -9.582562e-16+4.269506e-16j  8.095998e-17+3.283816e-17j   \n",
            "\n",
            "    (4.874175526636272e-17+1.2921093404023431e-16j)  \\\n",
            "0                       -1.998402e-01+0.000000e+00j   \n",
            "1                       -5.563421e-02+0.000000e+00j   \n",
            "2                       -2.347363e-01+0.000000e+00j   \n",
            "3                       -2.080512e-01+0.000000e+00j   \n",
            "4                       -1.705664e-01+0.000000e+00j   \n",
            "5                       -2.924747e-01+0.000000e+00j   \n",
            "6                        8.337458e-02+0.000000e+00j   \n",
            "7                        4.688973e-01+0.000000e+00j   \n",
            "8                        1.196416e-01+0.000000e+00j   \n",
            "9                       -5.963142e-02+0.000000e+00j   \n",
            "10                      -6.617469e-02+0.000000e+00j   \n",
            "11                       1.487181e-01+0.000000e+00j   \n",
            "12                       4.361682e-02+0.000000e+00j   \n",
            "13                      -1.543950e-01+0.000000e+00j   \n",
            "14                      -9.848078e-02+0.000000e+00j   \n",
            "15                      -1.234428e-01+0.000000e+00j   \n",
            "16                      -8.601115e-02+0.000000e+00j   \n",
            "17                       3.459576e-01+0.000000e+00j   \n",
            "18                       1.089201e-01+0.000000e+00j   \n",
            "19                      -5.318772e-01+0.000000e+00j   \n",
            "20                      -1.573223e-02+0.000000e+00j   \n",
            "21                       1.300095e-15+0.000000e+00j   \n",
            "22                       2.776392e-16+0.000000e+00j   \n",
            "23                       6.622707e-17+1.198047e-16j   \n",
            "24                       6.622707e-17-1.198047e-16j   \n",
            "\n",
            "    (4.874175526636272e-17-1.2921093404023431e-16j)  \n",
            "0                       -2.014773e-01+0.000000e+00j  \n",
            "1                        1.247030e-02+0.000000e+00j  \n",
            "2                       -1.376661e-01+0.000000e+00j  \n",
            "3                        1.160584e-01+0.000000e+00j  \n",
            "4                       -1.565038e-01+0.000000e+00j  \n",
            "5                       -1.653052e-01+0.000000e+00j  \n",
            "6                        1.049705e-01+0.000000e+00j  \n",
            "7                        3.556148e-01+0.000000e+00j  \n",
            "8                        1.146384e-01+0.000000e+00j  \n",
            "9                       -1.102024e-01+0.000000e+00j  \n",
            "10                      -1.765725e-01+0.000000e+00j  \n",
            "11                       4.467546e-01+0.000000e+00j  \n",
            "12                       4.404818e-02+0.000000e+00j  \n",
            "13                       4.677982e-01+0.000000e+00j  \n",
            "14                       1.821106e-01+0.000000e+00j  \n",
            "15                       1.217159e-01+0.000000e+00j  \n",
            "16                       3.225412e-03+0.000000e+00j  \n",
            "17                      -1.940566e-01+0.000000e+00j  \n",
            "18                      -5.804302e-02+0.000000e+00j  \n",
            "19                       4.137240e-01+0.000000e+00j  \n",
            "20                      -2.272992e-02+0.000000e+00j  \n",
            "21                       1.259224e-15+0.000000e+00j  \n",
            "22                      -1.017139e-15+0.000000e+00j  \n",
            "23                       1.100131e-16+7.484434e-17j  \n",
            "24                       1.100131e-16-7.484434e-17j  \n",
            "\n",
            "[25 rows x 25 columns]\n",
            "[ 1.23087708e+02+0.00000000e+00j  2.65082750e+00+0.00000000e+00j\n",
            " -2.05059389e+00+0.00000000e+00j  2.01415130e+00+0.00000000e+00j\n",
            " -1.79564054e+00+0.00000000e+00j -1.61681724e+00+0.00000000e+00j\n",
            " -1.47744509e+00+0.00000000e+00j -1.21949870e+00+0.00000000e+00j\n",
            " -1.39527406e+00+0.00000000e+00j -8.08644611e-01+0.00000000e+00j\n",
            " -4.78196172e-01+0.00000000e+00j -3.59611502e-01+0.00000000e+00j\n",
            " -2.48242971e-02+0.00000000e+00j  3.55450064e-01+0.00000000e+00j\n",
            "  4.69703792e-01+0.00000000e+00j  7.71645297e-01+0.00000000e+00j\n",
            "  1.00602072e+00+0.00000000e+00j  1.63390183e+00+0.00000000e+00j\n",
            "  1.58068919e+00+0.00000000e+00j  1.26180221e+00+0.00000000e+00j\n",
            "  1.39464668e+00+0.00000000e+00j -1.41421356e+00+0.00000000e+00j\n",
            "  1.41421356e+00+0.00000000e+00j  4.87417553e-17+1.29210934e-16j\n",
            "  4.87417553e-17-1.29210934e-16j]\n",
            "#################################################\n",
            "    -0.35961150229067307  -0.4781961716385835\n",
            "0          -2.014903e-01        -2.014903e-01\n",
            "1          -1.150098e-01        -1.150098e-01\n",
            "2           1.428839e-01         1.428839e-01\n",
            "3           1.506848e-01         1.506848e-01\n",
            "4          -1.429589e-02        -1.429589e-02\n",
            "5          -1.090710e-01        -1.090710e-01\n",
            "6           6.562950e-02         6.562950e-02\n",
            "7          -1.509565e-01        -1.509565e-01\n",
            "8           3.628023e-01         3.628023e-01\n",
            "9           1.596249e-01         1.596249e-01\n",
            "10          1.127206e-01         1.127206e-01\n",
            "11          3.340098e-02         3.340098e-02\n",
            "12          7.783441e-03         7.783441e-03\n",
            "13         -3.732441e-02        -3.732441e-02\n",
            "14          1.036952e-01         1.036952e-01\n",
            "15          1.012160e-01         1.012160e-01\n",
            "16          2.762822e-01         2.762822e-01\n",
            "17          5.377442e-02         5.377442e-02\n",
            "18          1.857148e-01         1.857148e-01\n",
            "19         -4.328857e-02        -4.328857e-02\n",
            "20         -2.342885e-01        -2.342885e-01\n",
            "21          1.789072e-15         1.960645e-15\n",
            "22          5.947300e-16         4.060124e-16\n",
            "23          3.772728e-01        -3.772728e-01\n",
            "24          5.868615e-01        -5.868615e-01\n",
            "2.0\n",
            "2.0\n",
            "#################################################\n",
            "-0.5868615245256357\n",
            "    -0.35961150229067307  -0.4781961716385835\n",
            "0               0.032798             0.385371\n",
            "1               0.119279             0.471852\n",
            "2               0.377172             0.729745\n",
            "3               0.384973             0.737546\n",
            "4               0.219993             0.572566\n",
            "5               0.125217             0.477791\n",
            "6               0.299918             0.652491\n",
            "7               0.083332             0.435905\n",
            "8               0.597091             0.949664\n",
            "9               0.393913             0.746486\n",
            "10              0.347009             0.699582\n",
            "11              0.267689             0.620263\n",
            "12              0.242072             0.594645\n",
            "13              0.196964             0.549537\n",
            "14              0.337984             0.690557\n",
            "15              0.335504             0.688077\n",
            "16              0.510571             0.863144\n",
            "17              0.288063             0.640636\n",
            "18              0.420003             0.772576\n",
            "19              0.191000             0.543573\n",
            "20              0.000000             0.352573\n",
            "21              0.234288             0.586862\n",
            "22              0.234288             0.586862\n",
            "23              0.611561             0.209589\n",
            "24              0.821150             0.000000\n",
            "    -0.35961150229067307  -0.4781961716385835\n",
            "0                      3                   38\n",
            "1                     11                   47\n",
            "2                     37                   72\n",
            "3                     38                   73\n",
            "4                     21                   57\n",
            "5                     12                   47\n",
            "6                     29                   65\n",
            "7                      8                   43\n",
            "8                     59                   94\n",
            "9                     39                   74\n",
            "10                    34                   69\n",
            "11                    26                   62\n",
            "12                    24                   59\n",
            "13                    19                   54\n",
            "14                    33                   69\n",
            "15                    33                   68\n",
            "16                    51                   86\n",
            "17                    28                   64\n",
            "18                    42                   77\n",
            "19                    19                   54\n",
            "20                     0                   35\n",
            "21                    23                   58\n",
            "22                    23                   58\n",
            "23                    61                   20\n",
            "24                    82                    0\n",
            "7885\n",
            "    -0.35961150229067307  -0.4781961716385835  records\n",
            "0                      0                   35        1\n",
            "1                      3                   38        1\n",
            "2                      8                   43        1\n",
            "3                     11                   47        1\n",
            "4                     12                   47        1\n",
            "5                     19                   54        2\n",
            "6                     21                   57        1\n",
            "7                     23                   58        2\n",
            "8                     24                   59        1\n",
            "9                     26                   62        1\n",
            "10                    28                   64        1\n",
            "11                    29                   65        1\n",
            "12                    33                   68        1\n",
            "13                    33                   69        1\n",
            "14                    34                   69        1\n",
            "15                    37                   72        1\n",
            "16                    38                   73        1\n",
            "17                    39                   74        1\n",
            "18                    42                   77        1\n",
            "19                    51                   86        1\n",
            "20                    59                   94        1\n",
            "21                    61                   20        1\n",
            "22                    82                    0        1\n",
            "[[0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (8,11,12,21,30,50) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_24 (Conv2D)          (None, 96, 96, 512)       13312     \n",
            "                                                                 \n",
            " max_pooling2d_24 (MaxPoolin  (None, 48, 48, 512)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 46, 46, 256)       1179904   \n",
            "                                                                 \n",
            " max_pooling2d_25 (MaxPoolin  (None, 23, 23, 256)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 21, 21, 128)       295040    \n",
            "                                                                 \n",
            " max_pooling2d_26 (MaxPoolin  (None, 10, 10, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 256)               3277056   \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,864,129\n",
            "Trainable params: 4,864,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "train data\n",
            "279\n",
            "TRAIN:  [ 46  50  51  55  56  61  62  63  64  65  66  67  68  69  70  71  72  73\n",
            "  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
            "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
            " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
            " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
            " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
            " 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
            " 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235\n",
            " 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253\n",
            " 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
            " 272 273 274 275 276 277 278] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 47 48\n",
            " 49 52 53 54 57 58 59 60]\n",
            "Epoch 1/30\n",
            "28/28 [==============================] - 93s 3s/step - loss: 0.7014 - accuracy: 0.5336 - val_loss: 0.6930 - val_accuracy: 0.4643\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.7048 - accuracy: 0.4753 - val_loss: 0.6911 - val_accuracy: 0.5357\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - 91s 3s/step - loss: 0.6934 - accuracy: 0.5381 - val_loss: 0.6894 - val_accuracy: 0.5536\n",
            "Epoch 4/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.7056 - accuracy: 0.5247 - val_loss: 0.7707 - val_accuracy: 0.4643\n",
            "Epoch 5/30\n",
            "28/28 [==============================] - 82s 3s/step - loss: 0.7370 - accuracy: 0.4529 - val_loss: 0.6893 - val_accuracy: 0.5893\n",
            "Epoch 6/30\n",
            "28/28 [==============================] - 86s 3s/step - loss: 0.7135 - accuracy: 0.4798 - val_loss: 0.6968 - val_accuracy: 0.4643\n",
            "Epoch 7/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6982 - accuracy: 0.5202 - val_loss: 0.6872 - val_accuracy: 0.5357\n",
            "Epoch 8/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6906 - accuracy: 0.5381 - val_loss: 0.6874 - val_accuracy: 0.5357\n",
            "Epoch 9/30\n",
            "28/28 [==============================] - 82s 3s/step - loss: 0.7026 - accuracy: 0.4664 - val_loss: 0.6934 - val_accuracy: 0.4643\n",
            "Epoch 10/30\n",
            "28/28 [==============================] - 86s 3s/step - loss: 0.6880 - accuracy: 0.5291 - val_loss: 0.7124 - val_accuracy: 0.5357\n",
            "Epoch 11/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.7082 - accuracy: 0.4798 - val_loss: 0.6874 - val_accuracy: 0.5357\n",
            "Epoch 12/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6920 - accuracy: 0.5202 - val_loss: 0.6961 - val_accuracy: 0.4643\n",
            "Epoch 13/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.7006 - accuracy: 0.5112 - val_loss: 0.6870 - val_accuracy: 0.5714\n",
            "Epoch 14/30\n",
            "28/28 [==============================] - 82s 3s/step - loss: 0.6958 - accuracy: 0.4798 - val_loss: 0.6857 - val_accuracy: 0.5536\n",
            "Epoch 15/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6911 - accuracy: 0.4753 - val_loss: 0.6894 - val_accuracy: 0.5536\n",
            "Epoch 16/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6847 - accuracy: 0.5605 - val_loss: 0.6854 - val_accuracy: 0.5357\n",
            "Epoch 17/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6862 - accuracy: 0.5471 - val_loss: 0.6860 - val_accuracy: 0.5893\n",
            "Epoch 18/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6858 - accuracy: 0.5561 - val_loss: 0.6851 - val_accuracy: 0.5536\n",
            "Epoch 19/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6853 - accuracy: 0.5830 - val_loss: 0.6851 - val_accuracy: 0.5357\n",
            "Epoch 20/30\n",
            "28/28 [==============================] - 94s 3s/step - loss: 0.6855 - accuracy: 0.5605 - val_loss: 0.6848 - val_accuracy: 0.5893\n",
            "Epoch 21/30\n",
            "28/28 [==============================] - 85s 3s/step - loss: 0.6837 - accuracy: 0.5740 - val_loss: 0.6848 - val_accuracy: 0.5536\n",
            "Epoch 22/30\n",
            "28/28 [==============================] - 94s 3s/step - loss: 0.6931 - accuracy: 0.5336 - val_loss: 0.6951 - val_accuracy: 0.4643\n",
            "Epoch 23/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6856 - accuracy: 0.5471 - val_loss: 0.7053 - val_accuracy: 0.5357\n",
            "Epoch 24/30\n",
            "28/28 [==============================] - 91s 3s/step - loss: 0.6813 - accuracy: 0.5426 - val_loss: 0.6971 - val_accuracy: 0.4643\n",
            "Epoch 25/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6951 - accuracy: 0.5202 - val_loss: 0.6837 - val_accuracy: 0.5357\n",
            "Epoch 26/30\n",
            "28/28 [==============================] - 82s 3s/step - loss: 0.6811 - accuracy: 0.5919 - val_loss: 0.6869 - val_accuracy: 0.5357\n",
            "Epoch 27/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6894 - accuracy: 0.5561 - val_loss: 0.6965 - val_accuracy: 0.4643\n",
            "Epoch 28/30\n",
            "28/28 [==============================] - 86s 3s/step - loss: 0.6939 - accuracy: 0.4933 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
            "Epoch 29/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6846 - accuracy: 0.5785 - val_loss: 0.6829 - val_accuracy: 0.5536\n",
            "Epoch 30/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6803 - accuracy: 0.5650 - val_loss: 0.6833 - val_accuracy: 0.5536\n",
            "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  47  48  49  52  53  54  57  58\n",
            "  59  60 104 109 112 113 114 115 116 117 119 121 122 123 124 125 126 127\n",
            " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
            " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
            " 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
            " 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235\n",
            " 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253\n",
            " 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
            " 272 273 274 275 276 277 278] TEST: [ 46  50  51  55  56  61  62  63  64  65  66  67  68  69  70  71  72  73\n",
            "  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
            "  92  93  94  95  96  97  98  99 100 101 102 103 105 106 107 108 110 111\n",
            " 118 120]\n",
            "Epoch 1/30\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.6940 - accuracy: 0.5247 - val_loss: 0.6913 - val_accuracy: 0.4643\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - 86s 3s/step - loss: 0.6884 - accuracy: 0.5561 - val_loss: 0.7297 - val_accuracy: 0.5357\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - 85s 3s/step - loss: 0.6843 - accuracy: 0.5785 - val_loss: 0.6870 - val_accuracy: 0.4643\n",
            "Epoch 4/30\n",
            "28/28 [==============================] - 82s 3s/step - loss: 0.6840 - accuracy: 0.5426 - val_loss: 0.6786 - val_accuracy: 0.5357\n",
            "Epoch 5/30\n",
            "28/28 [==============================] - 86s 3s/step - loss: 0.6825 - accuracy: 0.5426 - val_loss: 0.6784 - val_accuracy: 0.6429\n",
            "Epoch 6/30\n",
            "28/28 [==============================] - 85s 3s/step - loss: 0.6853 - accuracy: 0.5874 - val_loss: 0.6758 - val_accuracy: 0.6071\n",
            "Epoch 7/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6823 - accuracy: 0.5650 - val_loss: 0.6804 - val_accuracy: 0.5357\n",
            "Epoch 8/30\n",
            "28/28 [==============================] - 91s 3s/step - loss: 0.6817 - accuracy: 0.5740 - val_loss: 0.6824 - val_accuracy: 0.5536\n",
            "Epoch 9/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6833 - accuracy: 0.5874 - val_loss: 0.6749 - val_accuracy: 0.6071\n",
            "Epoch 10/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6833 - accuracy: 0.5426 - val_loss: 0.6899 - val_accuracy: 0.4643\n",
            "Epoch 11/30\n",
            "28/28 [==============================] - 86s 3s/step - loss: 0.6878 - accuracy: 0.5067 - val_loss: 0.6750 - val_accuracy: 0.5893\n",
            "Epoch 12/30\n",
            "28/28 [==============================] - 86s 3s/step - loss: 0.6904 - accuracy: 0.5247 - val_loss: 0.6959 - val_accuracy: 0.5357\n",
            "Epoch 13/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6870 - accuracy: 0.5874 - val_loss: 0.6762 - val_accuracy: 0.6786\n",
            "Epoch 14/30\n",
            "28/28 [==============================] - 86s 3s/step - loss: 0.6837 - accuracy: 0.5426 - val_loss: 0.6827 - val_accuracy: 0.5714\n",
            "Epoch 15/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6927 - accuracy: 0.4709 - val_loss: 0.6813 - val_accuracy: 0.5357\n",
            "Epoch 16/30\n",
            "28/28 [==============================] - 86s 3s/step - loss: 0.6950 - accuracy: 0.5291 - val_loss: 0.6774 - val_accuracy: 0.6607\n",
            "Epoch 17/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6860 - accuracy: 0.5381 - val_loss: 0.6757 - val_accuracy: 0.6429\n",
            "Epoch 18/30\n",
            "28/28 [==============================] - 86s 3s/step - loss: 0.6859 - accuracy: 0.5336 - val_loss: 0.6771 - val_accuracy: 0.5536\n",
            "Epoch 19/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6772 - accuracy: 0.5605 - val_loss: 0.6755 - val_accuracy: 0.6429\n",
            "Epoch 20/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6829 - accuracy: 0.5605 - val_loss: 0.6746 - val_accuracy: 0.6250\n",
            "Epoch 21/30\n",
            "28/28 [==============================] - 91s 3s/step - loss: 0.6773 - accuracy: 0.5426 - val_loss: 0.6866 - val_accuracy: 0.5357\n",
            "Epoch 22/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6849 - accuracy: 0.5516 - val_loss: 0.6755 - val_accuracy: 0.6607\n",
            "Epoch 23/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6777 - accuracy: 0.5919 - val_loss: 0.6744 - val_accuracy: 0.6071\n",
            "Epoch 24/30\n",
            "28/28 [==============================] - 82s 3s/step - loss: 0.6809 - accuracy: 0.5919 - val_loss: 0.6749 - val_accuracy: 0.6607\n",
            "Epoch 25/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6812 - accuracy: 0.5605 - val_loss: 0.6754 - val_accuracy: 0.6250\n",
            "Epoch 26/30\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.6779 - accuracy: 0.5471 - val_loss: 0.6859 - val_accuracy: 0.5000\n",
            "Epoch 27/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6864 - accuracy: 0.5426 - val_loss: 0.6739 - val_accuracy: 0.5893\n",
            "Epoch 28/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6732 - accuracy: 0.6233 - val_loss: 0.6746 - val_accuracy: 0.5893\n",
            "Epoch 29/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6802 - accuracy: 0.5605 - val_loss: 0.6749 - val_accuracy: 0.5893\n",
            "Epoch 30/30\n",
            "28/28 [==============================] - 91s 3s/step - loss: 0.6876 - accuracy: 0.5740 - val_loss: 0.6741 - val_accuracy: 0.6429\n",
            "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 105 106 107 108\n",
            " 110 111 118 120 164 165 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
            " 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
            " 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235\n",
            " 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253\n",
            " 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
            " 272 273 274 275 276 277 278] TEST: [104 109 112 113 114 115 116 117 119 121 122 123 124 125 126 127 128 129\n",
            " 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147\n",
            " 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 166 167\n",
            " 168 169]\n",
            "Epoch 1/30\n",
            "28/28 [==============================] - 95s 3s/step - loss: 0.6752 - accuracy: 0.6099 - val_loss: 0.6667 - val_accuracy: 0.6429\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - 91s 3s/step - loss: 0.6769 - accuracy: 0.6188 - val_loss: 0.6640 - val_accuracy: 0.5536\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - 93s 3s/step - loss: 0.6742 - accuracy: 0.5561 - val_loss: 0.6741 - val_accuracy: 0.5357\n",
            "Epoch 4/30\n",
            "28/28 [==============================] - 84s 3s/step - loss: 0.6749 - accuracy: 0.5516 - val_loss: 0.6715 - val_accuracy: 0.6429\n",
            "Epoch 5/30\n",
            "28/28 [==============================] - 93s 3s/step - loss: 0.6745 - accuracy: 0.5695 - val_loss: 0.6669 - val_accuracy: 0.5536\n",
            "Epoch 6/30\n",
            "28/28 [==============================] - 83s 3s/step - loss: 0.6738 - accuracy: 0.5830 - val_loss: 0.6641 - val_accuracy: 0.6429\n",
            "Epoch 7/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6710 - accuracy: 0.6143 - val_loss: 0.6652 - val_accuracy: 0.6071\n",
            "Epoch 8/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6733 - accuracy: 0.5874 - val_loss: 0.6656 - val_accuracy: 0.5536\n",
            "Epoch 9/30\n",
            "28/28 [==============================] - 89s 3s/step - loss: 0.6807 - accuracy: 0.5561 - val_loss: 0.6672 - val_accuracy: 0.5536\n",
            "Epoch 10/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6827 - accuracy: 0.5740 - val_loss: 0.6791 - val_accuracy: 0.5357\n",
            "Epoch 11/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6774 - accuracy: 0.5695 - val_loss: 0.6692 - val_accuracy: 0.6250\n",
            "Epoch 12/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6710 - accuracy: 0.6009 - val_loss: 0.6630 - val_accuracy: 0.5714\n",
            "Epoch 13/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6722 - accuracy: 0.6143 - val_loss: 0.6625 - val_accuracy: 0.5714\n",
            "Epoch 14/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6830 - accuracy: 0.6009 - val_loss: 0.6625 - val_accuracy: 0.6607\n",
            "Epoch 15/30\n",
            "28/28 [==============================] - 89s 3s/step - loss: 0.6760 - accuracy: 0.5785 - val_loss: 0.6630 - val_accuracy: 0.6429\n",
            "Epoch 16/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6795 - accuracy: 0.5740 - val_loss: 0.6881 - val_accuracy: 0.5179\n",
            "Epoch 17/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6709 - accuracy: 0.6368 - val_loss: 0.6631 - val_accuracy: 0.5536\n",
            "Epoch 18/30\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.6776 - accuracy: 0.5830 - val_loss: 0.6694 - val_accuracy: 0.5357\n",
            "Epoch 19/30\n",
            "28/28 [==============================] - 96s 3s/step - loss: 0.6701 - accuracy: 0.6054 - val_loss: 0.6626 - val_accuracy: 0.5357\n",
            "Epoch 20/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6695 - accuracy: 0.5919 - val_loss: 0.6613 - val_accuracy: 0.5536\n",
            "Epoch 21/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6725 - accuracy: 0.6009 - val_loss: 0.6631 - val_accuracy: 0.5893\n",
            "Epoch 22/30\n",
            "28/28 [==============================] - 89s 3s/step - loss: 0.6744 - accuracy: 0.6323 - val_loss: 0.6708 - val_accuracy: 0.6071\n",
            "Epoch 23/30\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.6700 - accuracy: 0.6188 - val_loss: 0.6689 - val_accuracy: 0.6607\n",
            "Epoch 24/30\n",
            "28/28 [==============================] - 84s 3s/step - loss: 0.6762 - accuracy: 0.6143 - val_loss: 0.6605 - val_accuracy: 0.6429\n",
            "Epoch 25/30\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.6642 - accuracy: 0.6054 - val_loss: 0.6627 - val_accuracy: 0.5714\n",
            "Epoch 26/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6841 - accuracy: 0.5336 - val_loss: 0.6674 - val_accuracy: 0.5536\n",
            "Epoch 27/30\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.6764 - accuracy: 0.5964 - val_loss: 0.6635 - val_accuracy: 0.5893\n",
            "Epoch 28/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6673 - accuracy: 0.6054 - val_loss: 0.6606 - val_accuracy: 0.6071\n",
            "Epoch 29/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6788 - accuracy: 0.5605 - val_loss: 0.6608 - val_accuracy: 0.6071\n",
            "Epoch 30/30\n",
            "28/28 [==============================] - 83s 3s/step - loss: 0.6649 - accuracy: 0.6009 - val_loss: 0.6615 - val_accuracy: 0.5714\n",
            "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 166 167 168 169 224 225 226 227 228 229 230 231 232 233 234 235\n",
            " 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253\n",
            " 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
            " 272 273 274 275 276 277 278] TEST: [164 165 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185\n",
            " 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203\n",
            " 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221\n",
            " 222 223]\n",
            "Epoch 1/30\n",
            "28/28 [==============================] - 95s 3s/step - loss: 0.6666 - accuracy: 0.5695 - val_loss: 0.6597 - val_accuracy: 0.6964\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - 84s 3s/step - loss: 0.6621 - accuracy: 0.6099 - val_loss: 0.6681 - val_accuracy: 0.6071\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6680 - accuracy: 0.5650 - val_loss: 0.6673 - val_accuracy: 0.5893\n",
            "Epoch 4/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6632 - accuracy: 0.6054 - val_loss: 0.6586 - val_accuracy: 0.6786\n",
            "Epoch 5/30\n",
            "28/28 [==============================] - 89s 3s/step - loss: 0.6704 - accuracy: 0.5830 - val_loss: 0.6752 - val_accuracy: 0.5893\n",
            "Epoch 6/30\n",
            "28/28 [==============================] - 87s 3s/step - loss: 0.6631 - accuracy: 0.6009 - val_loss: 0.6592 - val_accuracy: 0.6429\n",
            "Epoch 7/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6697 - accuracy: 0.5516 - val_loss: 0.6763 - val_accuracy: 0.5714\n",
            "Epoch 8/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6594 - accuracy: 0.5964 - val_loss: 0.6600 - val_accuracy: 0.6429\n",
            "Epoch 9/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6564 - accuracy: 0.6054 - val_loss: 0.6652 - val_accuracy: 0.6964\n",
            "Epoch 10/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6642 - accuracy: 0.5695 - val_loss: 0.6700 - val_accuracy: 0.6071\n",
            "Epoch 11/30\n",
            "28/28 [==============================] - 93s 3s/step - loss: 0.6483 - accuracy: 0.6143 - val_loss: 0.7089 - val_accuracy: 0.4464\n",
            "Epoch 12/30\n",
            "28/28 [==============================] - 89s 3s/step - loss: 0.6722 - accuracy: 0.5605 - val_loss: 0.6609 - val_accuracy: 0.6250\n",
            "Epoch 13/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6553 - accuracy: 0.6188 - val_loss: 0.6635 - val_accuracy: 0.6964\n",
            "Epoch 14/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6519 - accuracy: 0.5964 - val_loss: 0.6609 - val_accuracy: 0.6429\n",
            "Epoch 15/30\n",
            "28/28 [==============================] - 94s 3s/step - loss: 0.6562 - accuracy: 0.5605 - val_loss: 0.6609 - val_accuracy: 0.6607\n",
            "Epoch 16/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6536 - accuracy: 0.6099 - val_loss: 0.6686 - val_accuracy: 0.6607\n",
            "Epoch 17/30\n",
            "28/28 [==============================] - 94s 3s/step - loss: 0.6646 - accuracy: 0.6323 - val_loss: 0.6858 - val_accuracy: 0.6071\n",
            "Epoch 18/30\n",
            "28/28 [==============================] - 91s 3s/step - loss: 0.6682 - accuracy: 0.5471 - val_loss: 0.6642 - val_accuracy: 0.6786\n",
            "Epoch 19/30\n",
            "28/28 [==============================] - 94s 3s/step - loss: 0.6546 - accuracy: 0.6009 - val_loss: 0.6634 - val_accuracy: 0.6250\n",
            "Epoch 20/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6601 - accuracy: 0.6278 - val_loss: 0.6770 - val_accuracy: 0.6071\n",
            "Epoch 21/30\n",
            "28/28 [==============================] - 93s 3s/step - loss: 0.6488 - accuracy: 0.6009 - val_loss: 0.6783 - val_accuracy: 0.5893\n",
            "Epoch 22/30\n",
            "28/28 [==============================] - 89s 3s/step - loss: 0.6674 - accuracy: 0.5785 - val_loss: 0.6629 - val_accuracy: 0.6250\n",
            "Epoch 23/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6501 - accuracy: 0.5919 - val_loss: 0.6881 - val_accuracy: 0.5536\n",
            "Epoch 24/30\n",
            "28/28 [==============================] - 88s 3s/step - loss: 0.6756 - accuracy: 0.6009 - val_loss: 0.6636 - val_accuracy: 0.6250\n",
            "Epoch 25/30\n",
            "28/28 [==============================] - 89s 3s/step - loss: 0.6797 - accuracy: 0.5561 - val_loss: 0.6647 - val_accuracy: 0.6250\n",
            "Epoch 26/30\n",
            "28/28 [==============================] - 94s 3s/step - loss: 0.6489 - accuracy: 0.6009 - val_loss: 0.6636 - val_accuracy: 0.6429\n",
            "Epoch 27/30\n",
            "28/28 [==============================] - 92s 3s/step - loss: 0.6503 - accuracy: 0.5919 - val_loss: 0.6832 - val_accuracy: 0.6071\n",
            "Epoch 28/30\n",
            "28/28 [==============================] - 91s 3s/step - loss: 0.6489 - accuracy: 0.5830 - val_loss: 0.6676 - val_accuracy: 0.6250\n",
            "Epoch 29/30\n",
            "28/28 [==============================] - 90s 3s/step - loss: 0.6494 - accuracy: 0.6143 - val_loss: 0.6710 - val_accuracy: 0.7143\n",
            "Epoch 30/30\n",
            "28/28 [==============================] - 94s 3s/step - loss: 0.6529 - accuracy: 0.6188 - val_loss: 0.6825 - val_accuracy: 0.5714\n",
            "TRAIN:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223] TEST: [224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241\n",
            " 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259\n",
            " 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277\n",
            " 278]\n",
            "Epoch 1/30\n",
            "28/28 [==============================] - 93s 3s/step - loss: 0.6560 - accuracy: 0.5982 - val_loss: 0.6618 - val_accuracy: 0.5455\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - 93s 3s/step - loss: 0.6494 - accuracy: 0.6339 - val_loss: 0.6591 - val_accuracy: 0.5455\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - 89s 3s/step - loss: 0.6640 - accuracy: 0.5848 - val_loss: 0.6856 - val_accuracy: 0.5818\n",
            "Epoch 4/30\n",
            "10/28 [=========>....................] - ETA: 50s - loss: 0.6712 - accuracy: 0.5875"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "# import tensorflow as tf\n",
        "\n",
        "initial_G = nx.read_edgelist(\"/content/drive/MyDrive/ART_Inv/GNN/Biogrid/biogrid_minimum.edgelist\")  \n",
        "G = nx.Graph(initial_G)\n",
        "A = nx.adjacency_matrix(G).todense()\n",
        "\n",
        "print(\"Adjacency Matrix:\")\n",
        "print(A)\n",
        "\n",
        "# diagonal matrix\n",
        "D = np.diag(A.sum(axis=1))\n",
        "\n",
        "# graph laplacian\n",
        "L = D-A\n",
        "\n",
        "# eigenvalues and eigenvectors\n",
        "vals, vecs = np.linalg.eig(L)\n",
        "\n",
        "# create data frame with the second and the third minimum eigenvalue\n",
        "df = pd.DataFrame(vecs.T, \n",
        "                   columns=vals)\n",
        "print(df)\n",
        "print(vals)\n",
        "path ='/content/drive/MyDrive/ART_Inv/GNN/Biogrid/eigenvalues.csv'\n",
        "df = pd.read_csv(path)\n",
        "first_eigen_val = df.columns[np.where(df.columns == np.amin(df.columns))[0]][0]\n",
        "df = df.drop([first_eigen_val], axis=1)\n",
        "\n",
        "second_eigen_val = df.columns[np.where(df.columns == np.amin(df.columns))[0]][0]\n",
        "df2 = pd.DataFrame(df[second_eigen_val], columns=[second_eigen_val])\n",
        "df = df.drop([second_eigen_val], axis=1)\n",
        "\n",
        "third_eigen_val = df.columns[np.where(df.columns == np.amin(df.columns))[0]][0]\n",
        "col = df[third_eigen_val]\n",
        "\n",
        "df2[third_eigen_val] =  col\n",
        "\n",
        "print(\"#################################################\")\n",
        "print(df2)\n",
        "print(np.abs(np.round(df2[second_eigen_val])).max()+1)\n",
        "print(np.abs(np.round(df2[third_eigen_val])).max()+1)\n",
        "\n",
        "for i in range(len(df2)):\n",
        "  plt.plot(df2[third_eigen_val][i], df2[second_eigen_val][i], marker=\"x\",  markersize=7, color=\"green\")\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "print(\"#################################################\")\n",
        "\n",
        "print(df2[third_eigen_val].min())\n",
        "df2[third_eigen_val] = df2[third_eigen_val]+abs(df2[third_eigen_val].min())\n",
        "df2[second_eigen_val] = df2[second_eigen_val]+abs(df2[second_eigen_val].min())\n",
        "print(df2)\n",
        "for i in range(len(df2)):\n",
        "  plt.plot(df2[third_eigen_val][i], df2[second_eigen_val][i], marker=\"x\",  markersize=7, color=\"green\")\n",
        "# plt.show()\n",
        "\n",
        "df2 = df2*100\n",
        "df2 = df2.astype(\"int32\")\n",
        "print(df2)\n",
        "matrix = np.zeros((df2[third_eigen_val].max()+1, df2[second_eigen_val].max()+1))\n",
        "print(matrix.size)\n",
        "# df1 = df2[df2.duplicated(keep=False)]\n",
        "\n",
        "# print (df1)\n",
        "heat = df2.groupby(df2.columns.tolist()).size().reset_index().rename(columns={0:'records'})\n",
        "print(heat)\n",
        "\n",
        "matrix[heat[third_eigen_val], heat[second_eigen_val]] = heat[\"records\"]\n",
        "print(matrix)\n",
        "import seaborn as sns\n",
        "# sns.heatmap(matrix)\n",
        "# plt.show()\n",
        "\n",
        "######################## MAPEO DE GENES ########################\n",
        "\n",
        "\n",
        "matrix = np.zeros((100, 100))\n",
        "path ='/content/drive/MyDrive/ART_Inv/GNN/Biogrid/clinic_and_RNA_data_raw.csv'\n",
        "data = pd.read_csv(path)\n",
        "# print(data)\n",
        "X = []\n",
        "Y = []\n",
        "for patient in range(len(data)): # Recorro pacientes\n",
        "  # print(patient)\n",
        "  for i in range(len(df2)):\n",
        "    # print(df2.iloc[i])\n",
        "    # print(df2[third_eigen_val][i])\n",
        "    # print(df2[second_eigen_val][i])\n",
        "    # print(list(G.nodes)[i])\n",
        "    # print(data[list(G.nodes)[i]][patient])\n",
        "    matrix[df2[third_eigen_val][i], df2[second_eigen_val][i]] = data[list(G.nodes)[i]][patient]\n",
        "    matrix = matrix.astype(\"int32\")\n",
        "  X.append(matrix)\n",
        "  # print(data.PFS[patient])2\n",
        "  if data.PFS[patient] < 3:\n",
        "    Y.append(0)\n",
        "  else:\n",
        "    Y.append(1)\n",
        "print(X[0])\n",
        "\n",
        "#################################### CNN #####################################\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import activations\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "# input = tf.keras.layers.Input(shape=(None, None, 100, 100))\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(512, (5, 5), activation='relu', input_shape = (100,100,1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256))\n",
        "model.add(layers.Dense(256))\n",
        "model.add(layers.Dense(128))\n",
        "model.add(layers.Dense(1, activation='hard_sigmoid'))\n",
        "\n",
        "print(model.summary())\n",
        "# print(model.output_shape)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "train_data, test_data, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, stratify=Y)\n",
        "print(\"train data\")\n",
        "print(len(train_data))\n",
        "from sklearn.model_selection import StratifiedKFold # import KFold\n",
        "kf=StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
        "\n",
        "val_avg = []\n",
        "test_avg = []\n",
        "test_f1_score = []\n",
        "for train_index, val_index in kf.split(train_data, Y_train):\n",
        "    train_dataset=[]\n",
        "    val_dataset=[]\n",
        "    train_y_dataset = []\n",
        "    val_y_dataset = []\n",
        "    print(\"TRAIN: \", train_index, \"TEST:\", val_index)\n",
        "    for i in train_index:\n",
        "        train_dataset.append(train_data[i])\n",
        "        train_y_dataset.append(Y_train[i])\n",
        "    for i in val_index:\n",
        "        val_dataset.append(train_data[i])\n",
        "        val_y_dataset.append(Y_train[i])\n",
        "\n",
        "    # print(len(train_dataset))\n",
        "    # print(len(val_dataset))\n",
        "    train_y_dataset=np.array(train_y_dataset)\n",
        "    val_y_dataset=np.array(val_y_dataset)\n",
        "    train_dataset=np.array(train_dataset)\n",
        "    val_dataset=np.array(val_dataset)\n",
        "    # train_dataset=train_dataset.reshape(251,100,100)\n",
        "\n",
        "    # print(\"len\")\n",
        "    # print(len(train_y_dataset))\n",
        "    # print(len(train_dataset))\n",
        "\n",
        "    model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate= 0.00001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    history = model.fit(train_dataset, train_y_dataset, epochs=30, \n",
        "                    validation_data=(val_dataset, val_y_dataset), batch_size = 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PccEzZCj41GN"
      },
      "outputs": [],
      "source": [
        "test_data=np.array(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt0hfir52z5W"
      },
      "outputs": [],
      "source": [
        "test_data = test_data.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYdXOV_z2n4J"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_data, Y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "fb15f1e0f376981e7b6e1fc44ae8b8146823f10f258bcd6e448b0230b889fc06"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
