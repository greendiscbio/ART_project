{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cpu.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.loader import DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gene matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"../../../Sunitinib_Avelumab+Axitinib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A3GALT2</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AAED1</th>\n",
       "      <th>AAMDC</th>\n",
       "      <th>...</th>\n",
       "      <th>ZSWIM2</th>\n",
       "      <th>ZSWIM3</th>\n",
       "      <th>ZSWIM4</th>\n",
       "      <th>ZSWIM5</th>\n",
       "      <th>ZSWIM7</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.91</td>\n",
       "      <td>10.78</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.18</td>\n",
       "      <td>6.22</td>\n",
       "      <td>4.38</td>\n",
       "      <td>3.86</td>\n",
       "      <td>2.77</td>\n",
       "      <td>4.42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.73</td>\n",
       "      <td>4.40</td>\n",
       "      <td>4.23</td>\n",
       "      <td>3.72</td>\n",
       "      <td>3.18</td>\n",
       "      <td>3.96</td>\n",
       "      <td>3.82</td>\n",
       "      <td>4.12</td>\n",
       "      <td>8.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.26</td>\n",
       "      <td>9.77</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.25</td>\n",
       "      <td>6.16</td>\n",
       "      <td>6.61</td>\n",
       "      <td>5.14</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1.97</td>\n",
       "      <td>3.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.84</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.31</td>\n",
       "      <td>2.95</td>\n",
       "      <td>4.11</td>\n",
       "      <td>4.20</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.41</td>\n",
       "      <td>10.49</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.28</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.51</td>\n",
       "      <td>4.44</td>\n",
       "      <td>3.09</td>\n",
       "      <td>2.44</td>\n",
       "      <td>3.73</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.45</td>\n",
       "      <td>4.62</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.04</td>\n",
       "      <td>3.78</td>\n",
       "      <td>4.33</td>\n",
       "      <td>4.00</td>\n",
       "      <td>8.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.13</td>\n",
       "      <td>9.45</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.77</td>\n",
       "      <td>6.27</td>\n",
       "      <td>4.31</td>\n",
       "      <td>1.91</td>\n",
       "      <td>3.02</td>\n",
       "      <td>4.98</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.44</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.91</td>\n",
       "      <td>4.04</td>\n",
       "      <td>8.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.91</td>\n",
       "      <td>8.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.47</td>\n",
       "      <td>5.90</td>\n",
       "      <td>4.11</td>\n",
       "      <td>3.16</td>\n",
       "      <td>2.28</td>\n",
       "      <td>4.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.74</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.34</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.21</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.47</td>\n",
       "      <td>7.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>1.29</td>\n",
       "      <td>9.15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.16</td>\n",
       "      <td>6.24</td>\n",
       "      <td>4.81</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.35</td>\n",
       "      <td>3.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.11</td>\n",
       "      <td>4.52</td>\n",
       "      <td>3.57</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.54</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.67</td>\n",
       "      <td>8.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>2.76</td>\n",
       "      <td>9.95</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.02</td>\n",
       "      <td>7.07</td>\n",
       "      <td>6.14</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.29</td>\n",
       "      <td>3.43</td>\n",
       "      <td>4.66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.14</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.24</td>\n",
       "      <td>4.16</td>\n",
       "      <td>2.89</td>\n",
       "      <td>3.24</td>\n",
       "      <td>4.16</td>\n",
       "      <td>4.00</td>\n",
       "      <td>8.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1.13</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.72</td>\n",
       "      <td>6.21</td>\n",
       "      <td>5.04</td>\n",
       "      <td>2.03</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.93</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.92</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.61</td>\n",
       "      <td>9.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>1.74</td>\n",
       "      <td>8.96</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.98</td>\n",
       "      <td>6.16</td>\n",
       "      <td>5.75</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.61</td>\n",
       "      <td>3.95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.40</td>\n",
       "      <td>4.63</td>\n",
       "      <td>2.82</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.01</td>\n",
       "      <td>3.77</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4.34</td>\n",
       "      <td>7.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>2.12</td>\n",
       "      <td>10.50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.04</td>\n",
       "      <td>7.17</td>\n",
       "      <td>6.35</td>\n",
       "      <td>4.63</td>\n",
       "      <td>3.34</td>\n",
       "      <td>2.30</td>\n",
       "      <td>4.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.46</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.67</td>\n",
       "      <td>2.81</td>\n",
       "      <td>4.31</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.96</td>\n",
       "      <td>9.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows × 9999 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     A1BG    A2M  A2ML1  A3GALT2  A4GALT  AAAS  AACS  AADAT  AAED1  AAMDC  \\\n",
       "0    2.91  10.78   0.01     0.01    7.18  6.22  4.38   3.86   2.77   4.42   \n",
       "1    2.26   9.77   0.01     1.25    6.16  6.61  5.14   2.61   1.97   3.71   \n",
       "2    2.41  10.49   0.01     0.28    6.90  6.51  4.44   3.09   2.44   3.73   \n",
       "3    1.13   9.45   0.01     0.01    6.77  6.27  4.31   1.91   3.02   4.98   \n",
       "4    0.91   8.11   0.01     0.01    6.47  5.90  4.11   3.16   2.28   4.69   \n",
       "..    ...    ...    ...      ...     ...   ...   ...    ...    ...    ...   \n",
       "349  1.29   9.15   0.01     0.29    7.16  6.24  4.81   2.08   2.35   3.60   \n",
       "350  2.76   9.95   0.01     4.02    7.07  6.14  3.32   3.29   3.43   4.66   \n",
       "351  1.13   8.37   0.01     0.01    6.72  6.21  5.04   2.03   3.39   3.93   \n",
       "352  1.74   8.96   0.01     0.01    5.98  6.16  5.75   2.14   2.61   3.95   \n",
       "353  2.12  10.50   0.01     2.04    7.17  6.35  4.63   3.34   2.30   4.53   \n",
       "\n",
       "     ...  ZSWIM2  ZSWIM3  ZSWIM4  ZSWIM5  ZSWIM7  ZWILCH  ZWINT  ZXDA  ZYG11B  \\\n",
       "0    ...    0.01    2.73    4.40    4.23    3.72    3.18   3.96  3.82    4.12   \n",
       "1    ...    0.01    2.96    3.84    3.40    3.06    3.31   2.95  4.11    4.20   \n",
       "2    ...    0.01    2.45    4.62    2.48    3.50    3.04   3.78  4.33    4.00   \n",
       "3    ...    0.01    2.44    3.92    3.42    3.69    1.95   2.25  3.91    4.04   \n",
       "4    ...    0.01    1.74    4.82    0.01    4.34    2.63   2.21  4.30    4.47   \n",
       "..   ...     ...     ...     ...     ...     ...     ...    ...   ...     ...   \n",
       "349  ...    0.01    2.11    4.52    3.57    3.84    1.61   2.54  3.43    3.67   \n",
       "350  ...    0.01    2.14    5.00    3.24    4.16    2.89   3.24  4.16    4.00   \n",
       "351  ...    0.01    0.92    3.32    0.54    3.05    3.42   3.35  3.15    3.61   \n",
       "352  ...    0.01    3.40    4.63    2.82    3.23    3.01   3.77  4.44    4.34   \n",
       "353  ...    0.01    1.90    4.46    3.16    3.67    2.81   4.31  3.58    3.96   \n",
       "\n",
       "      ZYX  \n",
       "0    8.23  \n",
       "1    8.26  \n",
       "2    8.44  \n",
       "3    8.38  \n",
       "4    7.99  \n",
       "..    ...  \n",
       "349  8.25  \n",
       "350  8.97  \n",
       "351  9.07  \n",
       "352  7.93  \n",
       "353  9.06  \n",
       "\n",
       "[354 rows x 9999 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes = pd.read_csv('Data/Preprocessed_data/Gene_Matrix/biogrid_included_genes_anova_10000_f_selection.csv')\n",
    "Y = genes.Y\n",
    "\n",
    "genes = genes.iloc[:,1:10000] \n",
    "genes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A3GALT2</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AAED1</th>\n",
       "      <th>AAMDC</th>\n",
       "      <th>...</th>\n",
       "      <th>ZSWIM2</th>\n",
       "      <th>ZSWIM3</th>\n",
       "      <th>ZSWIM4</th>\n",
       "      <th>ZSWIM5</th>\n",
       "      <th>ZSWIM7</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.273585</td>\n",
       "      <td>0.810757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.696043</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.519789</td>\n",
       "      <td>0.685053</td>\n",
       "      <td>0.584570</td>\n",
       "      <td>0.696682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526042</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.888421</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.518041</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.321337</td>\n",
       "      <td>0.615658</td>\n",
       "      <td>0.464935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.212264</td>\n",
       "      <td>0.609562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220641</td>\n",
       "      <td>0.512590</td>\n",
       "      <td>0.691964</td>\n",
       "      <td>0.720317</td>\n",
       "      <td>0.462633</td>\n",
       "      <td>0.347181</td>\n",
       "      <td>0.584518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.265664</td>\n",
       "      <td>0.713684</td>\n",
       "      <td>0.605159</td>\n",
       "      <td>0.551546</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.395887</td>\n",
       "      <td>0.644128</td>\n",
       "      <td>0.472727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.752988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048043</td>\n",
       "      <td>0.645683</td>\n",
       "      <td>0.647321</td>\n",
       "      <td>0.535620</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.486647</td>\n",
       "      <td>0.587678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.461153</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.692460</td>\n",
       "      <td>0.481959</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.452442</td>\n",
       "      <td>0.572954</td>\n",
       "      <td>0.519481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.105660</td>\n",
       "      <td>0.545817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622302</td>\n",
       "      <td>0.540179</td>\n",
       "      <td>0.501319</td>\n",
       "      <td>0.338078</td>\n",
       "      <td>0.658754</td>\n",
       "      <td>0.785150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.450521</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.717895</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.201031</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.344473</td>\n",
       "      <td>0.587189</td>\n",
       "      <td>0.503896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.084906</td>\n",
       "      <td>0.278884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568345</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.448549</td>\n",
       "      <td>0.560498</td>\n",
       "      <td>0.439169</td>\n",
       "      <td>0.739336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.268229</td>\n",
       "      <td>0.511278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859127</td>\n",
       "      <td>0.376289</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.444730</td>\n",
       "      <td>0.740214</td>\n",
       "      <td>0.402597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0.120755</td>\n",
       "      <td>0.486056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049822</td>\n",
       "      <td>0.692446</td>\n",
       "      <td>0.526786</td>\n",
       "      <td>0.633245</td>\n",
       "      <td>0.368327</td>\n",
       "      <td>0.459941</td>\n",
       "      <td>0.567141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.364583</td>\n",
       "      <td>0.436090</td>\n",
       "      <td>0.749474</td>\n",
       "      <td>0.759921</td>\n",
       "      <td>0.113402</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.221080</td>\n",
       "      <td>0.455516</td>\n",
       "      <td>0.470130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.259434</td>\n",
       "      <td>0.645418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713523</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.240106</td>\n",
       "      <td>0.583630</td>\n",
       "      <td>0.780415</td>\n",
       "      <td>0.734597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372396</td>\n",
       "      <td>0.556391</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.823413</td>\n",
       "      <td>0.443299</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.408740</td>\n",
       "      <td>0.572954</td>\n",
       "      <td>0.657143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.105660</td>\n",
       "      <td>0.330677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613309</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.693931</td>\n",
       "      <td>0.359431</td>\n",
       "      <td>0.768546</td>\n",
       "      <td>0.619273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.111579</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.579897</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.434164</td>\n",
       "      <td>0.683117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0.163208</td>\n",
       "      <td>0.448207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480216</td>\n",
       "      <td>0.491071</td>\n",
       "      <td>0.881266</td>\n",
       "      <td>0.379004</td>\n",
       "      <td>0.537092</td>\n",
       "      <td>0.622433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700521</td>\n",
       "      <td>0.463659</td>\n",
       "      <td>0.591579</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.474227</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.480720</td>\n",
       "      <td>0.693950</td>\n",
       "      <td>0.387013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.199057</td>\n",
       "      <td>0.754980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361210</td>\n",
       "      <td>0.694245</td>\n",
       "      <td>0.575893</td>\n",
       "      <td>0.585752</td>\n",
       "      <td>0.592527</td>\n",
       "      <td>0.445104</td>\n",
       "      <td>0.714060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.309896</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.663158</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.259640</td>\n",
       "      <td>0.558719</td>\n",
       "      <td>0.680519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows × 9999 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1BG       A2M  A2ML1   A3GALT2    A4GALT      AAAS      AACS  \\\n",
       "0    0.273585  0.810757    0.0  0.000000  0.696043  0.517857  0.519789   \n",
       "1    0.212264  0.609562    0.0  0.220641  0.512590  0.691964  0.720317   \n",
       "2    0.226415  0.752988    0.0  0.048043  0.645683  0.647321  0.535620   \n",
       "3    0.105660  0.545817    0.0  0.000000  0.622302  0.540179  0.501319   \n",
       "4    0.084906  0.278884    0.0  0.000000  0.568345  0.375000  0.448549   \n",
       "..        ...       ...    ...       ...       ...       ...       ...   \n",
       "349  0.120755  0.486056    0.0  0.049822  0.692446  0.526786  0.633245   \n",
       "350  0.259434  0.645418    0.0  0.713523  0.676259  0.482143  0.240106   \n",
       "351  0.105660  0.330677    0.0  0.000000  0.613309  0.513393  0.693931   \n",
       "352  0.163208  0.448207    0.0  0.000000  0.480216  0.491071  0.881266   \n",
       "353  0.199057  0.754980    0.0  0.361210  0.694245  0.575893  0.585752   \n",
       "\n",
       "        AADAT     AAED1     AAMDC  ...  ZSWIM2    ZSWIM3    ZSWIM4    ZSWIM5  \\\n",
       "0    0.685053  0.584570  0.696682  ...     0.0  0.526042  0.406015  0.888421   \n",
       "1    0.462633  0.347181  0.584518  ...     0.0  0.585938  0.265664  0.713684   \n",
       "2    0.548043  0.486647  0.587678  ...     0.0  0.453125  0.461153  0.520000   \n",
       "3    0.338078  0.658754  0.785150  ...     0.0  0.450521  0.285714  0.717895   \n",
       "4    0.560498  0.439169  0.739336  ...     0.0  0.268229  0.511278  0.000000   \n",
       "..        ...       ...       ...  ...     ...       ...       ...       ...   \n",
       "349  0.368327  0.459941  0.567141  ...     0.0  0.364583  0.436090  0.749474   \n",
       "350  0.583630  0.780415  0.734597  ...     0.0  0.372396  0.556391  0.680000   \n",
       "351  0.359431  0.768546  0.619273  ...     0.0  0.054688  0.135338  0.111579   \n",
       "352  0.379004  0.537092  0.622433  ...     0.0  0.700521  0.463659  0.591579   \n",
       "353  0.592527  0.445104  0.714060  ...     0.0  0.309896  0.421053  0.663158   \n",
       "\n",
       "       ZSWIM7    ZWILCH  ZWINT      ZXDA    ZYG11B       ZYX  \n",
       "0    0.736111  0.518041  0.666  0.321337  0.615658  0.464935  \n",
       "1    0.605159  0.551546  0.464  0.395887  0.644128  0.472727  \n",
       "2    0.692460  0.481959  0.630  0.452442  0.572954  0.519481  \n",
       "3    0.730159  0.201031  0.324  0.344473  0.587189  0.503896  \n",
       "4    0.859127  0.376289  0.316  0.444730  0.740214  0.402597  \n",
       "..        ...       ...    ...       ...       ...       ...  \n",
       "349  0.759921  0.113402  0.382  0.221080  0.455516  0.470130  \n",
       "350  0.823413  0.443299  0.522  0.408740  0.572954  0.657143  \n",
       "351  0.603175  0.579897  0.544  0.149100  0.434164  0.683117  \n",
       "352  0.638889  0.474227  0.628  0.480720  0.693950  0.387013  \n",
       "353  0.726190  0.422680  0.736  0.259640  0.558719  0.680519  \n",
       "\n",
       "[354 rows x 9999 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = genes.columns\n",
    "d = scaler.fit_transform(genes)\n",
    "genes = pd.DataFrame(d, columns=names)\n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genes, test_genes, Y_train, Y_test = train_test_split(genes, Y, test_size=0.1, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='Graph_networks/Biogrid/Data/biogrid_minimum.edgelist'\n",
    "G_initial = nx.read_edgelist(path)\n",
    "G = G_initial.subgraph(genes.columns)\n",
    "nx.write_edgelist(G, \"Data/conected_graph.edgelist\")\n",
    "data = pd.read_csv(\"Data/conected_graph.edgelist\", delimiter=' ')\n",
    "edge_index1=data[data.columns[0]].to_numpy()\n",
    "edge_index2=data[data.columns[1]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = np.concatenate((edge_index1, edge_index2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FHL3', 'FHL3', 'FHL3', ..., 'UGT2A2', 'ZNF174', 'ZSCAN30'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6084"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(edge_index)\n",
    "len(list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index1 = le.transform(edge_index1)\n",
    "edge_index2 = le.transform(edge_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = [edge_index1]+[edge_index2]\n",
    "edge_index = np.array(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1875, 1875, 1875, ..., 5675, 6073, 6073],\n",
       "       [ 563, 1337, 1536, ..., 5674, 5939, 6071]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1875, 1875, 1875,  ..., 5675, 6073, 6073],\n",
       "        [ 563, 1337, 1536,  ..., 5674, 5939, 6071]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edge_index, dtype=torch.int64)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    }
   ],
   "source": [
    "train_data=[]\n",
    "for g in range(len(train_genes)):\n",
    "  b=[]\n",
    "  for i in train_genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    a.append(i)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.float, requires_grad=True).reshape([-1,1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y_train.iloc[g]], dtype=torch.float, requires_grad=True).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  train_data.append(data)\n",
    "\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "test_data=[]\n",
    "for g in range(len(test_genes)):\n",
    "  b=[]\n",
    "  for i in test_genes.iloc[g].to_numpy():\n",
    "    a=[]\n",
    "    a.append(i)\n",
    "    b.append(a)\n",
    "  x = torch.tensor([b], dtype=torch.float, requires_grad=True).reshape([-1,1])\n",
    "  edge_index = edge_index\n",
    "  y = torch.tensor([Y_test.iloc[g]], dtype=torch.float, requires_grad=True).reshape([-1, 1])\n",
    "  data = Data(x=x, edge_index=edge_index, y=y)\n",
    "  test_data.append(data)\n",
    "\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[9999, 1], edge_index=[2, 5083], y=[1, 1])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1274],\n",
       "        [0.4721],\n",
       "        [0.0000],\n",
       "        ...,\n",
       "        [0.3316],\n",
       "        [0.6050],\n",
       "        [0.5844]], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Patient sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 9999\n",
      "Number of charcateristics per node: 1\n",
      "Number of edges: 5083\n",
      "Average node degree: 0.51\n",
      "Has isolated nodes: True\n",
      "Has self-loops: False\n",
      "Is undirected: False\n",
      "Number of node features: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of charcateristics per node: {data.num_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "print(f'Number of node features: {data.num_node_features}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU\n",
    "from torch_geometric.nn import GINConv\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GIN(torch.nn.Module):\n",
    "#     def __init__(self, dim_h):\n",
    "#         super(GIN, self).__init__()\n",
    "#         self.conv1 = GINConv(\n",
    "#             Sequential(Linear(1, dim_h),\n",
    "#                        BatchNorm1d(dim_h), ReLU(),\n",
    "#                        Linear(dim_h, dim_h), ReLU()))\n",
    "#         self.conv2 = GINConv(\n",
    "#             Sequential(Linear(dim_h, dim_h), \n",
    "#                        BatchNorm1d(dim_h), ReLU(),\n",
    "#                        Linear(dim_h, dim_h), ReLU()))\n",
    "#         self.conv3 = GINConv(\n",
    "#             Sequential(Linear(dim_h, dim_h), \n",
    "#                        BatchNorm1d(dim_h), ReLU(),\n",
    "#                        Linear(dim_h, dim_h), ReLU()))\n",
    "#         self.lin1 = Linear(dim_h*3, dim_h*3)\n",
    "#         self.lin2 = Linear(dim_h*3, 1)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         # Node embeddings \n",
    "#         h1 = self.conv1(x, edge_index)\n",
    "#         h2 = self.conv2(h1, edge_index)\n",
    "#         h3 = self.conv3(h1, edge_index)\n",
    "\n",
    "#         # Graph-level readout\n",
    "#         h1 = global_add_pool(h1, batch)\n",
    "#         h2 = global_add_pool(h2, batch)\n",
    "#         h3 = global_add_pool(h3, batch)\n",
    "\n",
    "#         # Concatenate graph embeddings\n",
    "#         h = torch.cat((h1,h2,h3), dim=1)\n",
    "\n",
    "#         # Classifier\n",
    "#         h = self.lin1(h)\n",
    "#         h = h.relu()\n",
    "#         # h = F.dropout(h, p=0.5, training=self.training)\n",
    "#         h = self.lin2(h)\n",
    "#         m = nn.Sigmoid()\n",
    "#         return m(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    acc = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index)\n",
    "        loss = criterion(output, data.y)/ len(train_loader)\n",
    "        acc = accuracy(torch.round(output), data.y)\n",
    "        f1score = f1_score(data.y.detach().numpy(), torch.round(output).detach().numpy(), average='weighted')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f'Train loss: {loss:.4f}, Train Acc: {acc:.4f}, Train f1-score: {f1score:.4f}')\n",
    "        # print(data.y)\n",
    "        # print(output)\n",
    "    return loss, acc, f1score\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def validation(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    loss = 0\n",
    "    for data in val_loader:\n",
    "        output = model(data.x, data.edge_index)\n",
    "        loss = criterion(output, data.y)/ len(val_loader)\n",
    "        acc = accuracy(torch.round(output), data.y)\n",
    "        f1score = f1_score(data.y, torch.round(output), average='weighted')\n",
    "        # print(f'Val loss: {loss:.4f}, Val Acc: {acc:.4f}, Val f1-score: {f1score:.4f}')\n",
    "\n",
    "    return loss, acc, f1score\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def test(model, test_data):\n",
    "    acc = 0\n",
    "    test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "    for data in test_loader:\n",
    "        output = model(data.x, data.edge_index)\n",
    "        acc = accuracy(torch.round(output), data.y) / len(test_loader)\n",
    "        f1score = f1_score(data.y.detach().numpy(), torch.round(output).detach().numpy(), average='weighted')\n",
    "        print(f'Test Acc: {acc:.4f}, Test f1-score: {f1score:.4f}')\n",
    "    return acc, f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold # import KFold\n",
    "kf=StratifiedKFold(n_splits=10, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge\n",
    "from torch_geometric.nn import global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x132a9154b20>"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_epoch, color=\"red\", label=\"train accuracy\")\n",
    "plt.plot(val_epoch, color=\"blue\", label=\"val accuracy\")\n",
    "plt.plot(train_loss_, color=\"orange\", label=\"train loss\")\n",
    "plt.plot(val_loss_, color=\"purple\", label=\"val loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy and loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb15f1e0f376981e7b6e1fc44ae8b8146823f10f258bcd6e448b0230b889fc06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
